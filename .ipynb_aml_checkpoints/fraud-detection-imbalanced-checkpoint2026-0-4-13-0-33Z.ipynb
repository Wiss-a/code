{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.0"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31234,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Fraud Detection with Imbalanced Data - Practical Approach\n### Optimized for CPU usage on Kaggle\n#### Uses real data patterns instead of SMOTE synthetic generation","metadata":{}},{"cell_type":"markdown","source":"## 1. Import Libraries","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split, StratifiedKFold\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import (classification_report, confusion_matrix, \n                             roc_auc_score, roc_curve, precision_recall_curve,\n                             f1_score, precision_score, recall_score)\nfrom sklearn.ensemble import RandomForestClassifier\nfrom imblearn.under_sampling import RandomUnderSampler\nimport warnings\nwarnings.filterwarnings('ignore')\n\nprint(\"âœ“ All libraries imported successfully!\")","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 2. Load and Explore Data","metadata":{}},{"cell_type":"code","source":"# Load your dataset (adjust the path as needed)\n# Example for Kaggle: df = pd.read_csv('/kaggle/input/your-dataset-name/file.csv')\ndf = pd.read_csv('/kaggle/input/your-dataset-name/creditcard.csv')\n\nprint(\"Dataset Shape:\", df.shape)\nprint(\"\\n\" + \"=\"*50)\nprint(\"First few rows:\")\ndisplay(df.head())","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Check fraud distribution\n# Assuming 'Class' is your target column (0=legitimate, 1=fraud)\n# If your column has a different name, change 'Class' below\n\nprint(\"Fraud Distribution:\")\nprint(df['Class'].value_counts())\nprint(\"\\nFraud Percentage:\", (df['Class'].sum() / len(df)) * 100, \"%\")\n\n# Visualize distribution\nplt.figure(figsize=(8, 5))\ndf['Class'].value_counts().plot(kind='bar', color=['green', 'red'])\nplt.title('Original Class Distribution', fontsize=14, fontweight='bold')\nplt.xlabel('Class (0=Legitimate, 1=Fraud)')\nplt.ylabel('Count')\nplt.xticks(rotation=0)\nplt.show()","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 3. Data Preprocessing","metadata":{}},{"cell_type":"code","source":"# Separate features and target\nX = df.drop('Class', axis=1)\ny = df['Class']\n\n# Split into train and test sets (stratified to maintain fraud ratio)\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=42, stratify=y\n)\n\nprint(f\"Training set size: {len(X_train):,}\")\nprint(f\"Test set size: {len(X_test):,}\")\nprint(f\"\\nFraud in training: {y_train.sum()} ({(y_train.sum()/len(y_train)*100):.2f}%)\")\nprint(f\"Fraud in test: {y_test.sum()} ({(y_test.sum()/len(y_test)*100):.2f}%)\")","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Scale features (important for many algorithms)\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n\nprint(\"âœ“ Features scaled successfully!\")","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 4. Light Undersampling (Keeps Real Data Patterns)","metadata":{}},{"cell_type":"code","source":"# This creates a more balanced training set without synthetic data\n# Target: approximately 15% fraud (much more realistic than 30%)\n# Adjust sampling_strategy to change the ratio (0.15 = 15% fraud)\n\nrus = RandomUnderSampler(sampling_strategy=0.15, random_state=42)\nX_train_resampled, y_train_resampled = rus.fit_resample(X_train_scaled, y_train)\n\nprint(\"=\"*50)\nprint(\"After Light Undersampling:\")\nprint(f\"Training set size: {len(X_train_resampled):,}\")\nprint(f\"Fraud cases: {y_train_resampled.sum()} ({(y_train_resampled.sum()/len(y_train_resampled)*100):.2f}%)\")\nprint(f\"Legitimate cases: {len(y_train_resampled) - y_train_resampled.sum()}\")\nprint(\"=\"*50)\n\n# Visualize new distribution\nplt.figure(figsize=(10, 5))\nplt.subplot(1, 2, 1)\npd.Series(y_train).value_counts().plot(kind='bar', color=['green', 'red'])\nplt.title('Original Training Set', fontsize=12, fontweight='bold')\nplt.xlabel('Class')\nplt.ylabel('Count')\nplt.xticks(rotation=0)\n\nplt.subplot(1, 2, 2)\npd.Series(y_train_resampled).value_counts().plot(kind='bar', color=['green', 'red'])\nplt.title('After Undersampling', fontsize=12, fontweight='bold')\nplt.xlabel('Class')\nplt.ylabel('Count')\nplt.xticks(rotation=0)\nplt.tight_layout()\nplt.show()","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 5. Model Training - Random Forest","metadata":{}},{"cell_type":"code","source":"print(\"Training Random Forest model...\\n\")\n\n# Random Forest with class weights (handles imbalance well)\n# n_jobs=2 to limit CPU usage on Kaggle\nrf_model = RandomForestClassifier(\n    n_estimators=100,           # Moderate number of trees for CPU efficiency\n    max_depth=15,               # Limit depth to prevent overfitting and reduce CPU\n    min_samples_split=10,       # Require more samples to split\n    min_samples_leaf=5,         # Require more samples in leaf nodes\n    class_weight='balanced',    # KEY: Automatically handles imbalance\n    random_state=42,\n    n_jobs=2,                   # Limit parallel jobs for CPU\n    verbose=1\n)\n\n# Train on undersampled data\nrf_model.fit(X_train_resampled, y_train_resampled)\n\nprint(\"\\nâœ“ Model training complete!\")","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 6. Predictions","metadata":{}},{"cell_type":"code","source":"# Predict on test set (original imbalanced distribution)\ny_pred = rf_model.predict(X_test_scaled)\ny_pred_proba = rf_model.predict_proba(X_test_scaled)[:, 1]\n\nprint(\"âœ“ Predictions generated!\")","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 7. Model Evaluation","metadata":{}},{"cell_type":"code","source":"print(\"=\"*70)\nprint(\"MODEL EVALUATION ON TEST SET (Real Imbalanced Distribution)\")\nprint(\"=\"*70)\n\n# Classification Report\nprint(\"\\nClassification Report:\")\nprint(classification_report(y_test, y_pred, target_names=['Legitimate', 'Fraud']))","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Confusion Matrix\ncm = confusion_matrix(y_test, y_pred)\nprint(\"\\nConfusion Matrix:\")\nprint(cm)\n\n# Calculate key metrics\ntn, fp, fn, tp = cm.ravel()\nprint(f\"\\nTrue Negatives: {tn:,}\")\nprint(f\"False Positives: {fp:,}\")\nprint(f\"False Negatives: {fn:,} â† (Missed Fraud - BAD)\")\nprint(f\"True Positives: {tp:,} â† (Caught Fraud - GOOD)\")","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ROC-AUC Score and other metrics\nroc_auc = roc_auc_score(y_test, y_pred_proba)\nprecision = precision_score(y_test, y_pred)\nrecall = recall_score(y_test, y_pred)\nf1 = f1_score(y_test, y_pred)\n\nprint(\"\\n\" + \"=\"*70)\nprint(\"KEY PERFORMANCE METRICS\")\nprint(\"=\"*70)\nprint(f\"ðŸŽ¯ ROC-AUC Score: {roc_auc:.4f}\")\nprint(f\"\\nPrecision: {precision:.4f} (Of predicted fraud, how many were actually fraud?)\")\nprint(f\"Recall: {recall:.4f} (Of all actual fraud, how many did we catch?)\")\nprint(f\"F1-Score: {f1:.4f}\")\nprint(\"=\"*70)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 8. Visualizations","metadata":{}},{"cell_type":"code","source":"fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n\n# 1. Confusion Matrix Heatmap\nsns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[0, 0], cbar=True)\naxes[0, 0].set_title('Confusion Matrix', fontsize=14, fontweight='bold')\naxes[0, 0].set_ylabel('Actual')\naxes[0, 0].set_xlabel('Predicted')\naxes[0, 0].set_xticklabels(['Legitimate', 'Fraud'])\naxes[0, 0].set_yticklabels(['Legitimate', 'Fraud'])\n\n# 2. ROC Curve\nfpr, tpr, _ = roc_curve(y_test, y_pred_proba)\naxes[0, 1].plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.4f})')\naxes[0, 1].plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='Random Classifier')\naxes[0, 1].set_xlim([0.0, 1.0])\naxes[0, 1].set_ylim([0.0, 1.05])\naxes[0, 1].set_xlabel('False Positive Rate')\naxes[0, 1].set_ylabel('True Positive Rate')\naxes[0, 1].set_title('ROC Curve', fontsize=14, fontweight='bold')\naxes[0, 1].legend(loc=\"lower right\")\naxes[0, 1].grid(alpha=0.3)\n\n# 3. Precision-Recall Curve\nprecision_curve, recall_curve, _ = precision_recall_curve(y_test, y_pred_proba)\naxes[1, 0].plot(recall_curve, precision_curve, color='green', lw=2)\naxes[1, 0].set_xlabel('Recall')\naxes[1, 0].set_ylabel('Precision')\naxes[1, 0].set_title('Precision-Recall Curve', fontsize=14, fontweight='bold')\naxes[1, 0].grid(alpha=0.3)\naxes[1, 0].set_xlim([0.0, 1.0])\naxes[1, 0].set_ylim([0.0, 1.05])\n\n# 4. Feature Importance (Top 15)\nfeature_importance = pd.DataFrame({\n    'feature': X.columns,\n    'importance': rf_model.feature_importances_\n}).sort_values('importance', ascending=False).head(15)\n\naxes[1, 1].barh(range(len(feature_importance)), feature_importance['importance'], color='steelblue')\naxes[1, 1].set_yticks(range(len(feature_importance)))\naxes[1, 1].set_yticklabels(feature_importance['feature'])\naxes[1, 1].set_xlabel('Importance')\naxes[1, 1].set_title('Top 15 Feature Importances', fontsize=14, fontweight='bold')\naxes[1, 1].invert_yaxis()\n\nplt.tight_layout()\nplt.savefig('fraud_detection_results.png', dpi=300, bbox_inches='tight')\nplt.show()\n\nprint(\"âœ“ Visualizations saved as 'fraud_detection_results.png'\")","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 9. Threshold Analysis (Optional)","metadata":{}},{"cell_type":"code","source":"print(\"=\"*70)\nprint(\"THRESHOLD ANALYSIS\")\nprint(\"=\"*70)\nprint(\"Testing different probability thresholds to optimize recall...\\n\")\n\n# Try different thresholds to optimize recall (catching fraud)\nthresholds = [0.3, 0.4, 0.5, 0.6, 0.7]\nresults = []\n\nfor threshold in thresholds:\n    y_pred_threshold = (y_pred_proba >= threshold).astype(int)\n    recall_t = recall_score(y_test, y_pred_threshold)\n    precision_t = precision_score(y_test, y_pred_threshold)\n    f1_t = f1_score(y_test, y_pred_threshold)\n    \n    results.append({\n        'Threshold': threshold,\n        'Recall': f'{recall_t:.4f}',\n        'Precision': f'{precision_t:.4f}',\n        'F1-Score': f'{f1_t:.4f}'\n    })\n\nresults_df = pd.DataFrame(results)\nprint(\"Performance at Different Thresholds:\")\nprint(results_df.to_string(index=False))\nprint(\"\\nNote: Lower threshold = catch more fraud but more false alarms\")","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 10. Summary and Recommendations","metadata":{}},{"cell_type":"code","source":"print(\"\\n\" + \"=\"*70)\nprint(\"SUMMARY & RECOMMENDATIONS FOR YOUR TEACHER\")\nprint(\"=\"*70)\nprint(\"\"\"\nâœ“ This model is trained on REAL data with minimal resampling\nâœ“ Uses class weights to handle imbalance naturally\nâœ“ Light undersampling (15% fraud) maintains realistic patterns\nâœ“ Evaluation focuses on RECALL (catching fraud) not accuracy\nâœ“ ROC-AUC and Precision-Recall curves show true performance\nâœ“ This approach mirrors real-world fraud detection systems\n\nKEY INSIGHT:\nA model trained on artificial 30/70 balance will FAIL in production\nwhen it encounters real 0.17% fraud rate. This model is trained for\nreality and will generalize to production environments.\n\nWHY SMOTE DOESN'T WORK:\n- Creates synthetic fraud patterns by interpolation\n- Real fraud has specific, rare patterns that shouldn't be averaged\n- Model learns to detect \"SMOTE fraud\" not real fraud\n- Production deployment would show immediate performance degradation\n\"\"\")\nprint(\"=\"*70)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Optional: Save Model","metadata":{}},{"cell_type":"code","source":"# Uncomment to save the model\n# import joblib\n# joblib.dump(rf_model, 'fraud_detection_model.pkl')\n# joblib.dump(scaler, 'scaler.pkl')\n# print(\"âœ“ Model and scaler saved!\")","metadata":{},"outputs":[],"execution_count":null}]}