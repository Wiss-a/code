{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DÃ©tection de Fraude dans les Transactions FinanciÃ¨res\n",
    "## Azure Machine Learning + Power BI\n",
    "\n",
    "**Objectif**: EntraÃ®ner un modÃ¨le de forÃªts alÃ©atoires pour dÃ©tecter les transactions frauduleuses et prÃ©parer les visualisations pour Power BI.\n",
    "\n",
    "**Dataset**: 6M de transactions avec ~8k cas de fraude (classe trÃ¨s dÃ©sÃ©quilibrÃ©e)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Configuration et Installation des DÃ©pendances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installation des packages nÃ©cessaires\n",
    "!pip install azureml-sdk pandas numpy scikit-learn imbalanced-learn matplotlib seaborn mlflow joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Azure ML\n",
    "from azureml.core import Workspace, Dataset, Experiment, Run\n",
    "from azureml.core.model import Model\n",
    "\n",
    "# Machine Learning\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    classification_report, \n",
    "    confusion_matrix, \n",
    "    roc_auc_score, \n",
    "    roc_curve,\n",
    "    precision_recall_curve,\n",
    "    f1_score,\n",
    "    precision_score,\n",
    "    recall_score\n",
    ")\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "\n",
    "# MLflow pour le suivi d'expÃ©rience\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "\n",
    "# Joblib pour la sÃ©rialisation\n",
    "import joblib\n",
    "\n",
    "print(\"âœ… BibliothÃ¨ques importÃ©es avec succÃ¨s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Connexion Ã  Azure ML Workspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connexion au workspace Azure ML\n",
    "# Option 1: Depuis un fichier config.json\n",
    "try:\n",
    "    ws = Workspace.from_config()\n",
    "    print(f\"âœ… ConnectÃ© au workspace: {ws.name}\")\n",
    "except:\n",
    "    # Option 2: Connexion manuelle\n",
    "    ws = Workspace(\n",
    "        subscription_id='<VOTRE_SUBSCRIPTION_ID>',\n",
    "        resource_group='<VOTRE_RESOURCE_GROUP>',\n",
    "        workspace_name='<VOTRE_WORKSPACE_NAME>'\n",
    "    )\n",
    "    print(f\"âœ… ConnectÃ© au workspace: {ws.name}\")\n",
    "\n",
    "# CrÃ©er une expÃ©rience\n",
    "experiment_name = 'fraud-detection-experiment'\n",
    "experiment = Experiment(workspace=ws, name=experiment_name)\n",
    "print(f\"âœ… ExpÃ©rience crÃ©Ã©e: {experiment_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Ingestion - Chargement des DonnÃ©es"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 1: Charger depuis Azure Blob Storage ou Dataset enregistrÃ©\n",
    "# dataset = Dataset.get_by_name(ws, name='fraud_transactions')\n",
    "# df = dataset.to_pandas_dataframe()\n",
    "\n",
    "# Option 2: Charger depuis un fichier local ou un chemin\n",
    "# Remplacez par le chemin de votre dataset\n",
    "df = pd.read_csv('fraud_transactions.csv')\n",
    "\n",
    "# Afficher les colonnes disponibles\n",
    "print(\"Colonnes du dataset:\")\n",
    "print(df.columns.tolist())\n",
    "\n",
    "print(f\"\\nâœ… DonnÃ©es chargÃ©es: {df.shape[0]:,} lignes, {df.shape[1]} colonnes\")\n",
    "print(f\"\\nğŸ“Š Distribution des classes:\")\n",
    "print(df['isFraud'].value_counts())\n",
    "print(f\"\\nTaux de fraude: {df['isFraud'].mean()*100:.4f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AperÃ§u des donnÃ©es\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Informations sur le dataset\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Analyse Exploratoire des DonnÃ©es (EDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistiques descriptives\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisation de la distribution des classes\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Graphique en barres\n",
    "df['isFraud'].value_counts().plot(kind='bar', ax=axes[0], color=['#2ecc71', '#e74c3c'])\n",
    "axes[0].set_title('Distribution des Classes', fontsize=14, fontweight='bold')\n",
    "axes[0].set_xlabel('Classe (0=LÃ©gal, 1=Fraude)')\n",
    "axes[0].set_ylabel('Nombre de transactions')\n",
    "axes[0].set_xticklabels(['LÃ©gal', 'Fraude'], rotation=0)\n",
    "\n",
    "# Graphique en camembert\n",
    "colors = ['#2ecc71', '#e74c3c']\n",
    "df['isFraud'].value_counts().plot(kind='pie', ax=axes[1], autopct='%1.2f%%', colors=colors)\n",
    "axes[1].set_title('Proportion Fraude vs LÃ©gal', fontsize=14, fontweight='bold')\n",
    "axes[1].set_ylabel('')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('class_distribution.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"DÃ©sÃ©quilibre des classes: {df['isFraud'].value_counts()[0] / df['isFraud'].value_counts()[1]:.1f}:1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matrice de corrÃ©lation (pour les features numÃ©riques)\n",
    "numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "correlation_matrix = df[numeric_cols].corr()\n",
    "\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(correlation_matrix, annot=False, cmap='coolwarm', center=0, \n",
    "            square=True, linewidths=1, cbar_kws={\"shrink\": 0.8})\n",
    "plt.title('Matrice de CorrÃ©lation des Features', fontsize=16, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.savefig('correlation_matrix.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyse spÃ©cifique aux transactions financiÃ¨res\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# 1. Distribution des montants (Ã©chelle log pour mieux voir)\n",
    "axes[0, 0].hist(df['amount'], bins=50, edgecolor='black', alpha=0.7)\n",
    "axes[0, 0].set_xlabel('Montant de Transaction', fontsize=11)\n",
    "axes[0, 0].set_ylabel('FrÃ©quence', fontsize=11)\n",
    "axes[0, 0].set_title('Distribution des Montants de Transaction', fontsize=13, fontweight='bold')\n",
    "axes[0, 0].set_yscale('log')\n",
    "\n",
    "# 2. Fraude par type de transaction\n",
    "type_fraud = df.groupby([col for col in df.columns if col.startswith('type_')][0].replace({0: 'Non', 1: 'Oui'}))['isFraud'].value_counts().unstack(fill_value=0)\n",
    "if 'type_PAYMENT' in df.columns:\n",
    "    fraud_by_type = pd.DataFrame({\n",
    "        'Type': [],\n",
    "        'Fraude': [],\n",
    "        'LÃ©gal': []\n",
    "    })\n",
    "    for col in [c for c in df.columns if c.startswith('type_')]:\n",
    "        type_name = col.replace('type_', '')\n",
    "        fraud_count = df[df[col] == 1]['isFraud'].sum()\n",
    "        legit_count = len(df[df[col] == 1]) - fraud_count\n",
    "        fraud_by_type = pd.concat([fraud_by_type, pd.DataFrame({\n",
    "            'Type': [type_name],\n",
    "            'Fraude': [fraud_count],\n",
    "            'LÃ©gal': [legit_count]\n",
    "        })], ignore_index=True)\n",
    "    \n",
    "    fraud_by_type.set_index('Type').plot(kind='bar', ax=axes[0, 1], color=['#e74c3c', '#2ecc71'])\n",
    "    axes[0, 1].set_xlabel('Type de Transaction', fontsize=11)\n",
    "    axes[0, 1].set_ylabel('Nombre de Transactions', fontsize=11)\n",
    "    axes[0, 1].set_title('Fraude par Type de Transaction', fontsize=13, fontweight='bold')\n",
    "    axes[0, 1].legend(['Fraude', 'LÃ©gal'])\n",
    "    axes[0, 1].set_xticklabels(fraud_by_type['Type'], rotation=45, ha='right')\n",
    "\n",
    "# 3. Montant moyen par statut de fraude\n",
    "amount_by_fraud = df.groupby('isFraud')['amount'].mean()\n",
    "axes[1, 0].bar(['LÃ©gal', 'Fraude'], amount_by_fraud.values, color=['#2ecc71', '#e74c3c'])\n",
    "axes[1, 0].set_ylabel('Montant Moyen', fontsize=11)\n",
    "axes[1, 0].set_title('Montant Moyen par Statut', fontsize=13, fontweight='bold')\n",
    "for i, v in enumerate(amount_by_fraud.values):\n",
    "    axes[1, 0].text(i, v + v*0.02, f'{v:,.2f}', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "# 4. Distribution des fraudes vs lÃ©gitimes (montants)\n",
    "df[df['isFraud'] == 0]['amount'].hist(bins=50, alpha=0.6, label='LÃ©gal', color='green', ax=axes[1, 1])\n",
    "df[df['isFraud'] == 1]['amount'].hist(bins=50, alpha=0.6, label='Fraude', color='red', ax=axes[1, 1])\n",
    "axes[1, 1].set_xlabel('Montant', fontsize=11)\n",
    "axes[1, 1].set_ylabel('FrÃ©quence', fontsize=11)\n",
    "axes[1, 1].set_title('Distribution des Montants: Fraude vs LÃ©gal', fontsize=13, fontweight='bold')\n",
    "axes[1, 1].legend()\n",
    "axes[1, 1].set_yscale('log')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('financial_analysis.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Statistiques clÃ©s\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ğŸ“Š STATISTIQUES CLÃ‰S DES TRANSACTIONS\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nMontant moyen des transactions lÃ©gales: {df[df['isFraud'] == 0]['amount'].mean():,.2f}\")\n",
    "print(f\"Montant moyen des transactions frauduleuses: {df[df['isFraud'] == 1]['amount'].mean():,.2f}\")\n",
    "print(f\"\\nMontant mÃ©dian des transactions lÃ©gales: {df[df['isFraud'] == 0]['amount'].median():,.2f}\")\n",
    "print(f\"Montant mÃ©dian des transactions frauduleuses: {df[df['isFraud'] == 1]['amount'].median():,.2f}\")\n",
    "\n",
    "if 'type_TRANSFER' in df.columns:\n",
    "    print(f\"\\nTaux de fraude pour TRANSFER: {df[df['type_TRANSFER'] == 1]['isFraud'].mean()*100:.2f}%\")\n",
    "if 'type_CASH_OUT' in df.columns:\n",
    "    print(f\"Taux de fraude pour CASH_OUT: {df[df['type_CASH_OUT'] == 1]['isFraud'].mean()*100:.2f}%\")\n",
    "if 'type_PAYMENT' in df.columns:\n",
    "    print(f\"Taux de fraude pour PAYMENT: {df[df['type_PAYMENT'] == 1]['isFraud'].mean()*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. PrÃ©paration des DonnÃ©es (Data Processing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gestion des valeurs manquantes\n",
    "print(\"Valeurs manquantes par colonne:\")\n",
    "print(df.isnull().sum())\n",
    "\n",
    "# Remplir ou supprimer les valeurs manquantes selon le besoin\n",
    "# Exemple: df.fillna(df.median(), inplace=True)\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "print(f\"\\nâœ… DonnÃ©es aprÃ¨s nettoyage: {df.shape[0]:,} lignes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Engineering spÃ©cifique au dataset de transactions financiÃ¨res\n",
    "\n",
    "# 1. Encoder le type de transaction (PAYMENT, TRANSFER, CASH_OUT, etc.)\n",
    "df = pd.get_dummies(df, columns=['type'], prefix='type', drop_first=False)\n",
    "\n",
    "# 2. CrÃ©er des features dÃ©rivÃ©es\n",
    "# Variation de balance pour l'origine\n",
    "df['balanceChange_orig'] = df['oldbalanceOrg'] - df['newbalanceOrig']\n",
    "\n",
    "# Variation de balance pour la destination\n",
    "df['balanceChange_dest'] = df['newbalanceDest'] - df['oldbalanceDest']\n",
    "\n",
    "# Ratio de la transaction par rapport au solde d'origine\n",
    "df['amountToBalanceRatio_orig'] = df['amount'] / (df['oldbalanceOrg'] + 1)  # +1 pour Ã©viter division par 0\n",
    "\n",
    "# Indicateur si le compte d'origine est vidÃ©\n",
    "df['isOriginEmpty'] = (df['newbalanceOrig'] == 0).astype(int)\n",
    "\n",
    "# Indicateur si la destination avait un solde nul\n",
    "df['isDestEmpty'] = (df['oldbalanceDest'] == 0).astype(int)\n",
    "\n",
    "# DiffÃ©rence entre le montant et le changement de balance (dÃ©tecte des anomalies)\n",
    "df['errorBalanceOrig'] = df['balanceChange_orig'] - df['amount']\n",
    "df['errorBalanceDest'] = df['balanceChange_dest'] - df['amount']\n",
    "\n",
    "# 3. Supprimer les colonnes non nÃ©cessaires pour le modÃ¨le\n",
    "# Les identifiants (nameOrig, nameDest) ne sont pas utiles pour la prÃ©diction\n",
    "columns_to_drop = ['nameOrig', 'nameDest', 'isFlaggedFraud']\n",
    "df = df.drop(columns=columns_to_drop, errors='ignore')\n",
    "\n",
    "print(\"âœ… Feature Engineering complÃ©tÃ©\")\n",
    "print(f\"Nombre total de features: {df.shape[1]}\")\n",
    "print(f\"\\nNouvelles features crÃ©Ã©es:\")\n",
    "print(\"  - balanceChange_orig: Variation du solde origine\")\n",
    "print(\"  - balanceChange_dest: Variation du solde destination\")\n",
    "print(\"  - amountToBalanceRatio_orig: Ratio montant/solde\")\n",
    "print(\"  - isOriginEmpty: Compte origine vidÃ©\")\n",
    "print(\"  - isDestEmpty: Destination avec solde initial nul\")\n",
    "print(\"  - errorBalanceOrig/Dest: DÃ©tection d'anomalies de balance\")\n",
    "print(f\"\\nTypes de transactions encodÃ©s: {[col for col in df.columns if col.startswith('type_')]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SÃ©paration des features et de la target\n",
    "X = df.drop('isFraud', axis=1)\n",
    "y = df['isFraud']\n",
    "\n",
    "# Sauvegarder les noms des colonnes pour plus tard\n",
    "feature_names = X.columns.tolist()\n",
    "\n",
    "print(f\"Features (X): {X.shape}\")\n",
    "print(f\"Target (y): {y.shape}\")\n",
    "print(f\"\\nDistribution de y:\\n{y.value_counts()}\")\n",
    "print(f\"\\nFeatures utilisÃ©es:\")\n",
    "for i, feat in enumerate(feature_names, 1):\n",
    "    print(f\"  {i:2d}. {feat}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Split Train/Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Division train/test (80/20)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, \n",
    "    test_size=0.2, \n",
    "    random_state=42, \n",
    "    stratify=y  # Maintenir la proportion des classes\n",
    ")\n",
    "\n",
    "print(f\"Train set: {X_train.shape[0]:,} transactions\")\n",
    "print(f\"Test set: {X_test.shape[0]:,} transactions\")\n",
    "print(f\"\\nDistribution train:\")\n",
    "print(y_train.value_counts())\n",
    "print(f\"\\nDistribution test:\")\n",
    "print(y_test.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Normalisation des DonnÃ©es"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardisation des features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(\"âœ… DonnÃ©es normalisÃ©es\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Data Balancing - Gestion du DÃ©sÃ©quilibre des Classes\n",
    "\n",
    "Avec un ratio de ~750:1 (6M lÃ©gitimes vs 8K fraudes), nous utilisons une approche hybride:\n",
    "- **SMOTE** pour augmenter les cas de fraude\n",
    "- **RandomUnderSampler** pour rÃ©duire les cas lÃ©gitimes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# StratÃ©gie de rÃ©Ã©quilibrage hybride\n",
    "# SMOTE pour oversampling de la classe minoritaire\n",
    "# Puis undersampling de la classe majoritaire\n",
    "\n",
    "# Calculer les ratios souhaitÃ©s\n",
    "fraud_count = y_train.sum()\n",
    "legit_count = len(y_train) - fraud_count\n",
    "\n",
    "print(f\"Avant rÃ©Ã©quilibrage:\")\n",
    "print(f\"  - Transactions lÃ©gitimes: {legit_count:,}\")\n",
    "print(f\"  - Transactions frauduleuses: {fraud_count:,}\")\n",
    "print(f\"  - Ratio: {legit_count/fraud_count:.1f}:1\")\n",
    "\n",
    "# Pipeline de rÃ©Ã©quilibrage\n",
    "# D'abord SMOTE pour crÃ©er des exemples synthÃ©tiques de fraude\n",
    "# Puis undersampling pour rÃ©duire les transactions lÃ©gitimes\n",
    "sampling_strategy_smote = 0.5  # Augmenter les fraudes Ã  50% des lÃ©gitimes\n",
    "sampling_strategy_under = 0.8  # RÃ©duire pour avoir un ratio de 1:1.25\n",
    "\n",
    "over = SMOTE(sampling_strategy=sampling_strategy_smote, random_state=42)\n",
    "under = RandomUnderSampler(sampling_strategy=sampling_strategy_under, random_state=42)\n",
    "\n",
    "# Appliquer le pipeline\n",
    "X_train_balanced, y_train_balanced = over.fit_resample(X_train_scaled, y_train)\n",
    "X_train_balanced, y_train_balanced = under.fit_resample(X_train_balanced, y_train_balanced)\n",
    "\n",
    "print(f\"\\nAprÃ¨s rÃ©Ã©quilibrage:\")\n",
    "print(f\"  - Total: {len(y_train_balanced):,} transactions\")\n",
    "print(f\"  - Distribution:\")\n",
    "print(pd.Series(y_train_balanced).value_counts())\n",
    "print(f\"  - Nouveau ratio: {pd.Series(y_train_balanced).value_counts()[0] / pd.Series(y_train_balanced).value_counts()[1]:.2f}:1\")\n",
    "print(\"\\nâœ… DonnÃ©es rÃ©Ã©quilibrÃ©es avec succÃ¨s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Model Training - EntraÃ®nement du ModÃ¨le Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DÃ©marrer le run MLflow\n",
    "run = experiment.start_logging()\n",
    "mlflow.sklearn.autolog()\n",
    "\n",
    "print(\"ğŸš€ DÃ©marrage de l'entraÃ®nement du modÃ¨le Random Forest...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Configuration du modÃ¨le Random Forest\n",
    "rf_model = RandomForestClassifier(\n",
    "    n_estimators=100,          # Nombre d'arbres\n",
    "    max_depth=20,              # Profondeur maximale\n",
    "    min_samples_split=10,      # Minimum d'Ã©chantillons pour split\n",
    "    min_samples_leaf=5,        # Minimum d'Ã©chantillons par feuille\n",
    "    max_features='sqrt',       # Nombre de features Ã  considÃ©rer\n",
    "    random_state=42,\n",
    "    n_jobs=-1,                 # Utiliser tous les CPU\n",
    "    verbose=1,\n",
    "    class_weight='balanced'    # Poids automatique des classes\n",
    ")\n",
    "\n",
    "# EntraÃ®nement\n",
    "start_time = datetime.now()\n",
    "rf_model.fit(X_train_balanced, y_train_balanced)\n",
    "training_time = (datetime.now() - start_time).total_seconds()\n",
    "\n",
    "print(f\"\\nâœ… EntraÃ®nement terminÃ© en {training_time:.2f} secondes\")\n",
    "\n",
    "# Log des hyperparamÃ¨tres\n",
    "run.log('n_estimators', 100)\n",
    "run.log('max_depth', 20)\n",
    "run.log('training_time_seconds', training_time)\n",
    "run.log('training_samples', len(y_train_balanced))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Ã‰valuation du ModÃ¨le"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PrÃ©dictions sur le test set\n",
    "y_pred = rf_model.predict(X_test_scaled)\n",
    "y_pred_proba = rf_model.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "# MÃ©triques de performance\n",
    "accuracy = rf_model.score(X_test_scaled, y_test)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"ğŸ“Š RÃ‰SULTATS DU MODÃˆLE\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Accuracy:  {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall:    {recall:.4f}\")\n",
    "print(f\"F1-Score:  {f1:.4f}\")\n",
    "print(f\"ROC-AUC:   {roc_auc:.4f}\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Log des mÃ©triques dans Azure ML\n",
    "run.log('accuracy', accuracy)\n",
    "run.log('precision', precision)\n",
    "run.log('recall', recall)\n",
    "run.log('f1_score', f1)\n",
    "run.log('roc_auc', roc_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rapport de classification dÃ©taillÃ©\n",
    "print(\"\\nğŸ“‹ Classification Report:\")\n",
    "print(classification_report(y_test, y_pred, target_names=['LÃ©gal', 'Fraude']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matrice de confusion\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=['LÃ©gal', 'Fraude'], \n",
    "            yticklabels=['LÃ©gal', 'Fraude'])\n",
    "plt.title('Matrice de Confusion', fontsize=16, fontweight='bold')\n",
    "plt.ylabel('Vraie Classe')\n",
    "plt.xlabel('Classe PrÃ©dite')\n",
    "plt.tight_layout()\n",
    "plt.savefig('confusion_matrix.png', dpi=300, bbox_inches='tight')\n",
    "run.log_image('confusion_matrix', plot=plt)\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nVrais NÃ©gatifs (TN): {cm[0, 0]:,}\")\n",
    "print(f\"Faux Positifs (FP): {cm[0, 1]:,}\")\n",
    "print(f\"Faux NÃ©gatifs (FN): {cm[1, 0]:,}\")\n",
    "print(f\"Vrais Positifs (TP): {cm[1, 1]:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Courbe ROC\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_pred_proba)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.4f})')\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='Random Classifier')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('Taux de Faux Positifs (FPR)', fontsize=12)\n",
    "plt.ylabel('Taux de Vrais Positifs (TPR)', fontsize=12)\n",
    "plt.title('Courbe ROC - DÃ©tection de Fraude', fontsize=16, fontweight='bold')\n",
    "plt.legend(loc=\"lower right\", fontsize=11)\n",
    "plt.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig('roc_curve.png', dpi=300, bbox_inches='tight')\n",
    "run.log_image('roc_curve', plot=plt)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Courbe Precision-Recall\n",
    "precision_curve, recall_curve, _ = precision_recall_curve(y_test, y_pred_proba)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.plot(recall_curve, precision_curve, color='blue', lw=2)\n",
    "plt.xlabel('Recall', fontsize=12)\n",
    "plt.ylabel('Precision', fontsize=12)\n",
    "plt.title('Courbe Precision-Recall', fontsize=16, fontweight='bold')\n",
    "plt.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig('precision_recall_curve.png', dpi=300, bbox_inches='tight')\n",
    "run.log_image('precision_recall_curve', plot=plt)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Feature Importance - Importance des Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extraire l'importance des features\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': feature_names,\n",
    "    'importance': rf_model.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "# Top 20 features les plus importantes\n",
    "top_n = 20\n",
    "top_features = feature_importance.head(top_n)\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.barh(range(len(top_features)), top_features['importance'], color='steelblue')\n",
    "plt.yticks(range(len(top_features)), top_features['feature'])\n",
    "plt.xlabel('Importance', fontsize=12)\n",
    "plt.title(f'Top {top_n} Features les Plus Importantes', fontsize=16, fontweight='bold')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.savefig('feature_importance.png', dpi=300, bbox_inches='tight')\n",
    "run.log_image('feature_importance', plot=plt)\n",
    "plt.show()\n",
    "\n",
    "# Afficher le dataframe\n",
    "print(\"\\nğŸ“Š Top 10 Features:\")\n",
    "print(feature_importance.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Sauvegarde du ModÃ¨le"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sauvegarder le modÃ¨le et le scaler localement\n",
    "model_filename = 'fraud_detection_rf_model.pkl'\n",
    "scaler_filename = 'scaler.pkl'\n",
    "\n",
    "joblib.dump(rf_model, model_filename)\n",
    "joblib.dump(scaler, scaler_filename)\n",
    "\n",
    "print(f\"âœ… ModÃ¨le sauvegardÃ©: {model_filename}\")\n",
    "print(f\"âœ… Scaler sauvegardÃ©: {scaler_filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enregistrer le modÃ¨le dans Azure ML\n",
    "run.upload_file(name='outputs/' + model_filename, path_or_stream=model_filename)\n",
    "run.upload_file(name='outputs/' + scaler_filename, path_or_stream=scaler_filename)\n",
    "\n",
    "# Enregistrer comme modÃ¨le Azure ML\n",
    "model = run.register_model(\n",
    "    model_name='fraud-detection-rf',\n",
    "    model_path='outputs/' + model_filename,\n",
    "    description='Random Forest model for fraud detection',\n",
    "    tags={\n",
    "        'algorithm': 'RandomForest',\n",
    "        'framework': 'scikit-learn',\n",
    "        'accuracy': f'{accuracy:.4f}',\n",
    "        'f1_score': f'{f1:.4f}',\n",
    "        'roc_auc': f'{roc_auc:.4f}'\n",
    "    }\n",
    ")\n",
    "\n",
    "print(f\"âœ… ModÃ¨le enregistrÃ© dans Azure ML: {model.name}, Version: {model.version}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Terminer le run\n",
    "run.complete()\n",
    "print(\"âœ… ExpÃ©rience Azure ML terminÃ©e\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. PrÃ©paration des DonnÃ©es pour Power BI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CrÃ©er un DataFrame avec les prÃ©dictions pour Power BI\n",
    "results_df = X_test.copy()\n",
    "results_df['actual_fraud'] = y_test.values\n",
    "results_df['predicted_fraud'] = y_pred\n",
    "results_df['fraud_probability'] = y_pred_proba\n",
    "\n",
    "# CatÃ©goriser les prÃ©dictions\n",
    "results_df['prediction_category'] = results_df['fraud_probability'].apply(\n",
    "    lambda x: 'Haute Suspicion' if x >= 0.8 else \n",
    "              ('Suspicion Moyenne' if x >= 0.5 else 'Faible Suspicion')\n",
    ")\n",
    "\n",
    "# Identifier les erreurs du modÃ¨le\n",
    "results_df['prediction_result'] = 'Correct'\n",
    "results_df.loc[(results_df['actual_fraud'] == 1) & (results_df['predicted_fraud'] == 0), 'prediction_result'] = 'Faux NÃ©gatif'\n",
    "results_df.loc[(results_df['actual_fraud'] == 0) & (results_df['predicted_fraud'] == 1), 'prediction_result'] = 'Faux Positif'\n",
    "\n",
    "print(f\"âœ… DataFrame de rÃ©sultats crÃ©Ã©: {results_df.shape}\")\n",
    "results_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CrÃ©er un rÃ©sumÃ© des mÃ©triques pour Power BI\n",
    "metrics_summary = pd.DataFrame({\n",
    "    'Metric': ['Accuracy', 'Precision', 'Recall', 'F1-Score', 'ROC-AUC'],\n",
    "    'Value': [accuracy, precision, recall, f1, roc_auc]\n",
    "})\n",
    "\n",
    "print(\"\\nğŸ“Š RÃ©sumÃ© des MÃ©triques:\")\n",
    "print(metrics_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CrÃ©er des statistiques par catÃ©gorie de prÃ©diction\n",
    "category_stats = results_df.groupby('prediction_category').agg({\n",
    "    'fraud_probability': ['count', 'mean', 'min', 'max'],\n",
    "    'actual_fraud': 'sum'\n",
    "}).round(4)\n",
    "category_stats.columns = ['Count', 'Avg_Probability', 'Min_Probability', 'Max_Probability', 'Actual_Frauds']\n",
    "category_stats = category_stats.reset_index()\n",
    "\n",
    "print(\"\\nğŸ“Š Statistiques par CatÃ©gorie:\")\n",
    "print(category_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyse des transactions suspectes (probabilitÃ© > 0.8)\n",
    "suspicious_transactions = results_df[results_df['fraud_probability'] > 0.8].copy()\n",
    "suspicious_transactions = suspicious_transactions.sort_values('fraud_probability', ascending=False)\n",
    "\n",
    "print(f\"\\nğŸš¨ Transactions Hautement Suspectes: {len(suspicious_transactions):,}\")\n",
    "print(f\"   - Vraies fraudes dÃ©tectÃ©es: {suspicious_transactions['actual_fraud'].sum():,}\")\n",
    "print(f\"   - Taux de prÃ©cision: {suspicious_transactions['actual_fraud'].mean()*100:.2f}%\")\n",
    "\n",
    "suspicious_transactions.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Export des DonnÃ©es pour Power BI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export vers CSV pour Power BI\n",
    "results_df.to_csv('fraud_predictions_powerbi.csv', index=False)\n",
    "metrics_summary.to_csv('model_metrics_powerbi.csv', index=False)\n",
    "category_stats.to_csv('category_statistics_powerbi.csv', index=False)\n",
    "suspicious_transactions.to_csv('suspicious_transactions_powerbi.csv', index=False)\n",
    "feature_importance.to_csv('feature_importance_powerbi.csv', index=False)\n",
    "\n",
    "print(\"âœ… Fichiers exportÃ©s pour Power BI:\")\n",
    "print(\"   - fraud_predictions_powerbi.csv\")\n",
    "print(\"   - model_metrics_powerbi.csv\")\n",
    "print(\"   - category_statistics_powerbi.csv\")\n",
    "print(\"   - suspicious_transactions_powerbi.csv\")\n",
    "print(\"   - feature_importance_powerbi.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15. Guide d'IntÃ©gration Power BI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\"\"\n",
    "â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
    "â•‘           GUIDE D'INTÃ‰GRATION POWER BI                             â•‘\n",
    "â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "ğŸ“Š FICHIERS EXPORTÃ‰S POUR POWER BI:\n",
    "   âœ“ fraud_predictions_powerbi.csv - Toutes les prÃ©dictions\n",
    "   âœ“ model_metrics_powerbi.csv - MÃ©triques du modÃ¨le\n",
    "   âœ“ category_statistics_powerbi.csv - Stats par catÃ©gorie\n",
    "   âœ“ suspicious_transactions_powerbi.csv - Transactions suspectes\n",
    "   âœ“ feature_importance_powerbi.csv - Importance des variables\n",
    "\n",
    "ğŸ¯ VISUALISATIONS RECOMMANDÃ‰ES DANS POWER BI:\n",
    "\n",
    "1. DASHBOARD PRINCIPAL:\n",
    "   â€¢ Carte de KPI: Nombre total de fraudes dÃ©tectÃ©es\n",
    "   â€¢ Carte de KPI: Taux de prÃ©cision du modÃ¨le\n",
    "   â€¢ Carte de KPI: F1-Score\n",
    "   â€¢ Graphique en anneau: RÃ©partition LÃ©gitimes vs Fraudes\n",
    "\n",
    "2. ANALYSE DES TRANSACTIONS SUSPECTES:\n",
    "   â€¢ Tableau: Top 100 transactions les plus suspectes\n",
    "   â€¢ Graphique en barres: Nombre de transactions par catÃ©gorie de suspicion\n",
    "   â€¢ Graphique linÃ©aire: Ã‰volution temporelle des fraudes (si date disponible)\n",
    "\n",
    "3. PERFORMANCE DU MODÃˆLE:\n",
    "   â€¢ Matrice de confusion (visuel personnalisÃ©)\n",
    "   â€¢ Graphique en colonnes: MÃ©triques du modÃ¨le\n",
    "   â€¢ Courbe ROC (si possible avec visuel personnalisÃ©)\n",
    "\n",
    "4. ANALYSE DES FEATURES:\n",
    "   â€¢ Graphique en barres horizontales: Top 20 features importantes\n",
    "   â€¢ Graphique de dispersion: Relation entre features et fraude\n",
    "\n",
    "5. FILTRES INTERACTIFS:\n",
    "   â€¢ Filtre par catÃ©gorie de prÃ©diction\n",
    "   â€¢ Filtre par rÃ©sultat de prÃ©diction (Correct, FP, FN)\n",
    "   â€¢ Filtre par probabilitÃ© de fraude (seuils ajustables)\n",
    "\n",
    "ğŸ’¡ Ã‰TAPES D'IMPORT DANS POWER BI:\n",
    "   1. Ouvrir Power BI Desktop\n",
    "   2. Obtenir des donnÃ©es > Fichier CSV\n",
    "   3. SÃ©lectionner les fichiers CSV exportÃ©s\n",
    "   4. Transformer les donnÃ©es si nÃ©cessaire (Power Query)\n",
    "   5. CrÃ©er les relations entre les tables\n",
    "   6. Construire vos visualisations\n",
    "   7. Publier vers Power BI Service pour partage\n",
    "\n",
    "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 16. RÃ©sumÃ© Final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\"\"\n",
    "â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
    "â•‘                    RÃ‰SUMÃ‰ DU PROJET                                â•‘\n",
    "â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "âœ… Ã‰TAPES COMPLÃ‰TÃ‰ES:\n",
    "   1. âœ“ Data Ingestion - Chargement de 6M de transactions\n",
    "   2. âœ“ Data Processing - Nettoyage et feature engineering\n",
    "   3. âœ“ Data Balancing - RÃ©Ã©quilibrage SMOTE + UnderSampling\n",
    "   4. âœ“ Model Training - Random Forest avec 100 arbres\n",
    "   5. âœ“ Experiment Tracking - Enregistrement dans Azure ML\n",
    "   6. âœ“ Model Evaluation - MÃ©triques et visualisations\n",
    "   7. âœ“ Model Deployment - Sauvegarde et enregistrement Azure\n",
    "   8. âœ“ Power BI Export - Fichiers CSV prÃªts pour l'import\n",
    "\n",
    "ğŸ‰ PROJET COMPLÃ‰TÃ‰ AVEC SUCCÃˆS! ğŸ‰\n",
    "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\"\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
