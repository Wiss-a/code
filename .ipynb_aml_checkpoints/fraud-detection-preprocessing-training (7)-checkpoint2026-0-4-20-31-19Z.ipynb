{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Fraud Detection - Data Preprocessing & Model Training\n",
        "## Azure Cloud Data-Driven Application Project\n",
        "\n",
        "**Project:** Detection of fraud in financial transactions  \n",
        "**Dataset:** 6M rows ‚Üí Truncated & Balanced for optimal training  \n",
        "**Goal:** Train a high-performance fraud detection model\n",
        "\n",
        "---\n",
        "\n",
        "## Pipeline Overview:\n",
        "1. **Data Loading & Exploration**\n",
        "2. **Strategic Truncation** (All frauds + 500k non-frauds)\n",
        "3. **Data Balancing with SMOTE** (40/60 or customizable ratio)\n",
        "4. **Feature Engineering & Preprocessing**\n",
        "5. **Model Training** (Multiple algorithms)\n",
        "6. **Model Evaluation & Comparison**\n",
        "7. **Export Best Model** for deployment"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üì¶ 1. Import Required Libraries"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "pip install mlflow"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "trusted": true,
        "gather": {
          "logged": 1765915151158
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install xgboost lightgbm\n",
        "%pip install imbalanced-learn\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Data manipulation\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datetime import datetime\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Visualization\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "sns.set_style('whitegrid')\n",
        "\n",
        "# Data preprocessing\n",
        "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
        "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
        "from sklearn.utils import resample\n",
        "\n",
        "# Balancing techniques\n",
        "from imblearn.over_sampling import SMOTE, ADASYN, BorderlineSMOTE\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "from imblearn.combine import SMOTETomek, SMOTEENN\n",
        "\n",
        "# Machine Learning models\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from lightgbm import LGBMClassifier\n",
        "\n",
        "# Model evaluation\n",
        "from sklearn.metrics import (\n",
        "    classification_report, confusion_matrix, \n",
        "    accuracy_score, precision_score, recall_score, \n",
        "    f1_score, roc_auc_score, roc_curve,\n",
        "    precision_recall_curve, average_precision_score\n",
        ")\n",
        "\n",
        "# Model persistence\n",
        "import joblib\n",
        "import pickle\n",
        "\n",
        "# For experiment tracking (if using MLflow)\n",
        "try:\n",
        "    import mlflow\n",
        "    import mlflow.sklearn\n",
        "    MLFLOW_AVAILABLE = True\n",
        "    print(\"‚úÖ MLflow is available for experiment tracking\")\n",
        "except ImportError:\n",
        "    MLFLOW_AVAILABLE = False\n",
        "    print(\"‚ö†Ô∏è MLflow not available. Install with: pip install mlflow\")\n",
        "\n",
        "print(\"\\n‚úÖ All libraries imported successfully!\")\n",
        "print(f\"Pandas version: {pd.__version__}\")\n",
        "print(f\"NumPy version: {np.__version__}\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-12-16T13:16:03.334377Z",
          "iopub.execute_input": "2025-12-16T13:16:03.334769Z",
          "iopub.status.idle": "2025-12-16T13:16:09.760633Z",
          "shell.execute_reply.started": "2025-12-16T13:16:03.334741Z",
          "shell.execute_reply": "2025-12-16T13:16:09.759570Z"
        },
        "gather": {
          "logged": 1767558588446
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üìä 2. Load and Explore Dataset"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install azureml-dataset-runtime --upgrade"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1765906716103
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Configuration\n",
        "from datetime import datetime\n",
        "import pandas as pd\n",
        "from azureml.core import Workspace, Dataset\n",
        "\n",
        "DATA_PATH = 'fraud_dataset.csv'  \n",
        "TARGET_COLUMN = 'isFraud'\n",
        "\n",
        "print(\"üìÇ Loading dataset...\")\n",
        "print(f\"File: {DATA_PATH}\")\n",
        "print(f\"Loading started at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
        "\n",
        "# Connect to Azure ML Workspace\n",
        "ws = Workspace.from_config()\n",
        "datastore = ws.get_default_datastore()\n",
        "\n",
        "# Load dataset FROM BLOB STORAGE\n",
        "dataset = Dataset.Tabular.from_delimited_files(\n",
        "    path=[(datastore, DATA_PATH)]\n",
        ")\n",
        "\n",
        "df = dataset.to_pandas_dataframe()\n",
        "\n",
        "print(f\"‚úÖ Dataset loaded successfully!\")\n",
        "print(f\"Loading completed at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
        "\n",
        "# Display basic information\n",
        "print(\"=\"*60)\n",
        "print(\"DATASET OVERVIEW\")\n",
        "print(\"=\"*60)\n",
        "print(f\"Total rows: {df.shape[0]:,}\")\n",
        "print(f\"Total columns: {df.shape[1]:,}\")\n",
        "print(f\"Memory usage: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
        "print(\"\\nFirst few rows:\")\n",
        "display(df.head())\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "trusted": true,
        "gather": {
          "logged": 1767558609588
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Examine data types and missing values\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"DATA QUALITY CHECK\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "print(\"\\nüìã Column Information:\")\n",
        "df.info()\n",
        "\n",
        "print(\"\\n‚ùì Missing Values:\")\n",
        "missing = df.isnull().sum()\n",
        "missing_pct = 100 * missing / len(df)\n",
        "missing_df = pd.DataFrame({\n",
        "    'Missing Count': missing,\n",
        "    'Percentage': missing_pct\n",
        "}).sort_values('Missing Count', ascending=False)\n",
        "print(missing_df[missing_df['Missing Count'] > 0])\n",
        "\n",
        "print(\"\\nüìä Basic Statistics:\")\n",
        "display(df.describe())"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "trusted": true,
        "gather": {
          "logged": 1767558611964
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üéØ 3. Analyze Class Distribution (Before Truncation)"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "print(\"=\"*60)\n",
        "print(\"CLASS DISTRIBUTION ANALYSIS\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Count fraud vs non-fraud cases\n",
        "fraud_counts = df[TARGET_COLUMN].value_counts()\n",
        "fraud_pct = df[TARGET_COLUMN].value_counts(normalize=True) * 100\n",
        "\n",
        "print(\"\\nüìä Original Dataset Distribution:\")\n",
        "print(f\"\\nNon-Fraud (0): {fraud_counts[0]:,} ({fraud_pct[0]:.2f}%)\")\n",
        "print(f\"Fraud (1):     {fraud_counts[1]:,} ({fraud_pct[1]:.2f}%)\")\n",
        "print(f\"\\nImbalance Ratio: 1:{fraud_counts[0]/fraud_counts[1]:.1f}\")\n",
        "print(f\"Total samples: {len(df):,}\")\n",
        "\n",
        "# Visualization\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# Bar plot\n",
        "fraud_counts.plot(kind='bar', ax=axes[0], color=['#2ecc71', '#e74c3c'])\n",
        "axes[0].set_title('Class Distribution (Count)', fontsize=14, fontweight='bold')\n",
        "axes[0].set_xlabel('Class (0=Non-Fraud, 1=Fraud)')\n",
        "axes[0].set_ylabel('Count')\n",
        "axes[0].set_xticklabels(['Non-Fraud', 'Fraud'], rotation=0)\n",
        "for i, v in enumerate(fraud_counts):\n",
        "    axes[0].text(i, v + max(fraud_counts)*0.02, f'{v:,}', ha='center', fontweight='bold')\n",
        "\n",
        "# Pie chart\n",
        "axes[1].pie(fraud_counts, labels=['Non-Fraud', 'Fraud'], autopct='%1.2f%%',\n",
        "            colors=['#2ecc71', '#e74c3c'], startangle=90, textprops={'fontweight': 'bold'})\n",
        "axes[1].set_title('Class Distribution (Percentage)', fontsize=14, fontweight='bold')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n‚ö†Ô∏è This dataset is highly imbalanced - balancing is necessary!\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "trusted": true,
        "gather": {
          "logged": 1767558626553
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ‚úÇÔ∏è 4. Strategic Dataset Truncation\n",
        "\n",
        "**Strategy:**\n",
        "- Keep ALL fraud cases (minority class)\n",
        "- Sample 500,000 non-fraud cases (majority class)\n",
        "- This reduces computational load while preserving all fraud patterns"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"=\"*60)\n",
        "print(\"DATASET TRUNCATION\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Separate fraud and non-fraud cases\n",
        "fraud_cases = df[df[TARGET_COLUMN] == 1]\n",
        "non_fraud_cases = df[df[TARGET_COLUMN] == 0]\n",
        "\n",
        "print(f\"\\nüìä Original Split:\")\n",
        "print(f\"Fraud cases:     {len(fraud_cases):,}\")\n",
        "print(f\"Non-fraud cases: {len(non_fraud_cases):,}\")\n",
        "\n",
        "# Keep all frauds, sample non-frauds\n",
        "NON_FRAUD_SAMPLE_SIZE = 500_000\n",
        "\n",
        "print(f\"\\n‚úÇÔ∏è Truncation Strategy:\")\n",
        "print(f\"‚úì Keep ALL {len(fraud_cases):,} fraud cases\")\n",
        "print(f\"‚úì Sample {NON_FRAUD_SAMPLE_SIZE:,} non-fraud cases\")\n",
        "\n",
        "# Random sampling of non-fraud cases\n",
        "non_fraud_sampled = non_fraud_cases.sample(\n",
        "    n=min(NON_FRAUD_SAMPLE_SIZE, len(non_fraud_cases)),\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Combine truncated datasets\n",
        "df_truncated = pd.concat([fraud_cases, non_fraud_sampled], axis=0)\n",
        "df_truncated = df_truncated.sample(frac=1, random_state=42).reset_index(drop=True)  # Shuffle\n",
        "\n",
        "print(f\"\\n‚úÖ Truncation Complete!\")\n",
        "print(f\"\\nüìä Truncated Dataset:\")\n",
        "print(f\"Total rows: {len(df_truncated):,}\")\n",
        "print(f\"Fraud:      {len(df_truncated[df_truncated[TARGET_COLUMN] == 1]):,}\")\n",
        "print(f\"Non-fraud:  {len(df_truncated[df_truncated[TARGET_COLUMN] == 0]):,}\")\n",
        "\n",
        "truncated_ratio = len(df_truncated[df_truncated[TARGET_COLUMN] == 0]) / len(df_truncated[df_truncated[TARGET_COLUMN] == 1])\n",
        "print(f\"\\nNew Imbalance Ratio: 1:{truncated_ratio:.1f}\")\n",
        "print(f\"Size reduction: {100 * (1 - len(df_truncated)/len(df)):.1f}%\")\n",
        "\n",
        "# Memory cleanup\n",
        "del df, fraud_cases, non_fraud_cases, non_fraud_sampled\n",
        "import gc\n",
        "gc.collect()\n",
        "\n",
        "print(\"\\nüßπ Memory cleaned up\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "trusted": true,
        "gather": {
          "logged": 1767558634216
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üîß 5. Feature Engineering & Preprocessing"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "!{sys.executable} -m pip install category_encoders"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1765915186158
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Assuming df_truncated is already loaded\n",
        "TARGET_COLUMN = 'isFraud'  # Replace with your actual target column name\n",
        "\n",
        "# 1Ô∏è‚É£ Identify categorical columns\n",
        "categorical_cols = df_truncated.select_dtypes(include=['object', 'category']).columns\n",
        "numeric_cols = df_truncated.select_dtypes(include=[np.number]).columns\n",
        "\n",
        "# 2Ô∏è‚É£ Handle missing values\n",
        "print(\"=\"*60)\n",
        "print(\"FEATURE ENGINEERING - Missing Values\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "for col in numeric_cols:\n",
        "    if df_truncated[col].isnull().sum() > 0:\n",
        "        df_truncated[col].fillna(df_truncated[col].median(), inplace=True)\n",
        "        print(f\"Filled missing values in {col} with median\")\n",
        "\n",
        "for col in categorical_cols:\n",
        "    if df_truncated[col].isnull().sum() > 0:\n",
        "        df_truncated[col].fillna(df_truncated[col].mode()[0], inplace=True)\n",
        "        print(f\"Filled missing values in {col} with mode\")\n",
        "\n",
        "# 3Ô∏è‚É£ Encode categorical columns using Target Encoding\n",
        "!pip install -q category_encoders\n",
        "import category_encoders as ce\n",
        "\n",
        "if len(categorical_cols) > 0:\n",
        "    print(f\"\\nüî§ Encoding {len(categorical_cols)} categorical columns using Target Encoding...\")\n",
        "    encoder = ce.TargetEncoder(cols=categorical_cols)\n",
        "    df_truncated[categorical_cols] = encoder.fit_transform(df_truncated[categorical_cols], df_truncated[TARGET_COLUMN])\n",
        "    print(\"‚úì Encoding complete\")\n",
        "else:\n",
        "    print(\"No categorical columns to encode\")\n",
        "\n",
        "# 4Ô∏è‚É£ Optional: Scale numeric columns\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "scaler = StandardScaler()\n",
        "df_truncated[numeric_cols] = scaler.fit_transform(df_truncated[numeric_cols])\n",
        "df_processed = df_truncated.copy()\n",
        "\n",
        "print(\"\\n‚úì Feature scaling complete\")\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "trusted": true,
        "gather": {
          "logged": 1767558645183
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üéØ 6. Data Balancing with SMOTE\n",
        "\n",
        "**Multiple balancing strategies available:**\n",
        "- **SMOTE**: Synthetic Minority Over-sampling Technique\n",
        "- **ADASYN**: Adaptive Synthetic Sampling\n",
        "- **BorderlineSMOTE**: Focus on borderline cases\n",
        "- **SMOTETomek**: SMOTE + Tomek Links cleaning\n",
        "\n",
        "We'll use **SMOTE** with customizable ratio (40:60, 45:55, or 50:50)"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "import imblearn\n",
        "from imblearn.over_sampling import SMOTE\n",
        "print(imblearn.__file__)\n",
        "print(SMOTE)\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1767558655186
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Split original dataset BEFORE balancing\n",
        "X_train_orig, X_test_orig, y_train_orig, y_test_orig = train_test_split(\n",
        "    X, y,\n",
        "    test_size=0.2,\n",
        "    random_state=42,\n",
        "    stratify=y\n",
        ")\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1767558657867
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from imblearn.over_sampling import SMOTE\n",
        "print(\"=\"*60)\n",
        "print(\"DATA BALANCING WITH SMOTE\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Separate features and target\n",
        "X = df_processed.drop(columns=[TARGET_COLUMN])\n",
        "y = df_processed[TARGET_COLUMN]\n",
        "\n",
        "print(f\"\\nüìä Before Balancing:\")\n",
        "print(f\"Features shape: {X.shape}\")\n",
        "print(f\"Target distribution:\")\n",
        "print(y.value_counts())\n",
        "print(f\"Fraud ratio: {100 * y.mean():.2f}%\")\n",
        "\n",
        "# Choose your desired balance ratio\n",
        "# Options: 0.4 (40%), 0.45 (45%), 0.5 (50%)\n",
        "DESIRED_FRAUD_RATIO = 0.40  # ‚ö†Ô∏è ADJUST THIS (0.40 = 40/60 split)\n",
        "\n",
        "# Calculate sampling strategy\n",
        "# sampling_strategy: ratio of minority class to majority class after resampling\n",
        "# For 40% fraud: we want fraud/(non-fraud) = 0.4/0.6 = 0.667\n",
        "sampling_strategy = DESIRED_FRAUD_RATIO / (1 - DESIRED_FRAUD_RATIO)\n",
        "\n",
        "print(f\"\\nüéØ Target Balance:\")\n",
        "print(f\"Desired fraud ratio: {DESIRED_FRAUD_RATIO*100:.0f}%\")\n",
        "print(f\"Sampling strategy: {sampling_strategy:.3f}\")\n",
        "\n",
        "# Apply SMOTE\n",
        "print(f\"\\n‚öôÔ∏è Applying SMOTE...\")\n",
        "smote = SMOTE(\n",
        "    sampling_strategy=sampling_strategy,\n",
        "    random_state=42,\n",
        "    k_neighbors=5)\n",
        "\n",
        "X_balanced, y_balanced = smote.fit_resample(X, y)\n",
        "\n",
        "print(f\"\\n‚úÖ SMOTE Complete!\")\n",
        "print(f\"\\nüìä After Balancing:\")\n",
        "print(f\"Features shape: {X_balanced.shape}\")\n",
        "print(f\"Target distribution:\")\n",
        "print(pd.Series(y_balanced).value_counts())\n",
        "fraud_pct_balanced = 100 * pd.Series(y_balanced).mean()\n",
        "print(f\"\\nFraud ratio: {fraud_pct_balanced:.2f}%\")\n",
        "print(f\"Total samples: {len(X_balanced):,}\")\n",
        "print(f\"Synthetic samples created: {len(X_balanced) - len(X):,}\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "trusted": true,
        "gather": {
          "logged": 1765979168672
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize the balancing effect\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# Before SMOTE\n",
        "y.value_counts().plot(kind='bar', ax=axes[0], color=['#2ecc71', '#e74c3c'])\n",
        "axes[0].set_title('Before SMOTE', fontsize=14, fontweight='bold')\n",
        "axes[0].set_xlabel('Class')\n",
        "axes[0].set_ylabel('Count')\n",
        "axes[0].set_xticklabels(['Non-Fraud', 'Fraud'], rotation=0)\n",
        "for i, v in enumerate(y.value_counts()):\n",
        "    axes[0].text(i, v + max(y.value_counts())*0.02, f'{v:,}', ha='center', fontweight='bold')\n",
        "\n",
        "# After SMOTE\n",
        "pd.Series(y_balanced).value_counts().plot(kind='bar', ax=axes[1], color=['#2ecc71', '#e74c3c'])\n",
        "axes[1].set_title('After SMOTE', fontsize=14, fontweight='bold')\n",
        "axes[1].set_xlabel('Class')\n",
        "axes[1].set_ylabel('Count')\n",
        "axes[1].set_xticklabels(['Non-Fraud', 'Fraud'], rotation=0)\n",
        "for i, v in enumerate(pd.Series(y_balanced).value_counts()):\n",
        "    axes[1].text(i, v + max(pd.Series(y_balanced).value_counts())*0.02, f'{v:,}', ha='center', fontweight='bold')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nüìà Dataset is now balanced and ready for training!\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "trusted": true,
        "gather": {
          "logged": 1765979172124
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_balanced = X_balanced.drop(columns=leakage_cols)\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1765979177524
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üîÄ 7. Train-Test Split & Scaling"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "leakage_cols = ['nameOrig', 'nameDest', 'isFlaggedFraud']\n",
        "X = X.drop(columns=leakage_cols)\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1765979180191
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "smote = SMOTE(sampling_strategy=sampling_strategy, random_state=42, k_neighbors=5)\n",
        "X_train_bal, y_train_bal = smote.fit_resample(X_train_orig, y_train_orig)\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1765979193569
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"=\"*60)\n",
        "print(\"TRAIN-TEST SPLIT & FEATURE SCALING\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Split the balanced dataset\n",
        "TEST_SIZE = 0.2  # 80% train, 20% test\n",
        "RANDOM_STATE = 42\n",
        "\n",
        "# 60% train, 20% validation, 20% test\n",
        "X_temp, X_test, y_temp, y_test = train_test_split(\n",
        "    X_balanced, y_balanced,\n",
        "    test_size=0.2,\n",
        "    random_state=42,\n",
        "    stratify=y_balanced\n",
        ")\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    X_temp, y_temp,\n",
        "    test_size=0.25,  # 0.25 * 0.8 = 0.2\n",
        "    random_state=42,\n",
        "    stratify=y_temp\n",
        ")\n",
        "\n",
        "scaler = RobustScaler()\n",
        "\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_val_scaled   = scaler.transform(X_val)\n",
        "X_test_scaled  = scaler.transform(X_test)\n",
        "print(f\"\\n‚úÖ Data split complete:\")\n",
        "print(f\"\\nTraining set:   {X_train.shape[0]:,} samples ({100*(1-TEST_SIZE):.0f}%)\")\n",
        "print(f\"Test set:       {X_test.shape[0]:,} samples ({100*TEST_SIZE:.0f}%)\")\n",
        "print(f\"Number of features: {X_train.shape[1]}\")\n",
        "\n",
        "print(f\"\\nüìä Class distribution:\")\n",
        "print(f\"Train - Fraud: {pd.Series(y_train).mean()*100:.2f}%\")\n",
        "print(f\"Test  - Fraud: {pd.Series(y_test).mean()*100:.2f}%\")\n",
        "\n",
        "# Feature Scaling (Important for many algorithms)\n",
        "print(f\"\\n‚öôÔ∏è Applying feature scaling...\")\n",
        "\n",
        "# RobustScaler is better for data with outliers (common in fraud detection)\n",
        "\n",
        "print(\"Features utilis√©es :\", X_train.columns.tolist())\n",
        "\n",
        "# R√©cup√©ration des noms de features pour l'analyse apr√®s scaling\n",
        "if hasattr(X_train, 'columns'):\n",
        "    feature_names = X_train.columns\n",
        "else:\n",
        "    feature_names = [f'feature_{i}' for i in range(X_train_scaled.shape[1])]\n",
        "\n",
        "# Cr√©er DataFrame pour statistiques\n",
        "X_train_scaled_df = pd.DataFrame(X_train_scaled, columns=feature_names)\n",
        "\n",
        "# Alternative: StandardScaler() for normal distribution\n",
        "\n",
        "\n",
        "print(\"‚úÖ Scaling complete using RobustScaler\")\n",
        "print(\"\\nüì¶ Data is now ready for model training!\")\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "trusted": true,
        "gather": {
          "logged": 1765979205881
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ü§ñ 8. Model Training & Evaluation\n",
        "\n",
        "We'll train multiple models and compare their performance:\n",
        "1. Logistic Regression (baseline)\n",
        "2. Random Forest\n",
        "3. Gradient Boosting\n",
        "4. XGBoost\n",
        "5. LightGBM"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# PARTIE 1: CONFIGURATION AZURE ML + MLFLOW\n",
        "# =============================================================================\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\" \"*20 + \"üöÄ AZURE ML - MODEL TRAINING PIPELINE\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Imports essentiels\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datetime import datetime\n",
        "import json\n",
        "import os\n",
        "\n",
        "# Azure ML\n",
        "from azureml.core import Workspace, Experiment, Run, Dataset, Model\n",
        "from azureml.core.compute import ComputeTarget\n",
        "import mlflow\n",
        "import mlflow.sklearn\n",
        "from mlflow.tracking import MlflowClient\n",
        "\n",
        "# Machine Learning\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from xgboost import XGBClassifier\n",
        "from lightgbm import LGBMClassifier\n",
        "\n",
        "# M√©triques\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, precision_score, recall_score, f1_score,\n",
        "    roc_auc_score, confusion_matrix, classification_report,\n",
        "    roc_curve, precision_recall_curve\n",
        ")\n",
        "\n",
        "# Visualisation\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "sns.set_style('whitegrid')\n",
        "\n",
        "print(\"\\n‚úÖ Toutes les biblioth√®ques import√©es avec succ√®s!\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1765979218637
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# CONNEXION AU WORKSPACE AZURE ML\n",
        "# =============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"üì° CONNEXION √Ä AZURE ML WORKSPACE\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "try:\n",
        "    # M√©thode 1: Depuis le contexte du run (si dans Azure ML Compute)\n",
        "    run_context = Run.get_context()\n",
        "    \n",
        "    if hasattr(run_context, 'experiment'):\n",
        "        ws = run_context.experiment.workspace\n",
        "        print(\"‚úÖ Connect√© depuis Azure ML Compute Instance\")\n",
        "    else:\n",
        "        # M√©thode 2: Depuis config.json local\n",
        "        ws = Workspace.from_config()\n",
        "        print(\"‚úÖ Connect√© depuis configuration locale\")\n",
        "    \n",
        "    print(f\"\\nüìã Workspace Details:\")\n",
        "    print(f\"   Nom: {ws.name}\")\n",
        "    print(f\"   R√©gion: {ws.location}\")\n",
        "    print(f\"   Resource Group: {ws.resource_group}\")\n",
        "    print(f\"   Subscription ID: {ws.subscription_id[:8]}...\")\n",
        "    \n",
        "    # Configuration MLflow pour Azure ML\n",
        "    mlflow_uri = ws.get_mlflow_tracking_uri()\n",
        "    mlflow.set_tracking_uri(mlflow_uri)\n",
        "    print(f\"\\nüîó MLflow Tracking URI configur√©: {mlflow_uri}\")\n",
        "    \n",
        "    AZURE_ML_CONNECTED = True\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"‚ö†Ô∏è Erreur de connexion: {str(e)}\")\n",
        "    print(\"üí° Utilisation du mode local pour d√©monstration\")\n",
        "    AZURE_ML_CONNECTED = False\n",
        "    mlflow.set_tracking_uri(\"./mlruns\")\n",
        "\n",
        "# Cr√©er/R√©cup√©rer l'exp√©rience\n",
        "EXPERIMENT_NAME = \"fraud-detection-training\"\n",
        "experiment = Experiment(workspace=ws, name=EXPERIMENT_NAME) if AZURE_ML_CONNECTED else None\n",
        "\n",
        "if AZURE_ML_CONNECTED:\n",
        "    print(f\"\\nüß™ Exp√©rience: {EXPERIMENT_NAME}\")\n",
        "    mlflow.set_experiment(EXPERIMENT_NAME)\n",
        "else:\n",
        "    mlflow.set_experiment(\"fraud-detection-local\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1765979341510
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# PARTIE 2: CHARGEMENT DES DONN√âES (depuis Azure Blob Storage)\n",
        "# =============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"üìÇ CHARGEMENT DES DONN√âES DEPUIS AZURE BLOB STORAGE\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Supposons que X_train_scaled, X_test_scaled, y_train, y_test sont d√©j√† pr√©par√©s\n",
        "# Si ce n'est pas le cas, d√©commentez le code ci-dessous:\n",
        "\n",
        "\"\"\"\n",
        "# Charger depuis Azure Blob Storage\n",
        "datastore = ws.get_default_datastore()\n",
        "dataset = Dataset.Tabular.from_delimited_files(\n",
        "    path=[(datastore, 'fraud_dataset_processed.csv')]\n",
        ")\n",
        "df = dataset.to_pandas_dataframe()\n",
        "\n",
        "# S√©parer features et target\n",
        "X = df.drop(columns=['isFraud'])\n",
        "y = df['isFraud']\n",
        "\n",
        "# Split train/test\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "# Scaling\n",
        "from sklearn.preprocessing import RobustScaler\n",
        "scaler = RobustScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\"\"\"\n",
        "\n",
        "print(f\"‚úÖ Donn√©es charg√©es:\")\n",
        "print(f\"   Training samples: {len(X_train_scaled):,}\")\n",
        "print(f\"   Test samples: {len(X_test_scaled):,}\")\n",
        "print(f\"   Features: {X_train_scaled.shape[1]}\")\n",
        "print(f\"   Fraud ratio (train): {pd.Series(y_train).mean()*100:.2f}%\")\n",
        "\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1765979346247
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# PARTIE 3: FONCTION D'√âVALUATION COMPL√àTE\n",
        "# =============================================================================\n",
        "\n",
        "def evaluate_model_comprehensive(model, X_test, y_test, model_name=\"Model\"):\n",
        "    \"\"\"\n",
        "    √âvaluation compl√®te du mod√®le avec visualisations professionnelles\n",
        "    \n",
        "    Returns:\n",
        "        dict: M√©triques de performance\n",
        "        tuple: Figures matplotlib pour logging\n",
        "    \"\"\"\n",
        "    \n",
        "    # Pr√©dictions\n",
        "    y_pred = model.predict(X_test)\n",
        "    y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
        "    \n",
        "    # Calcul des m√©triques\n",
        "    metrics = {\n",
        "        'accuracy': accuracy_score(y_test, y_pred),\n",
        "        'precision': precision_score(y_test, y_pred, zero_division=0),\n",
        "        'recall': recall_score(y_test, y_pred, zero_division=0),\n",
        "        'f1_score': f1_score(y_test, y_pred, zero_division=0),\n",
        "        'roc_auc': roc_auc_score(y_test, y_pred_proba)\n",
        "    }\n",
        "    \n",
        "    # Confusion Matrix\n",
        "    cm = confusion_matrix(y_test, y_pred)\n",
        "    \n",
        "    # M√©triques suppl√©mentaires pour fraude\n",
        "    tn, fp, fn, tp = cm.ravel()\n",
        "    metrics['true_negatives'] = int(tn)\n",
        "    metrics['false_positives'] = int(fp)\n",
        "    metrics['false_negatives'] = int(fn)\n",
        "    metrics['true_positives'] = int(tp)\n",
        "    metrics['fraud_detection_rate'] = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
        "    metrics['false_alarm_rate'] = fp / (fp + tn) if (fp + tn) > 0 else 0\n",
        "    \n",
        "    # Affichage des r√©sultats\n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(f\"üìä {model_name} - R√âSULTATS D'√âVALUATION\")\n",
        "    print(f\"{'='*70}\")\n",
        "    print(f\"Accuracy:  {metrics['accuracy']:.4f} ({metrics['accuracy']*100:.2f}%)\")\n",
        "    print(f\"Precision: {metrics['precision']:.4f} (Qualit√© des alertes fraude)\")\n",
        "    print(f\"Recall:    {metrics['recall']:.4f} (D√©tection des fraudes)\")\n",
        "    print(f\"F1-Score:  {metrics['f1_score']:.4f} (Balance Precision/Recall)\")\n",
        "    print(f\"ROC-AUC:   {metrics['roc_auc']:.4f} (Capacit√© de discrimination)\")\n",
        "    \n",
        "    print(f\"\\nüéØ M√©triques m√©tier sp√©cifiques:\")\n",
        "    print(f\"Taux de d√©tection fraude: {metrics['fraud_detection_rate']*100:.2f}%\")\n",
        "    print(f\"Taux de fausses alertes:  {metrics['false_alarm_rate']*100:.2f}%\")\n",
        "    \n",
        "    print(f\"\\nüî¢ Confusion Matrix:\")\n",
        "    print(f\"   TN: {tn:,}  |  FP: {fp:,}\")\n",
        "    print(f\"   FN: {fn:,}  |  TP: {tp:,}\")\n",
        "    \n",
        "    # Visualisations\n",
        "    fig = plt.figure(figsize=(18, 5))\n",
        "    \n",
        "    # 1. Confusion Matrix\n",
        "    ax1 = plt.subplot(1, 3, 1)\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
        "                xticklabels=['Non-Fraude', 'Fraude'],\n",
        "                yticklabels=['Non-Fraude', 'Fraude'],\n",
        "                ax=ax1, cbar_kws={'label': 'Nombre de cas'})\n",
        "    ax1.set_title(f'{model_name}\\nMatrice de Confusion', fontweight='bold', fontsize=12)\n",
        "    ax1.set_ylabel('Vraie Classe')\n",
        "    ax1.set_xlabel('Classe Pr√©dite')\n",
        "    \n",
        "    # 2. ROC Curve\n",
        "    ax2 = plt.subplot(1, 3, 2)\n",
        "    fpr, tpr, _ = roc_curve(y_test, y_pred_proba)\n",
        "    ax2.plot(fpr, tpr, linewidth=2.5, \n",
        "             label=f'ROC (AUC = {metrics[\"roc_auc\"]:.3f})', color='#e74c3c')\n",
        "    ax2.plot([0, 1], [0, 1], 'k--', linewidth=1.5, label='Hasard')\n",
        "    ax2.set_xlabel('Taux de Faux Positifs (FPR)')\n",
        "    ax2.set_ylabel('Taux de Vrais Positifs (TPR)')\n",
        "    ax2.set_title(f'{model_name}\\nCourbe ROC', fontweight='bold', fontsize=12)\n",
        "    ax2.legend(loc='lower right')\n",
        "    ax2.grid(alpha=0.3)\n",
        "    \n",
        "    # 3. Precision-Recall Curve\n",
        "    ax3 = plt.subplot(1, 3, 3)\n",
        "    precision_vals, recall_vals, _ = precision_recall_curve(y_test, y_pred_proba)\n",
        "    ax3.plot(recall_vals, precision_vals, linewidth=2.5, color='#2ecc71')\n",
        "    ax3.set_xlabel('Recall (Taux de d√©tection)')\n",
        "    ax3.set_ylabel('Precision (Qualit√© des alertes)')\n",
        "    ax3.set_title(f'{model_name}\\nCourbe Precision-Recall', fontweight='bold', fontsize=12)\n",
        "    ax3.grid(alpha=0.3)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show() \n",
        "    return metrics, fig\n",
        "\n",
        "print(\"\\n‚úÖ Fonction d'√©valuation configur√©e\")\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1765979354267
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "leakage_cols = ['nameOrig', 'nameDest', 'isFlaggedFraud']\n",
        "X = X.drop(columns=leakage_cols)\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1765975404462
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# PARTIE 4: CONFIGURATION DES MOD√àLES (VERSION CORRIG√âE)\n",
        "# =============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"ü§ñ CONFIGURATION DES MOD√àLES DE MACHINE LEARNING\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# IMPORTANT: V√©rifier d'abord le d√©s√©quilibre r√©el de vos donn√©es\n",
        "print(f\"\\nüìä Analyse du d√©s√©quilibre des classes:\")\n",
        "fraud_count = y_train.sum()\n",
        "total_count = len(y_train)\n",
        "fraud_ratio = fraud_count / total_count\n",
        "print(f\"   ‚Ä¢ Transactions frauduleuses: {fraud_count:,} ({fraud_ratio*100:.2f}%)\")\n",
        "print(f\"   ‚Ä¢ Transactions l√©gitimes: {total_count - fraud_count:,} ({(1-fraud_ratio)*100:.2f}%)\")\n",
        "print(f\"   ‚Ä¢ Ratio d√©s√©quilibre: 1:{int(1/fraud_ratio)}\")\n",
        "\n",
        "# Calculer le scale_pos_weight optimal pour XGBoost\n",
        "# scale_pos_weight = (nombre de n√©gatifs) / (nombre de positifs)\n",
        "scale_pos_weight = (total_count - fraud_count) / fraud_count\n",
        "print(f\"   ‚Ä¢ scale_pos_weight optimal: {scale_pos_weight:.2f}\")\n",
        "\n",
        "# Configuration des mod√®les avec hyperparam√®tres CORRIG√âS\n",
        "models_config = {\n",
        "    'Logistic_Regression': {\n",
        "        'model': LogisticRegression(\n",
        "            max_iter=1000,\n",
        "            random_state=42,\n",
        "            n_jobs=-1,\n",
        "            class_weight='balanced',  # OK pour la r√©gression logistique\n",
        "            solver='saga',\n",
        "            penalty='l2',\n",
        "            C=1.0  # R√©gularisation standard\n",
        "        ),\n",
        "        'description': 'Mod√®le lin√©aire baseline - Rapide et interpr√©table',\n",
        "        'color': '#3498db'\n",
        "    },\n",
        "    \n",
        "    'Random_Forest': {\n",
        "        'model': RandomForestClassifier(\n",
        "            n_estimators=100,  # R√©duit de 150 √† 100\n",
        "            max_depth=15,      # R√©duit de 20 √† 15 (√©vite l'overfitting)\n",
        "            min_samples_split=10,  # Augment√© de 5 √† 10\n",
        "            min_samples_leaf=4,    # Augment√© de 2 √† 4\n",
        "            max_features='sqrt',\n",
        "            random_state=42,\n",
        "            n_jobs=-1,\n",
        "            class_weight='balanced_subsample',  # Mieux que 'balanced'\n",
        "            verbose=0,\n",
        "            max_samples=0.8  # Sous-√©chantillonnage pour √©viter overfitting\n",
        "        ),\n",
        "        'description': 'Ensemble d\\'arbres - Robuste et performant',\n",
        "        'color': '#2ecc71'\n",
        "    },\n",
        "    \n",
        "    'XGBoost': {\n",
        "        'model': XGBClassifier(\n",
        "            n_estimators=100,  # R√©duit de 150\n",
        "            max_depth=6,       # R√©duit de 10 √† 6 (XGBoost recommande 3-10)\n",
        "            learning_rate=0.05, # R√©duit de 0.1 √† 0.05 (apprentissage plus lent)\n",
        "            subsample=0.8,\n",
        "            colsample_bytree=0.8,\n",
        "            gamma=0.1,\n",
        "            min_child_weight=3,  # Ajout√© pour √©viter overfitting\n",
        "            scale_pos_weight=min(scale_pos_weight, 10),  # Limit√© √† max 10\n",
        "            random_state=42,\n",
        "            n_jobs=-1,\n",
        "            eval_metric='logloss',\n",
        "            verbosity=0,\n",
        "            reg_alpha=0.1,  # R√©gularisation L1\n",
        "            reg_lambda=1.0  # R√©gularisation L2\n",
        "        ),\n",
        "        'description': 'Gradient Boosting optimis√© - Tr√®s haute performance',\n",
        "        'color': '#e74c3c'\n",
        "    },\n",
        "    \n",
        "    'LightGBM': {\n",
        "        'model': LGBMClassifier(\n",
        "            n_estimators=100,\n",
        "            max_depth=6,      # R√©duit de 10\n",
        "            learning_rate=0.05, # R√©duit de 0.1\n",
        "            subsample=0.8,\n",
        "            colsample_bytree=0.8,\n",
        "            num_leaves=31,\n",
        "            min_child_samples=30,  # Augment√© de 20 √† 30\n",
        "            min_child_weight=0.001,\n",
        "            random_state=42,\n",
        "            n_jobs=-1,\n",
        "            verbose=-1,\n",
        "            is_unbalance=True,  # OK pour LightGBM\n",
        "            reg_alpha=0.1,   # R√©gularisation L1\n",
        "            reg_lambda=1.0   # R√©gularisation L2\n",
        "        ),\n",
        "        'description': 'Gradient Boosting l√©ger - Rapide et efficace',\n",
        "        'color': '#f39c12'\n",
        "    }\n",
        "}\n",
        "\n",
        "print(f\"\\nüìã {len(models_config)} mod√®les configur√©s avec hyperparam√®tres optimis√©s:\")\n",
        "for name, config in models_config.items():\n",
        "    print(f\"   ‚úì {name}: {config['description']}\")\n",
        "\n",
        "print(\"\\n‚öôÔ∏è Ajustements cl√©s pour √©viter les faux positifs:\")\n",
        "print(\"   ‚Ä¢ Profondeur d'arbres r√©duite (√©vite overfitting)\")\n",
        "print(\"   ‚Ä¢ Learning rate plus faible (convergence stable)\")\n",
        "print(\"   ‚Ä¢ R√©gularisation L1/L2 ajout√©e\")\n",
        "print(\"   ‚Ä¢ Min samples augment√©s (g√©n√©ralisation)\")\n",
        "print(\"   ‚Ä¢ scale_pos_weight limit√© (√©vite biais extr√™me)\")\n",
        "\n",
        "# =============================================================================\n",
        "# AJOUT: FONCTION POUR AJUSTER LE SEUIL DE D√âCISION\n",
        "# =============================================================================\n",
        "\n",
        "def find_optimal_threshold(y_true, y_pred_proba, metric='f1'):\n",
        "    \"\"\"\n",
        "    Trouve le seuil optimal pour la classification\n",
        "    au lieu d'utiliser le seuil par d√©faut de 0.5\n",
        "    \n",
        "    Args:\n",
        "        y_true: Vraies √©tiquettes\n",
        "        y_pred_proba: Probabilit√©s pr√©dites\n",
        "        metric: 'f1', 'precision', 'recall', ou 'balanced'\n",
        "    \n",
        "    Returns:\n",
        "        float: Seuil optimal\n",
        "    \"\"\"\n",
        "    from sklearn.metrics import precision_recall_curve, f1_score\n",
        "    \n",
        "    thresholds = np.arange(0.1, 0.9, 0.01)\n",
        "    scores = []\n",
        "    \n",
        "    for thresh in thresholds:\n",
        "        y_pred = (y_pred_proba >= thresh).astype(int)\n",
        "        \n",
        "        if metric == 'f1':\n",
        "            score = f1_score(y_true, y_pred)\n",
        "        elif metric == 'precision':\n",
        "            score = precision_score(y_true, y_pred, zero_division=0)\n",
        "        elif metric == 'recall':\n",
        "            score = recall_score(y_true, y_pred, zero_division=0)\n",
        "        elif metric == 'balanced':\n",
        "            # Balance entre precision et recall\n",
        "            prec = precision_score(y_true, y_pred, zero_division=0)\n",
        "            rec = recall_score(y_true, y_pred, zero_division=0)\n",
        "            score = 2 * (prec * rec) / (prec + rec + 1e-10)\n",
        "        \n",
        "        scores.append(score)\n",
        "    \n",
        "    optimal_idx = np.argmax(scores)\n",
        "    optimal_threshold = thresholds[optimal_idx]\n",
        "    \n",
        "    return optimal_threshold, scores[optimal_idx]\n",
        "\n",
        "print(\"\\n‚úÖ Configuration termin√©e!\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# =============================================================================\n",
        "# BONUS: AFFICHER LES STATISTIQUES DES FEATURES\n",
        "# =============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"üìä STATISTIQUES DES FEATURES (apr√®s scaling)\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Cr√©er un DataFrame avec les features scal√©es\n",
        "X_train_scaled_df = pd.DataFrame(\n",
        "    X_train_scaled,\n",
        "    columns=feature_names\n",
        ")\n",
        "\n",
        "print(\"\\nüìà Top 5 features avec variance la plus √©lev√©e:\")\n",
        "variances = X_train_scaled_df.var().sort_values(ascending=False)\n",
        "for i, (feature, var) in enumerate(variances.head(5).items(), 1):\n",
        "    print(f\"   {i}. {feature}: {var:.4f}\")\n",
        "\n",
        "print(\"\\nüìâ Statistiques descriptives:\")\n",
        "print(X_train_scaled_df.describe().round(4))\n",
        "\n",
        "# V√©rifier s'il y a des valeurs extr√™mes\n",
        "print(\"\\n‚ö†Ô∏è D√©tection de valeurs extr√™mes (|z-score| > 3):\")\n",
        "outliers_per_feature = (np.abs(X_train_scaled_df) > 3).sum()\n",
        "for feature, count in outliers_per_feature[outliers_per_feature > 0].items():\n",
        "    percentage = (count / len(X_train_scaled_df)) * 100\n",
        "    print(f\"   ‚Ä¢ {feature}: {count:,} outliers ({percentage:.2f}%)\")\n",
        "\n",
        "print(\"=\"*80)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1765979361310
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# ENTRA√éNEMENT DES MOD√àLES AVEC AJUSTEMENT DU SEUIL\n",
        "# =============================================================================\n",
        "from datetime import datetime\n",
        "from sklearn.metrics import classification_report, roc_auc_score, precision_recall_curve\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"üéØ ENTRA√éNEMENT DES MOD√àLES AVEC OPTIMISATION DU SEUIL\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Dictionnaire pour stocker les r√©sultats\n",
        "results = {}\n",
        "optimal_thresholds = {}\n",
        "\n",
        "for model_name, config in models_config.items():\n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(f\"üöÄ Entra√Ænement: {model_name}\")\n",
        "    print(f\"   {config['description']}\")\n",
        "    print(f\"{'='*80}\")\n",
        "    \n",
        "    # 1. ENTRA√éNEMENT\n",
        "    print(f\"\\n‚è≥ Entra√Ænement en cours...\")\n",
        "    start_time = datetime.now()\n",
        "    \n",
        "    model = config['model']\n",
        "    model.fit(X_train_scaled, y_train)\n",
        "    \n",
        "    training_time = (datetime.now() - start_time).total_seconds()\n",
        "    print(f\"‚úÖ Entra√Ænement termin√© en {training_time:.2f}s\")\n",
        "    \n",
        "    # 2. PR√âDICTIONS SUR VALIDATION\n",
        "    print(f\"\\nüìä Pr√©dictions sur validation set...\")\n",
        "    y_val_proba = model.predict_proba(X_val_scaled)[:, 1]\n",
        "    \n",
        "    # 3. TROUVER LE SEUIL OPTIMAL\n",
        "    print(f\"\\nüéØ Recherche du seuil optimal...\")\n",
        "    \n",
        "    # Tester diff√©rents seuils\n",
        "    thresholds_to_test = np.arange(0.1, 0.9, 0.01)\n",
        "    f1_scores = []\n",
        "    \n",
        "    for thresh in thresholds_to_test:\n",
        "        y_pred_thresh = (y_val_proba >= thresh).astype(int)\n",
        "        f1 = f1_score(y_val, y_pred_thresh)\n",
        "        f1_scores.append(f1)\n",
        "    \n",
        "    # Trouver le meilleur seuil\n",
        "    optimal_idx = np.argmax(f1_scores)\n",
        "    optimal_threshold = thresholds_to_test[optimal_idx]\n",
        "    optimal_f1 = f1_scores[optimal_idx]\n",
        "    \n",
        "    print(f\"   ‚úÖ Seuil optimal trouv√©: {optimal_threshold:.3f}\")\n",
        "    print(f\"   ‚úÖ F1-Score optimal: {optimal_f1:.4f}\")\n",
        "    \n",
        "    optimal_thresholds[model_name] = optimal_threshold\n",
        "    \n",
        "    # 4. PR√âDICTIONS AVEC SEUIL OPTIMAL\n",
        "    y_val_pred_optimal = (y_val_proba >= optimal_threshold).astype(int)\n",
        "    \n",
        "    # 5. PR√âDICTIONS AVEC SEUIL PAR D√âFAUT (0.5) pour comparaison\n",
        "    y_val_pred_default = (y_val_proba >= 0.5).astype(int)\n",
        "    \n",
        "    # 6. M√âTRIQUES AVEC SEUIL OPTIMAL\n",
        "    print(f\"\\nüìà M√©triques avec seuil optimal ({optimal_threshold:.3f}):\")\n",
        "    \n",
        "    accuracy_optimal = accuracy_score(y_val, y_val_pred_optimal)\n",
        "    precision_optimal = precision_score(y_val, y_val_pred_optimal)\n",
        "    recall_optimal = recall_score(y_val, y_val_pred_optimal)\n",
        "    f1_optimal = f1_score(y_val, y_val_pred_optimal)\n",
        "    roc_auc_optimal = roc_auc_score(y_val, y_val_proba)\n",
        "    \n",
        "    print(f\"   ‚Ä¢ Accuracy:  {accuracy_optimal:.4f}\")\n",
        "    print(f\"   ‚Ä¢ Precision: {precision_optimal:.4f}\")\n",
        "    print(f\"   ‚Ä¢ Recall:    {recall_optimal:.4f}\")\n",
        "    print(f\"   ‚Ä¢ F1-Score:  {f1_optimal:.4f}\")\n",
        "    print(f\"   ‚Ä¢ ROC-AUC:   {roc_auc_optimal:.4f}\")\n",
        "    \n",
        "    # 7. COMPARAISON AVEC SEUIL PAR D√âFAUT\n",
        "    print(f\"\\nüìä Comparaison avec seuil par d√©faut (0.5):\")\n",
        "    \n",
        "    accuracy_default = accuracy_score(y_val, y_val_pred_default)\n",
        "    precision_default = precision_score(y_val, y_val_pred_default)\n",
        "    recall_default = recall_score(y_val, y_val_pred_default)\n",
        "    f1_default = f1_score(y_val, y_val_pred_default)\n",
        "    \n",
        "    print(f\"   ‚Ä¢ Accuracy:  {accuracy_default:.4f} (Œî {accuracy_optimal - accuracy_default:+.4f})\")\n",
        "    print(f\"   ‚Ä¢ Precision: {precision_default:.4f} (Œî {precision_optimal - precision_default:+.4f})\")\n",
        "    print(f\"   ‚Ä¢ Recall:    {recall_default:.4f} (Œî {recall_optimal - recall_default:+.4f})\")\n",
        "    print(f\"   ‚Ä¢ F1-Score:  {f1_default:.4f} (Œî {f1_optimal - f1_default:+.4f})\")\n",
        "    \n",
        "    # 8. MATRICE DE CONFUSION\n",
        "    print(f\"\\nüìä Matrice de confusion (seuil optimal):\")\n",
        "    cm = confusion_matrix(y_val, y_val_pred_optimal)\n",
        "    print(f\"   TN: {cm[0,0]:,}  |  FP: {cm[0,1]:,}\")\n",
        "    print(f\"   FN: {cm[1,0]:,}  |  TP: {cm[1,1]:,}\")\n",
        "    \n",
        "    # Calculer les taux\n",
        "    tn, fp, fn, tp = cm.ravel()\n",
        "    fpr = fp / (fp + tn) if (fp + tn) > 0 else 0\n",
        "    fnr = fn / (fn + tp) if (fn + tp) > 0 else 0\n",
        "    \n",
        "    print(f\"\\n   ‚Ä¢ Taux de faux positifs (FPR): {fpr:.4f} ({fpr*100:.2f}%)\")\n",
        "    print(f\"   ‚Ä¢ Taux de faux n√©gatifs (FNR): {fnr:.4f} ({fnr*100:.2f}%)\")\n",
        "    \n",
        "    # 9. SAUVEGARDER LES R√âSULTATS\n",
        "    results[model_name] = {\n",
        "        'model': model,\n",
        "        'optimal_threshold': optimal_threshold,\n",
        "        'metrics_optimal': {\n",
        "            'accuracy': accuracy_optimal,\n",
        "            'precision': precision_optimal,\n",
        "            'recall': recall_optimal,\n",
        "            'f1_score': f1_optimal,\n",
        "            'roc_auc': roc_auc_optimal\n",
        "        },\n",
        "        'metrics_default': {\n",
        "            'accuracy': accuracy_default,\n",
        "            'precision': precision_default,\n",
        "            'recall': recall_default,\n",
        "            'f1_score': f1_default\n",
        "        },\n",
        "        'confusion_matrix': cm,\n",
        "        'training_time': training_time,\n",
        "        'y_val_proba': y_val_proba\n",
        "    }\n",
        "\n",
        "# =============================================================================\n",
        "# COMPARAISON FINALE DES MOD√àLES\n",
        "# =============================================================================\n",
        "\n",
        "print(\"\\n\\n\" + \"=\"*80)\n",
        "print(\"üèÜ COMPARAISON FINALE DES MOD√àLES\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "comparison_df = pd.DataFrame({\n",
        "    'Mod√®le': list(results.keys()),\n",
        "    'Seuil Optimal': [results[m]['optimal_threshold'] for m in results.keys()],\n",
        "    'Accuracy': [results[m]['metrics_optimal']['accuracy'] for m in results.keys()],\n",
        "    'Precision': [results[m]['metrics_optimal']['precision'] for m in results.keys()],\n",
        "    'Recall': [results[m]['metrics_optimal']['recall'] for m in results.keys()],\n",
        "    'F1-Score': [results[m]['metrics_optimal']['f1_score'] for m in results.keys()],\n",
        "    'ROC-AUC': [results[m]['metrics_optimal']['roc_auc'] for m in results.keys()],\n",
        "    'Temps (s)': [results[m]['training_time'] for m in results.keys()]\n",
        "})\n",
        "\n",
        "print(\"\\nüìä Tableau comparatif:\")\n",
        "print(comparison_df.to_string(index=False))\n",
        "\n",
        "# Trouver le meilleur mod√®le\n",
        "best_model_name = comparison_df.loc[comparison_df['F1-Score'].idxmax(), 'Mod√®le']\n",
        "print(f\"\\nüèÜ Meilleur mod√®le: {best_model_name}\")\n",
        "print(f\"   ‚Ä¢ F1-Score: {results[best_model_name]['metrics_optimal']['f1_score']:.4f}\")\n",
        "print(f\"   ‚Ä¢ Seuil optimal: {results[best_model_name]['optimal_threshold']:.3f}\")\n",
        "\n",
        "# =============================================================================\n",
        "# SAUVEGARDER LE MEILLEUR MOD√àLE AVEC SON SEUIL\n",
        "# =============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"üíæ SAUVEGARDE DU MEILLEUR MOD√àLE\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "best_model = results[best_model_name]['model']\n",
        "best_threshold = results[best_model_name]['optimal_threshold']\n",
        "\n",
        "# Sauvegarder le mod√®le\n",
        "joblib.dump(best_model, 'outputs/best_model.pkl')\n",
        "print(f\"‚úÖ Mod√®le sauvegard√©: outputs/best_model.pkl\")\n",
        "\n",
        "# Sauvegarder le scaler\n",
        "joblib.dump(scaler, 'outputs/scaler.pkl')\n",
        "print(f\"‚úÖ Scaler sauvegard√©: outputs/scaler.pkl\")\n",
        "\n",
        "# Sauvegarder les m√©tadonn√©es AVEC LE SEUIL OPTIMAL\n",
        "metadata = {\n",
        "    'best_model': best_model_name,\n",
        "    'optimal_threshold': best_threshold,  # ‚Üê IMPORTANT!\n",
        "    'feature_names': list(feature_names),\n",
        "    'all_models': {}\n",
        "}\n",
        "\n",
        "for model_name in results.keys():\n",
        "    metadata['all_models'][model_name] = {\n",
        "        'optimal_threshold': results[model_name]['optimal_threshold'],\n",
        "        'metrics': results[model_name]['metrics_optimal']\n",
        "    }\n",
        "\n",
        "with open('outputs/metadata.json', 'w') as f:\n",
        "    json.dump(metadata, f, indent=4)\n",
        "\n",
        "print(f\"‚úÖ M√©tadonn√©es sauvegard√©es: outputs/metadata.json\")\n",
        "print(f\"\\n‚ö° IMPORTANT: Seuil optimal = {best_threshold:.3f} (pas 0.5!)\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"‚úÖ ENTRA√éNEMENT TERMIN√â AVEC SUCC√àS!\")\n",
        "print(\"=\"*80)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1765979630929
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"üöÄ ENTRA√éNEMENT DES MOD√àLES (VERSION CORRIG√âE)\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Dictionnaire pour stocker les r√©sultats\n",
        "all_results = {}\n",
        "\n",
        "# Boucle d'entra√Ænement\n",
        "for model_name, config in models_config.items():\n",
        "    \n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(f\"üîµ Entra√Ænement: {model_name}\")\n",
        "    print(f\"   {config['description']}\")\n",
        "    print(f\"{'='*80}\")\n",
        "    \n",
        "    # D√©marrer un run MLflow\n",
        "    with mlflow.start_run(run_name=model_name) as run:\n",
        "        \n",
        "        # Obtenir le mod√®le\n",
        "        model = config['model']\n",
        "        \n",
        "        # ‚è±Ô∏è Entra√Ænement\n",
        "        print(\"‚è≥ Entra√Ænement en cours...\")\n",
        "        start_time = datetime.now()\n",
        "        \n",
        "        model.fit(X_train_scaled, y_train)\n",
        "        \n",
        "        training_time = (datetime.now() - start_time).total_seconds()\n",
        "        print(f\"‚úÖ Entra√Ænement termin√© en {training_time:.2f} secondes\")\n",
        "        \n",
        "        # üìä √âvaluation\n",
        "        print(\"\\nüìä √âvaluation du mod√®le...\")\n",
        "        metrics, viz_figure = evaluate_model_comprehensive(\n",
        "            model, X_test_scaled, y_test, model_name\n",
        "        )\n",
        "        plt.show()\n",
        "        # Ajouter le temps d'entra√Ænement\n",
        "        metrics['training_time_seconds'] = training_time\n",
        "        \n",
        "        # üìù Logging vers MLflow\n",
        "        print(\"\\nüìù Logging vers MLflow...\")\n",
        "        \n",
        "        # 1. Param√®tres\n",
        "        model_params = {\n",
        "            'model_type': model_name,\n",
        "            'algorithm': model.__class__.__name__,\n",
        "            'train_samples': len(X_train_scaled),\n",
        "            'test_samples': len(X_test_scaled),\n",
        "            'n_features': X_train_scaled.shape[1],\n",
        "            'training_time': training_time\n",
        "        }\n",
        "        \n",
        "        # Ajouter les hyperparam√®tres (filtrer les non-JSON serializable)\n",
        "        if hasattr(model, 'get_params'):\n",
        "            hyperparams = model.get_params()\n",
        "            for key, value in hyperparams.items():\n",
        "                if isinstance(value, (int, float, str, bool, type(None))):\n",
        "                    model_params[f'hp_{key}'] = value\n",
        "        \n",
        "        mlflow.log_params(model_params)\n",
        "        \n",
        "        # 2. M√©triques (filtrer pour ne garder que les nombres)\n",
        "        numeric_metrics = {\n",
        "            k: float(v) for k, v in metrics.items() \n",
        "            if isinstance(v, (int, float, np.integer, np.floating))\n",
        "        }\n",
        "        mlflow.log_metrics(numeric_metrics)\n",
        "        \n",
        "        # 3. Visualisations\n",
        "        mlflow.log_figure(viz_figure, f\"{model_name}_evaluation.png\")\n",
        "        plt.close(viz_figure)\n",
        "        \n",
        "        # 4. Classification Report\n",
        "        report = classification_report(\n",
        "            y_test, model.predict(X_test_scaled),\n",
        "            target_names=['Non-Fraude', 'Fraude']\n",
        "        )\n",
        "        with open(f\"{model_name}_report.txt\", \"w\") as f:\n",
        "            f.write(report)\n",
        "        mlflow.log_artifact(f\"{model_name}_report.txt\")\n",
        "        os.remove(f\"{model_name}_report.txt\")\n",
        "        \n",
        "        # 5. Sauvegarder le mod√®le (SANS registered_model_name pour √©viter l'erreur)\n",
        "        try:\n",
        "            mlflow.sklearn.log_model(\n",
        "                model, \n",
        "                \"model\"\n",
        "                # PAS de registered_model_name ici\n",
        "            )\n",
        "            print(\"‚úÖ Mod√®le sauvegard√© dans MLflow\")\n",
        "        except Exception as e:\n",
        "            print(f\"‚ö†Ô∏è Avertissement MLflow: {str(e)}\")\n",
        "            print(\"   Le mod√®le sera enregistr√© manuellement\")\n",
        "        \n",
        "        # 6. Tags pour organisation\n",
        "        mlflow.set_tags({\n",
        "            'project': 'CDDA-Fraud-Detection',\n",
        "            'framework': 'scikit-learn',\n",
        "            'use_case': 'fraud_detection',\n",
        "            'data_balancing': 'SMOTE',\n",
        "            'deployment_ready': 'true'\n",
        "        })\n",
        "        \n",
        "        print(f\"‚úÖ Run ID: {run.info.run_id}\")\n",
        "        \n",
        "        # Stocker les r√©sultats\n",
        "        all_results[model_name] = {\n",
        "            'model': model,\n",
        "            'metrics': metrics,\n",
        "            'run_id': run.info.run_id,\n",
        "            'color': config['color']\n",
        "        }\n",
        "    \n",
        "    print(f\"\\n{'='*80}\")\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1765974011614
        },
        "jupyter": {
          "source_hidden": true,
          "outputs_hidden": true
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "scaled_data = scaler.transform(input_data)\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1765978920493
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# ENREGISTREMENT MANUEL DU MEILLEUR MOD√àLE (apr√®s comparaison)\n",
        "# =============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"üíæ ENREGISTREMENT MANUEL DES MOD√àLES DANS AZURE ML\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Fonction pour enregistrer un mod√®le manuellement\n",
        "def register_model_manually(model, model_name, metrics, run_id):\n",
        "    \"\"\"\n",
        "    Enregistre un mod√®le dans Azure ML Model Registry de mani√®re manuelle\n",
        "    \"\"\"\n",
        "    try:\n",
        "        from azureml.core import Model\n",
        "        \n",
        "        # Cr√©er un dossier temporaire\n",
        "        temp_dir = f'temp_model_{model_name}'\n",
        "        os.makedirs(temp_dir, exist_ok=True)\n",
        "        \n",
        "        # Sauvegarder le mod√®le localement\n",
        "        model_path = os.path.join(temp_dir, 'model.pkl')\n",
        "        joblib.dump(model, model_path)\n",
        "        \n",
        "        # Enregistrer dans Azure ML\n",
        "        registered_model = Model.register(\n",
        "            workspace=ws,\n",
        "            model_path=temp_dir,\n",
        "            model_name=f'fraud_detection_{model_name.lower()}',\n",
        "            description=f'{model_name} - F1:{metrics[\"f1_score\"]:.4f} ROC-AUC:{metrics[\"roc_auc\"]:.4f}',\n",
        "            tags={\n",
        "                'model_type': model_name,\n",
        "                'f1_score': f'{metrics[\"f1_score\"]:.4f}',\n",
        "                'roc_auc': f'{metrics[\"roc_auc\"]:.4f}',\n",
        "                'accuracy': f'{metrics[\"accuracy\"]:.4f}',\n",
        "                'mlflow_run_id': run_id,\n",
        "                'deployment_ready': 'true'\n",
        "            },\n",
        "            model_framework='ScikitLearn'\n",
        "        )\n",
        "        \n",
        "        # Nettoyer\n",
        "        import shutil\n",
        "        shutil.rmtree(temp_dir)\n",
        "        \n",
        "        print(f\"‚úÖ {model_name} enregistr√©:\")\n",
        "        print(f\"   Nom: {registered_model.name}\")\n",
        "        print(f\"   Version: {registered_model.version}\")\n",
        "        print(f\"   ID: {registered_model.id}\")\n",
        "        \n",
        "        return registered_model\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Erreur lors de l'enregistrement de {model_name}: {str(e)}\")\n",
        "        return None\n",
        "\n",
        "# Enregistrer tous les mod√®les\n",
        "if AZURE_ML_CONNECTED:\n",
        "    print(\"\\nüìù Enregistrement de tous les mod√®les dans Azure ML...\")\n",
        "    registered_models = {}\n",
        "    \n",
        "    for model_name, results in all_results.items():\n",
        "        registered_model = register_model_manually(\n",
        "            model=results['model'],\n",
        "            model_name=model_name,\n",
        "            metrics=results['metrics'],\n",
        "            run_id=results['run_id']\n",
        "        )\n",
        "        if registered_model:\n",
        "            registered_models[model_name] = registered_model\n",
        "    \n",
        "    print(f\"\\n‚úÖ {len(registered_models)} mod√®les enregistr√©s avec succ√®s!\")\n",
        "else:\n",
        "    print(\"\\n‚ö†Ô∏è Mode local - enregistrement Azure ML d√©sactiv√©\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1765974324974
        },
        "jupyter": {
          "source_hidden": true
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# SAUVEGARDER LE MEILLEUR MOD√àLE LOCALEMENT POUR L'ENREGISTREMENT\n",
        "# =============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"üèÜ IDENTIFICATION ET SAUVEGARDE DU MEILLEUR MOD√àLE\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# 1. Trouver le meilleur mod√®le bas√© sur F1-Score ou ROC-AUC\n",
        "best_model_name = None\n",
        "best_score = 0\n",
        "best_metric = 'f1_score'  # ou 'roc_auc' selon votre pr√©f√©rence\n",
        "\n",
        "print(f\"\\nüìä Comparaison des mod√®les (crit√®re: {best_metric}):\")\n",
        "for model_name, results in all_results.items():\n",
        "    score = results['metrics'][best_metric]\n",
        "    print(f\"   {model_name}: {score:.4f}\")\n",
        "    \n",
        "    if score > best_score:\n",
        "        best_score = score\n",
        "        best_model_name = model_name\n",
        "\n",
        "print(f\"\\nü•á Meilleur mod√®le: {best_model_name} ({best_metric}={best_score:.4f})\")\n",
        "\n",
        "# 2. R√©cup√©rer le meilleur mod√®le\n",
        "best_model = all_results[best_model_name]['model']\n",
        "best_metrics = all_results[best_model_name]['metrics']\n",
        "\n",
        "# 3. Cr√©er le dossier outputs\n",
        "os.makedirs('outputs', exist_ok=True)\n",
        "\n",
        "# 4. Sauvegarder le meilleur mod√®le\n",
        "print(\"\\nüíæ Sauvegarde des artifacts dans /outputs...\")\n",
        "\n",
        "# Sauvegarder le mod√®le\n",
        "with open('outputs/best_model.pkl', 'wb') as f:\n",
        "    joblib.dump(best_model, f)\n",
        "print(\"   ‚úÖ best_model.pkl\")\n",
        "\n",
        "# Sauvegarder le scaler (si vous l'avez utilis√©)\n",
        "if 'scaler' in globals():\n",
        "    with open('outputs/scaler.pkl', 'wb') as f:\n",
        "        joblib.dump(scaler, f)\n",
        "    print(\"   ‚úÖ scaler.pkl\")\n",
        "else:\n",
        "    # Cr√©er un scaler dummy si n√©cessaire\n",
        "    from sklearn.preprocessing import StandardScaler\n",
        "    dummy_scaler = StandardScaler()\n",
        "    with open('outputs/scaler.pkl', 'wb') as f:\n",
        "        joblib.dump(dummy_scaler, f)\n",
        "    print(\"   ‚ö†Ô∏è  scaler.pkl (dummy - remplacez par le vrai scaler)\")\n",
        "\n",
        "# Sauvegarder les m√©tadonn√©es\n",
        "metadata = {\n",
        "    'best_model_name': best_model_name,\n",
        "    'best_model_type': type(best_model).__name__,\n",
        "    'metrics': {k: float(v) for k, v in best_metrics.items()},\n",
        "    'training_date': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
        "    'all_models_comparison': {\n",
        "        name: {\n",
        "            'f1_score': float(res['metrics']['f1_score']),\n",
        "            'roc_auc': float(res['metrics']['roc_auc']),\n",
        "            'accuracy': float(res['metrics']['accuracy'])\n",
        "        }\n",
        "        for name, res in all_results.items()\n",
        "    },\n",
        "    'feature_names': X_test.columns.tolist() if hasattr(X_test, 'columns') else [],\n",
        "    'n_features': X_test.shape[1] if hasattr(X_test, 'shape') else 0\n",
        "}\n",
        "\n",
        "with open('outputs/metadata.json', 'w') as f:\n",
        "    json.dump(metadata, f, indent=2)\n",
        "print(\"   ‚úÖ metadata.json\")\n",
        "\n",
        "# 5. V√©rification\n",
        "print(\"\\nüîç V√©rification des fichiers:\")\n",
        "for filename in ['best_model.pkl', 'scaler.pkl', 'metadata.json']:\n",
        "    filepath = f'outputs/{filename}'\n",
        "    if os.path.exists(filepath):\n",
        "        size = os.path.getsize(filepath) / 1024  # En KB\n",
        "        print(f\"   ‚úÖ {filename} ({size:.2f} KB)\")\n",
        "    else:\n",
        "        print(f\"   ‚ùå {filename} MANQUANT!\")\n",
        "\n",
        "print(\"\\n‚úÖ Artifacts pr√™ts pour l'enregistrement Azure ML!\")\n",
        "print(\"   Vous pouvez maintenant ex√©cuter le code d'enregistrement.\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1765970616506
        },
        "jupyter": {
          "source_hidden": true
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# SAUVEGARDE LOCALE (Toujours fonctionnel)\n",
        "# =============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"üíæ SAUVEGARDE LOCALE DES MOD√àLES (Backup)\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Cr√©er le dossier outputs\n",
        "os.makedirs('outputs', exist_ok=True)\n",
        "\n",
        "# Sauvegarder tous les mod√®les localement\n",
        "for model_name, results in all_results.items():\n",
        "    model_path = f'outputs/{model_name.lower().replace(\" \", \"_\")}.pkl'\n",
        "    joblib.dump(results['model'], model_path)\n",
        "    print(f\"‚úÖ {model_name} ‚Üí {model_path}\")\n",
        "\n",
        "# Sauvegarder le scaler\n",
        "scaler_path = 'outputs/scaler.pkl'\n",
        "joblib.dump(scaler, scaler_path)\n",
        "print(f\"‚úÖ Scaler ‚Üí {scaler_path}\")\n",
        "\n",
        "# Sauvegarder les m√©tadonn√©es de tous les mod√®les\n",
        "all_metadata = {}\n",
        "for model_name, results in all_results.items():\n",
        "    all_metadata[model_name] = {\n",
        "        'metrics': {\n",
        "            k: float(v) if isinstance(v, (int, float, np.integer, np.floating)) else str(v)\n",
        "            for k, v in results['metrics'].items()\n",
        "        },\n",
        "        'run_id': results['run_id']\n",
        "    }\n",
        "\n",
        "with open('outputs/all_models_metadata.json', 'w') as f:\n",
        "    json.dump(all_metadata, f, indent=4)\n",
        "print(f\"‚úÖ M√©tadonn√©es ‚Üí outputs/all_models_metadata.json\")\n",
        "\n",
        "print(\"\\n‚úÖ Tous les mod√®les sauvegard√©s localement!\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1765979740669
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üìä 9. Deploiement"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# √âTAPE 1: PR√âPARER LES FICHIERS DE D√âPLOIEMENT\n",
        "# =============================================================================\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\" \"*20 + \"üöÄ D√âPLOIEMENT AZURE ML ENDPOINT\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "import os\n",
        "from azureml.core import Workspace, Model, Environment\n",
        "from azureml.core.model import InferenceConfig\n",
        "from azureml.core.webservice import AciWebservice, Webservice\n",
        "from azureml.exceptions import WebserviceException\n",
        "import json\n",
        "\n",
        "# Connexion au workspace\n",
        "try:\n",
        "    ws = Workspace.from_config()\n",
        "    print(f\"‚úÖ Connect√© au workspace: {ws.name}\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Erreur de connexion: {e}\")\n",
        "    print(\"üí° Assurez-vous que config.json existe\")\n",
        "    raise"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "trusted": true,
        "gather": {
          "logged": 1765970634064
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# √âTAPE 2: CR√âER LE SCRIPT DE SCORING\n",
        "# =============================================================================\n",
        "\n",
        "print(\"\\nüìù Cr√©ation du script de scoring (score.py)...\")\n",
        "\n",
        "score_script = \"\"\"\n",
        "import json\n",
        "import joblib\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from azureml.core.model import Model\n",
        "\n",
        "def init():\n",
        "    '''\n",
        "    Cette fonction est appel√©e une seule fois au d√©marrage du service.\n",
        "    Elle charge le mod√®le et le scaler en m√©moire.\n",
        "    '''\n",
        "    global model, scaler, feature_names\n",
        "    \n",
        "    try:\n",
        "        # Charger le mod√®le\n",
        "        model_path = Model.get_model_path('fraud-detection-model')\n",
        "        model = joblib.load(model_path)\n",
        "        print(f\"‚úÖ Mod√®le charg√©: {type(model).__name__}\")\n",
        "        \n",
        "        # Charger le scaler\n",
        "        scaler_path = Model.get_model_path('fraud-detection-scaler')\n",
        "        scaler = joblib.load(scaler_path)\n",
        "        print(f\"‚úÖ Scaler charg√©: {type(scaler).__name__}\")\n",
        "        \n",
        "        # Charger les noms de features (optionnel)\n",
        "        try:\n",
        "            metadata_path = Model.get_model_path('fraud-detection-metadata')\n",
        "            with open(metadata_path, 'r') as f:\n",
        "                metadata = json.load(f)\n",
        "                feature_names = metadata.get('feature_names', [])\n",
        "                print(f\"‚úÖ M√©tadonn√©es charg√©es ({len(feature_names)} features)\")\n",
        "        except:\n",
        "            feature_names = []\n",
        "            print(\"‚ö†Ô∏è M√©tadonn√©es non disponibles\")\n",
        "        \n",
        "        print(\"‚úÖ Initialisation r√©ussie!\")\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Erreur lors de l'initialisation: {str(e)}\")\n",
        "        raise\n",
        "\n",
        "def run(raw_data):\n",
        "    '''\n",
        "    Cette fonction est appel√©e pour chaque requ√™te HTTP.\n",
        "    Elle re√ßoit les donn√©es, fait la pr√©diction et retourne le r√©sultat.\n",
        "    \n",
        "    Format d'entr√©e attendu:\n",
        "    {\n",
        "        \"data\": [[feature1, feature2, ..., featureN]],\n",
        "        \"transaction_ids\": [\"TXN_001\", \"TXN_002\"]  # optionnel\n",
        "    }\n",
        "    \n",
        "    Format de sortie:\n",
        "    {\n",
        "        \"predictions\": [\n",
        "            {\n",
        "                \"transaction_id\": \"TXN_001\",\n",
        "                \"is_fraud\": true/false,\n",
        "                \"fraud_probability\": 0.85,\n",
        "                \"confidence\": 0.85,\n",
        "                \"risk_level\": \"HIGH/MEDIUM/LOW\"\n",
        "            }\n",
        "        ],\n",
        "        \"model_info\": {\n",
        "            \"model_name\": \"XGBoost\",\n",
        "            \"version\": \"1.0\"\n",
        "        },\n",
        "        \"status\": \"success\"\n",
        "    }\n",
        "    '''\n",
        "    try:\n",
        "        # Parser les donn√©es JSON\n",
        "        data = json.loads(raw_data)\n",
        "        \n",
        "        # Extraire les features\n",
        "        if 'data' not in data:\n",
        "            return json.dumps({\n",
        "                'error': 'Missing \"data\" field in request',\n",
        "                'status': 'error'\n",
        "            })\n",
        "        \n",
        "        input_data = np.array(data['data'])\n",
        "        transaction_ids = data.get('transaction_ids', \n",
        "                                   [f'TXN_{i:04d}' for i in range(len(input_data))])\n",
        "        \n",
        "        # Validation de la forme des donn√©es\n",
        "        if len(input_data.shape) == 1:\n",
        "            input_data = input_data.reshape(1, -1)\n",
        "        \n",
        "        # Preprocessing: Scaling\n",
        "        scaled_data = scaler.transform(input_data)\n",
        "        \n",
        "        # Pr√©diction\n",
        "        predictions = model.predict(scaled_data)\n",
        "        probabilities = model.predict_proba(scaled_data)\n",
        "        \n",
        "        # Formater les r√©sultats\n",
        "        results = []\n",
        "        for i, (pred, proba) in enumerate(zip(predictions, probabilities)):\n",
        "            fraud_prob = float(proba[1])\n",
        "            \n",
        "            # D√©terminer le niveau de risque\n",
        "            if fraud_prob >= 0.7:\n",
        "                risk_level = \"HIGH\"\n",
        "            elif fraud_prob >= 0.4:\n",
        "                risk_level = \"MEDIUM\"\n",
        "            else:\n",
        "                risk_level = \"LOW\"\n",
        "            \n",
        "            results.append({\n",
        "                'transaction_id': transaction_ids[i],\n",
        "                'is_fraud': bool(pred == 1),\n",
        "                'fraud_probability': round(fraud_prob, 4),\n",
        "                'legitimate_probability': round(float(proba[0]), 4),\n",
        "                'confidence': round(float(max(proba)), 4),\n",
        "                'risk_level': risk_level,\n",
        "                'recommendation': (\n",
        "                    'BLOCK - Fraude probable' if fraud_prob >= 0.7 else\n",
        "                    'REVIEW - Investigation recommand√©e' if fraud_prob >= 0.4 else\n",
        "                    'APPROVE - Transaction s√ªre'\n",
        "                )\n",
        "            })\n",
        "        \n",
        "        # R√©ponse compl√®te\n",
        "        response = {\n",
        "            'predictions': results,\n",
        "            'model_info': {\n",
        "                'model_name': type(model).__name__,\n",
        "                'version': '1.0',\n",
        "                'features_count': input_data.shape[1]\n",
        "            },\n",
        "            'metadata': {\n",
        "                'total_transactions': len(results),\n",
        "                'fraud_detected': sum(1 for r in results if r['is_fraud']),\n",
        "                'avg_fraud_probability': round(\n",
        "                    sum(r['fraud_probability'] for r in results) / len(results), 4\n",
        "                )\n",
        "            },\n",
        "            'status': 'success'\n",
        "        }\n",
        "        \n",
        "        return json.dumps(response)\n",
        "        \n",
        "    except Exception as e:\n",
        "        error_response = {\n",
        "            'error': str(e),\n",
        "            'error_type': type(e).__name__,\n",
        "            'status': 'error',\n",
        "            'message': 'Une erreur est survenue lors de la pr√©diction'\n",
        "        }\n",
        "        return json.dumps(error_response)\n",
        "\"\"\"\n",
        "\n",
        "# Sauvegarder le script\n",
        "with open('score.py', 'w', encoding='utf-8') as f:\n",
        "    f.write(score_script)\n",
        "\n",
        "print(\"‚úÖ score.py cr√©√© avec succ√®s\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1765970636457
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# √âTAPE 3: CR√âER LE FICHIER D'ENVIRONNEMENT\n",
        "# =============================================================================\n",
        "\n",
        "print(\"\\nüì¶ Cr√©ation de l'environnement (fraud_env.yml)...\")\n",
        "\n",
        "env_yml = \"\"\"name: fraud-detection-inference-env\n",
        "channels:\n",
        "  - conda-forge\n",
        "dependencies:\n",
        "  - python=3.10.18\n",
        "  - pip\n",
        "  - pip:\n",
        "      - azureml-defaults==1.60.0\n",
        "      - pandas==1.5.3\n",
        "      - numpy==1.22.4\n",
        "      - scikit-learn==1.7.2\n",
        "      - joblib==1.2.0\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "with open('fraud_env.yml', 'w') as f:\n",
        "    f.write(env_yml)\n",
        "\n",
        "print(\"‚úÖ fraud_env.yml cr√©√©\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1765970641621
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# √âTAPE 4: ENREGISTRER LES MOD√àLES DANS AZURE ML\n",
        "# =============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"üìù ENREGISTREMENT DES MOD√àLES DANS AZURE ML MODEL REGISTRY\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "def register_model_file(workspace, model_path, model_name, description, tags=None):\n",
        "    \"\"\"Enregistre un mod√®le dans Azure ML\"\"\"\n",
        "    try:\n",
        "        # V√©rifier si le fichier existe\n",
        "        if not os.path.exists(model_path):\n",
        "            print(f\"‚ùå Fichier introuvable: {model_path}\")\n",
        "            return None\n",
        "        \n",
        "        # Enregistrer\n",
        "        model = Model.register(\n",
        "            workspace=workspace,\n",
        "            model_path=model_path,\n",
        "            model_name=model_name,\n",
        "            description=description,\n",
        "            tags=tags or {},\n",
        "            model_framework='ScikitLearn'\n",
        "        )\n",
        "        \n",
        "        print(f\"‚úÖ {model_name}\")\n",
        "        print(f\"   Version: {model.version}\")\n",
        "        print(f\"   ID: {model.id}\")\n",
        "        return model\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Erreur lors de l'enregistrement de {model_name}: {str(e)}\")\n",
        "        return None\n",
        "\n",
        "# Enregistrer le meilleur mod√®le\n",
        "print(\"\\n1Ô∏è‚É£ Enregistrement du mod√®le principal...\")\n",
        "model_registered = register_model_file(\n",
        "    workspace=ws,\n",
        "    model_path='outputs/best_model.pkl',\n",
        "    model_name='fraud-detection-model',\n",
        "    description='Meilleur mod√®le de d√©tection de fraude (XGBoost/LightGBM)',\n",
        "    tags={\n",
        "        'type': 'fraud_detection',\n",
        "        'framework': 'scikit-learn',\n",
        "        'deployment': 'production'\n",
        "    }\n",
        ")\n",
        "\n",
        "# Enregistrer le scaler\n",
        "print(\"\\n2Ô∏è‚É£ Enregistrement du scaler...\")\n",
        "scaler_registered = register_model_file(\n",
        "    workspace=ws,\n",
        "    model_path='outputs/scaler.pkl',\n",
        "    model_name='fraud-detection-scaler',\n",
        "    description='RobustScaler pour preprocessing des features',\n",
        "    tags={'type': 'preprocessing'}\n",
        ")\n",
        "\n",
        "# Enregistrer les m√©tadonn√©es (optionnel)\n",
        "print(\"\\n3Ô∏è‚É£ Enregistrement des m√©tadonn√©es...\")\n",
        "if os.path.exists('outputs/metadata.json'):\n",
        "    metadata_registered = register_model_file(\n",
        "        workspace=ws,\n",
        "        model_path='outputs/metadata.json',\n",
        "        model_name='fraud-detection-metadata',\n",
        "        description='M√©tadonn√©es du mod√®le (features, m√©triques)',\n",
        "        tags={'type': 'metadata'}\n",
        "    )\n",
        "\n",
        "if not model_registered or not scaler_registered:\n",
        "    print(\"\\n‚ùå Enregistrement des mod√®les √©chou√©. V√©rifiez les fichiers dans /outputs\")\n",
        "    raise Exception(\"Mod√®les non enregistr√©s\")\n",
        "\n",
        "print(\"\\n‚úÖ Tous les artifacts enregistr√©s avec succ√®s!\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1765978907375
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# √âTAPE 5: CR√âER L'ENVIRONNEMENT AZURE ML\n",
        "# =============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"üåç CR√âATION DE L'ENVIRONNEMENT AZURE ML\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Cr√©er l'environnement depuis le fichier YAML\n",
        "env = Environment.from_conda_specification(\n",
        "    name='fraud-detection-env',\n",
        "    file_path='fraud_env.yml'\n",
        ")\n",
        "\n",
        "# Enregistrer l'environnement\n",
        "env.register(workspace=ws)\n",
        "print(f\"‚úÖ Environnement '{env.name}' cr√©√© et enregistr√©\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1765965850705
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# √âTAPE 6: CONFIGURATION DU D√âPLOIEMENT\n",
        "# =============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"‚öôÔ∏è CONFIGURATION DU D√âPLOIEMENT\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Configuration de l'inf√©rence\n",
        "inference_config = InferenceConfig(\n",
        "    entry_script='score.py',\n",
        "    environment=env\n",
        ")\n",
        "print(\"‚úÖ Configuration d'inf√©rence cr√©√©e\")\n",
        "\n",
        "# Configuration du service (Azure Container Instance)\n",
        "deployment_config = AciWebservice.deploy_configuration(\n",
        "    cpu_cores=2,\n",
        "    memory_gb=4,\n",
        "    auth_enabled=True,  # Activer l'authentification\n",
        "    enable_app_insights=True,  # Activer le monitoring\n",
        "    collect_model_data=True,  # Collecter les donn√©es pour monitoring\n",
        "    description='API REST de d√©tection de fraude - Projet CDDA',\n",
        "    tags={\n",
        "        'project': 'CDDA',\n",
        "        'use_case': 'fraud_detection',\n",
        "        'department': 'data_science'\n",
        "    }\n",
        ")\n",
        "print(\"‚úÖ Configuration de d√©ploiement cr√©√©e (ACI)\")\n",
        "print(\"   ‚Ä¢ CPU: 2 cores\")\n",
        "print(\"   ‚Ä¢ RAM: 4 GB\")\n",
        "print(\"   ‚Ä¢ Auth: Activ√©e\")\n",
        "print(\"   ‚Ä¢ Monitoring: Activ√©\")\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1765965856761
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.core import Workspace\n",
        "ws.get_details()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1765965119541
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from azure.identity import DefaultAzureCredential\n",
        "from azure.ai.ml import MLClient\n",
        "\n",
        "credential = DefaultAzureCredential()  # ensure new token\n",
        "ml_client = MLClient(credential, subscription_id, resource_group, workspace_name)\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1765967998458
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# √âTAPE 7: D√âPLOYER LE SERVICE\n",
        "# =============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"üöÄ D√âPLOIEMENT DU SERVICE\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "service_name = 'fraud-detection-api'\n",
        "\n",
        "# V√©rifier si le service existe d√©j√†\n",
        "try:\n",
        "    existing_service = Webservice(workspace=ws, name=service_name)\n",
        "    print(f\"‚ö†Ô∏è Service '{service_name}' existe d√©j√†\")\n",
        "    \n",
        "    # Demander confirmation pour mise √† jour\n",
        "    update_service = True  # Mettre √† False si vous voulez garder l'ancien\n",
        "    \n",
        "    if update_service:\n",
        "        print(\"üîÑ Mise √† jour du service existant...\")\n",
        "        existing_service.delete()\n",
        "        print(\"‚úÖ Ancien service supprim√©\")\n",
        "    else:\n",
        "        print(\"‚ÑπÔ∏è Utilisation du service existant\")\n",
        "        service = existing_service\n",
        "        \n",
        "except WebserviceException:\n",
        "    print(f\"‚ÑπÔ∏è Aucun service existant nomm√© '{service_name}'\")\n",
        "\n",
        "# D√©ployer le nouveau service\n",
        "if 'service' not in locals():\n",
        "    print(f\"\\nüöÄ D√©ploiement en cours de '{service_name}'...\")\n",
        "    print(\"‚è≥ Cela peut prendre 5-10 minutes...\")\n",
        "    \n",
        "    service = Model.deploy(\n",
        "        workspace=ws,\n",
        "        name=service_name,\n",
        "        models=[model_registered, scaler_registered],\n",
        "        inference_config=inference_config,\n",
        "        deployment_config=deployment_config,\n",
        "        overwrite=True\n",
        "    )\n",
        "    \n",
        "    # Attendre la fin du d√©ploiement\n",
        "    service.wait_for_deployment(show_output=True)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1765966612600
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# √âTAPE 8: V√âRIFIER LE D√âPLOIEMENT\n",
        "# =============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"‚úÖ D√âPLOIEMENT R√âUSSI!\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "print(f\"\\nüìä Informations du service:\")\n",
        "print(f\"   Nom: {service.name}\")\n",
        "print(f\"   √âtat: {service.state}\")\n",
        "print(f\"   Scoring URI: {service.scoring_uri}\")\n",
        "\n",
        "# R√©cup√©rer les cl√©s d'authentification\n",
        "try:\n",
        "    primary_key, secondary_key = service.get_keys()\n",
        "    print(f\"\\nüîë Cl√©s d'authentification:\")\n",
        "    print(f\"   Primary Key: {primary_key[:20]}...\")\n",
        "    print(f\"   Secondary Key: {secondary_key[:20]}...\")\n",
        "except:\n",
        "    print(\"\\n‚ö†Ô∏è Authentification par cl√© non disponible\")\n",
        "\n",
        "# Swagger URI (documentation auto-g√©n√©r√©e)\n",
        "if hasattr(service, 'swagger_uri') and service.swagger_uri:\n",
        "    print(f\"\\nüìñ Documentation Swagger: {service.swagger_uri}\")\n",
        "\n",
        "# Sauvegarder les informations de d√©ploiement\n",
        "deployment_info = {\n",
        "    'service_name': service.name,\n",
        "    'scoring_uri': service.scoring_uri,\n",
        "    'state': service.state,\n",
        "    'primary_key': primary_key if 'primary_key' in locals() else None,\n",
        "    'deployment_date': str(pd.Timestamp.now())\n",
        "}\n",
        "\n",
        "with open('outputs/deployment_info.json', 'w') as f:\n",
        "    json.dump(deployment_info, f, indent=4)\n",
        "\n",
        "print(\"\\nüíæ Informations sauvegard√©es dans: outputs/deployment_info.json\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1765924285322
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# √âTAPE 9: TESTER L'ENDPOINT\n",
        "# =============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"üß™ TEST DE L'ENDPOINT\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "import requests\n",
        "import pandas as pd\n",
        "\n",
        "# Pr√©parer des donn√©es de test\n",
        "print(\"\\nüìä Pr√©paration des donn√©es de test...\")\n",
        "\n",
        "# Charger quelques transactions de test\n",
        "# (Remplacez par vos vraies donn√©es)\n",
        "test_data = {\n",
        "    'data': [\n",
        "        # Transaction 1: Caract√©ristiques normales\n",
        "        [100.0, 5000.0, 3000.0, 0.5, 0.2],\n",
        "        # Transaction 2: Caract√©ristiques suspectes\n",
        "        [50000.0, 100.0, 200000.0, 0.9, 0.8],\n",
        "        # Transaction 3: Borderline\n",
        "        [1000.0, 2000.0, 2500.0, 0.45, 0.35]\n",
        "    ],\n",
        "    'transaction_ids': ['TXN_001', 'TXN_002', 'TXN_003']\n",
        "}\n",
        "\n",
        "print(f\"‚úÖ {len(test_data['data'])} transactions de test pr√©par√©es\")\n",
        "\n",
        "# Faire la requ√™te\n",
        "print(\"\\nüåê Envoi de la requ√™te √† l'endpoint...\")\n",
        "\n",
        "headers = {\n",
        "    'Content-Type': 'application/json',\n",
        "    'Authorization': f'Bearer {primary_key}' if 'primary_key' in locals() else ''\n",
        "}\n",
        "\n",
        "try:\n",
        "    response = requests.post(\n",
        "        service.scoring_uri,\n",
        "        data=json.dumps(test_data),\n",
        "        headers=headers,\n",
        "        timeout=30\n",
        "    )\n",
        "    \n",
        "    if response.status_code == 200:\n",
        "        results = response.json()\n",
        "        \n",
        "        print(\"\\n‚úÖ PR√âDICTIONS RE√áUES:\")\n",
        "        print(\"=\"*70)\n",
        "        \n",
        "        for pred in results['predictions']:\n",
        "            status = \"üö® FRAUDE\" if pred['is_fraud'] else \"‚úÖ L√âGITIME\"\n",
        "            risk = pred['risk_level']\n",
        "            prob = pred['fraud_probability'] * 100\n",
        "            \n",
        "            print(f\"\\n{pred['transaction_id']}: {status}\")\n",
        "            print(f\"   Probabilit√© fraude: {prob:.1f}%\")\n",
        "            print(f\"   Niveau de risque: {risk}\")\n",
        "            print(f\"   Recommandation: {pred['recommendation']}\")\n",
        "        \n",
        "        print(\"\\n\" + \"=\"*70)\n",
        "        print(f\"üìä Statistiques globales:\")\n",
        "        print(f\"   Total: {results['metadata']['total_transactions']}\")\n",
        "        print(f\"   Fraudes d√©tect√©es: {results['metadata']['fraud_detected']}\")\n",
        "        print(f\"   Prob. moyenne: {results['metadata']['avg_fraud_probability']*100:.1f}%\")\n",
        "        \n",
        "    else:\n",
        "        print(f\"‚ùå Erreur HTTP {response.status_code}\")\n",
        "        print(f\"R√©ponse: {response.text}\")\n",
        "        \n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Erreur lors du test: {str(e)}\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1765924308513
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "print(sys.version)\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1765962393018
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "correlations = pd.concat([X_train, y_train], axis=1).corr()\n",
        "correlations['isFraud'].abs().sort_values(ascending=False).head(10)\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1765975219038
        }
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.18",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [],
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": false
    },
    "microsoft": {
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      },
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      }
    },
    "kernel_info": {
      "name": "python3"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}