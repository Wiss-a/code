{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Fraud Detection - Data Preprocessing & Model Training\n",
        "## Azure Cloud Data-Driven Application Project\n",
        "\n",
        "**Project:** Detection of fraud in financial transactions  \n",
        "**Dataset:** 6M rows ‚Üí Truncated & Balanced for optimal training  \n",
        "**Goal:** Train a high-performance fraud detection model\n",
        "\n",
        "---\n",
        "\n",
        "## Pipeline Overview:\n",
        "1. **Data Loading & Exploration**\n",
        "2. **Strategic Truncation** (All frauds + 500k non-frauds)\n",
        "3. **Data Balancing with SMOTE** (40/60 or customizable ratio)\n",
        "4. **Feature Engineering & Preprocessing**\n",
        "5. **Model Training** (Multiple algorithms)\n",
        "6. **Model Evaluation & Comparison**\n",
        "7. **Export Best Model** for deployment"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üì¶ 1. Import Required Libraries"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "pip install mlflow"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Requirement already satisfied: mlflow in /anaconda/envs/jupyter_env/lib/python3.10/site-packages (3.8.1)\nRequirement already satisfied: mlflow-skinny==3.8.1 in /anaconda/envs/jupyter_env/lib/python3.10/site-packages (from mlflow) (3.8.1)\nRequirement already satisfied: mlflow-tracing==3.8.1 in /anaconda/envs/jupyter_env/lib/python3.10/site-packages (from mlflow) (3.8.1)\nRequirement already satisfied: Flask-CORS<7 in /anaconda/envs/jupyter_env/lib/python3.10/site-packages (from mlflow) (6.0.2)\nRequirement already satisfied: Flask<4 in /anaconda/envs/jupyter_env/lib/python3.10/site-packages (from mlflow) (3.1.2)\nRequirement already satisfied: alembic!=1.10.0,<2 in /anaconda/envs/jupyter_env/lib/python3.10/site-packages (from mlflow) (1.17.2)\nRequirement already satisfied: cryptography<47,>=43.0.0 in /anaconda/envs/jupyter_env/lib/python3.10/site-packages (from mlflow) (45.0.5)\nRequirement already satisfied: docker<8,>=4.0.0 in /anaconda/envs/jupyter_env/lib/python3.10/site-packages (from mlflow) (7.1.0)\nRequirement already satisfied: graphene<4 in /anaconda/envs/jupyter_env/lib/python3.10/site-packages (from mlflow) (3.4.3)\nRequirement already satisfied: gunicorn<24 in /anaconda/envs/jupyter_env/lib/python3.10/site-packages (from mlflow) (23.0.0)\nRequirement already satisfied: huey<3,>=2.5.0 in /anaconda/envs/jupyter_env/lib/python3.10/site-packages (from mlflow) (2.5.5)\nRequirement already satisfied: matplotlib<4 in /anaconda/envs/jupyter_env/lib/python3.10/site-packages (from mlflow) (3.10.8)\nRequirement already satisfied: numpy<3 in /anaconda/envs/jupyter_env/lib/python3.10/site-packages (from mlflow) (1.25.2)\nRequirement already satisfied: pandas<3 in /anaconda/envs/jupyter_env/lib/python3.10/site-packages (from mlflow) (2.3.1)\nRequirement already satisfied: pyarrow<23,>=4.0.0 in /anaconda/envs/jupyter_env/lib/python3.10/site-packages (from mlflow) (20.0.0)\nRequirement already satisfied: scikit-learn<2 in /anaconda/envs/jupyter_env/lib/python3.10/site-packages (from mlflow) (1.7.2)\nRequirement already satisfied: scipy<2 in /anaconda/envs/jupyter_env/lib/python3.10/site-packages (from mlflow) (1.15.3)\nRequirement already satisfied: sqlalchemy<3,>=1.4.0 in /anaconda/envs/jupyter_env/lib/python3.10/site-packages (from mlflow) (2.0.45)\nRequirement already satisfied: cachetools<7,>=5.0.0 in /anaconda/envs/jupyter_env/lib/python3.10/site-packages (from mlflow-skinny==3.8.1->mlflow) (5.5.2)\nRequirement already satisfied: click<9,>=7.0 in /anaconda/envs/jupyter_env/lib/python3.10/site-packages (from mlflow-skinny==3.8.1->mlflow) (8.2.1)\nRequirement already satisfied: cloudpickle<4 in /anaconda/envs/jupyter_env/lib/python3.10/site-packages (from mlflow-skinny==3.8.1->mlflow) (3.1.1)\nRequirement already satisfied: databricks-sdk<1,>=0.20.0 in /anaconda/envs/jupyter_env/lib/python3.10/site-packages (from mlflow-skinny==3.8.1->mlflow) (0.76.0)\nRequirement already satisfied: fastapi<1 in /anaconda/envs/jupyter_env/lib/python3.10/site-packages (from mlflow-skinny==3.8.1->mlflow) (0.128.0)\nRequirement already satisfied: gitpython<4,>=3.1.9 in /anaconda/envs/jupyter_env/lib/python3.10/site-packages (from mlflow-skinny==3.8.1->mlflow) (3.1.46)\nRequirement already satisfied: importlib_metadata!=4.7.0,<9,>=3.7.0 in /anaconda/envs/jupyter_env/lib/python3.10/site-packages (from mlflow-skinny==3.8.1->mlflow) (8.7.0)\nRequirement already satisfied: opentelemetry-api<3,>=1.9.0 in /anaconda/envs/jupyter_env/lib/python3.10/site-packages (from mlflow-skinny==3.8.1->mlflow) (1.35.0)\nRequirement already satisfied: opentelemetry-proto<3,>=1.9.0 in /anaconda/envs/jupyter_env/lib/python3.10/site-packages (from mlflow-skinny==3.8.1->mlflow) (1.35.0)\nRequirement already satisfied: opentelemetry-sdk<3,>=1.9.0 in /anaconda/envs/jupyter_env/lib/python3.10/site-packages (from mlflow-skinny==3.8.1->mlflow) (1.35.0)\nRequirement already satisfied: packaging<26 in /anaconda/envs/jupyter_env/lib/python3.10/site-packages (from mlflow-skinny==3.8.1->mlflow) (25.0)\nRequirement already satisfied: protobuf<7,>=3.12.0 in /anaconda/envs/jupyter_env/lib/python3.10/site-packages (from mlflow-skinny==3.8.1->mlflow) (6.31.1)\nRequirement already satisfied: pydantic<3,>=2.0.0 in /anaconda/envs/jupyter_env/lib/python3.10/site-packages (from mlflow-skinny==3.8.1->mlflow) (2.11.7)\nRequirement already satisfied: python-dotenv<2,>=0.19.0 in /anaconda/envs/jupyter_env/lib/python3.10/site-packages (from mlflow-skinny==3.8.1->mlflow) (1.2.1)\nRequirement already satisfied: pyyaml<7,>=5.1 in /anaconda/envs/jupyter_env/lib/python3.10/site-packages (from mlflow-skinny==3.8.1->mlflow) (6.0.2)\nRequirement already satisfied: requests<3,>=2.17.3 in /anaconda/envs/jupyter_env/lib/python3.10/site-packages (from mlflow-skinny==3.8.1->mlflow) (2.32.4)\nRequirement already satisfied: sqlparse<1,>=0.4.0 in /anaconda/envs/jupyter_env/lib/python3.10/site-packages (from mlflow-skinny==3.8.1->mlflow) (0.5.5)\nRequirement already satisfied: typing-extensions<5,>=4.0.0 in /anaconda/envs/jupyter_env/lib/python3.10/site-packages (from mlflow-skinny==3.8.1->mlflow) (4.14.1)\nRequirement already satisfied: uvicorn<1 in /anaconda/envs/jupyter_env/lib/python3.10/site-packages (from mlflow-skinny==3.8.1->mlflow) (0.40.0)\nRequirement already satisfied: Mako in /anaconda/envs/jupyter_env/lib/python3.10/site-packages (from alembic!=1.10.0,<2->mlflow) (1.3.10)\nRequirement already satisfied: tomli in /anaconda/envs/jupyter_env/lib/python3.10/site-packages (from alembic!=1.10.0,<2->mlflow) (2.2.1)\nRequirement already satisfied: cffi>=1.14 in /anaconda/envs/jupyter_env/lib/python3.10/site-packages (from cryptography<47,>=43.0.0->mlflow) (1.15.0)\nRequirement already satisfied: google-auth~=2.0 in /anaconda/envs/jupyter_env/lib/python3.10/site-packages (from databricks-sdk<1,>=0.20.0->mlflow-skinny==3.8.1->mlflow) (2.40.3)\nRequirement already satisfied: urllib3>=1.26.0 in /anaconda/envs/jupyter_env/lib/python3.10/site-packages (from docker<8,>=4.0.0->mlflow) (2.5.0)\nRequirement already satisfied: starlette<0.51.0,>=0.40.0 in /anaconda/envs/jupyter_env/lib/python3.10/site-packages (from fastapi<1->mlflow-skinny==3.8.1->mlflow) (0.50.0)\nRequirement already satisfied: annotated-doc>=0.0.2 in /anaconda/envs/jupyter_env/lib/python3.10/site-packages (from fastapi<1->mlflow-skinny==3.8.1->mlflow) (0.0.4)\nRequirement already satisfied: blinker>=1.9.0 in /anaconda/envs/jupyter_env/lib/python3.10/site-packages (from Flask<4->mlflow) (1.9.0)\nRequirement already satisfied: itsdangerous>=2.2.0 in /anaconda/envs/jupyter_env/lib/python3.10/site-packages (from Flask<4->mlflow) (2.2.0)\nRequirement already satisfied: jinja2>=3.1.2 in /anaconda/envs/jupyter_env/lib/python3.10/site-packages (from Flask<4->mlflow) (3.1.6)\nRequirement already satisfied: markupsafe>=2.1.1 in /anaconda/envs/jupyter_env/lib/python3.10/site-packages (from Flask<4->mlflow) (2.1.1)\nRequirement already satisfied: werkzeug>=3.1.0 in /anaconda/envs/jupyter_env/lib/python3.10/site-packages (from Flask<4->mlflow) (3.1.3)\nRequirement already satisfied: gitdb<5,>=4.0.1 in /anaconda/envs/jupyter_env/lib/python3.10/site-packages (from gitpython<4,>=3.1.9->mlflow-skinny==3.8.1->mlflow) (4.0.12)\nRequirement already satisfied: smmap<6,>=3.0.1 in /anaconda/envs/jupyter_env/lib/python3.10/site-packages (from gitdb<5,>=4.0.1->gitpython<4,>=3.1.9->mlflow-skinny==3.8.1->mlflow) (5.0.2)\nRequirement already satisfied: pyasn1-modules>=0.2.1 in /anaconda/envs/jupyter_env/lib/python3.10/site-packages (from google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==3.8.1->mlflow) (0.4.2)\nRequirement already satisfied: rsa<5,>=3.1.4 in /anaconda/envs/jupyter_env/lib/python3.10/site-packages (from google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==3.8.1->mlflow) (4.9.1)\nRequirement already satisfied: graphql-core<3.3,>=3.1 in /anaconda/envs/jupyter_env/lib/python3.10/site-packages (from graphene<4->mlflow) (3.2.7)\nRequirement already satisfied: graphql-relay<3.3,>=3.1 in /anaconda/envs/jupyter_env/lib/python3.10/site-packages (from graphene<4->mlflow) (3.2.0)\nRequirement already satisfied: python-dateutil<3,>=2.7.0 in /anaconda/envs/jupyter_env/lib/python3.10/site-packages (from graphene<4->mlflow) (2.9.0.post0)\nRequirement already satisfied: zipp>=3.20 in /anaconda/envs/jupyter_env/lib/python3.10/site-packages (from importlib_metadata!=4.7.0,<9,>=3.7.0->mlflow-skinny==3.8.1->mlflow) (3.23.0)\nRequirement already satisfied: contourpy>=1.0.1 in /anaconda/envs/jupyter_env/lib/python3.10/site-packages (from matplotlib<4->mlflow) (1.3.2)\nRequirement already satisfied: cycler>=0.10 in /anaconda/envs/jupyter_env/lib/python3.10/site-packages (from matplotlib<4->mlflow) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /anaconda/envs/jupyter_env/lib/python3.10/site-packages (from matplotlib<4->mlflow) (4.61.1)\nRequirement already satisfied: kiwisolver>=1.3.1 in /anaconda/envs/jupyter_env/lib/python3.10/site-packages (from matplotlib<4->mlflow) (1.4.9)\nRequirement already satisfied: pillow>=8 in /anaconda/envs/jupyter_env/lib/python3.10/site-packages (from matplotlib<4->mlflow) (12.1.0)\nRequirement already satisfied: pyparsing>=3 in /anaconda/envs/jupyter_env/lib/python3.10/site-packages (from matplotlib<4->mlflow) (3.3.1)\nRequirement already satisfied: opentelemetry-semantic-conventions==0.56b0 in /anaconda/envs/jupyter_env/lib/python3.10/site-packages (from opentelemetry-sdk<3,>=1.9.0->mlflow-skinny==3.8.1->mlflow) (0.56b0)\nRequirement already satisfied: pytz>=2020.1 in /anaconda/envs/jupyter_env/lib/python3.10/site-packages (from pandas<3->mlflow) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /anaconda/envs/jupyter_env/lib/python3.10/site-packages (from pandas<3->mlflow) (2025.2)\nRequirement already satisfied: annotated-types>=0.6.0 in /anaconda/envs/jupyter_env/lib/python3.10/site-packages (from pydantic<3,>=2.0.0->mlflow-skinny==3.8.1->mlflow) (0.7.0)\nRequirement already satisfied: pydantic-core==2.33.2 in /anaconda/envs/jupyter_env/lib/python3.10/site-packages (from pydantic<3,>=2.0.0->mlflow-skinny==3.8.1->mlflow) (2.33.2)\nRequirement already satisfied: typing-inspection>=0.4.0 in /anaconda/envs/jupyter_env/lib/python3.10/site-packages (from pydantic<3,>=2.0.0->mlflow-skinny==3.8.1->mlflow) (0.4.1)\nRequirement already satisfied: six>=1.5 in /anaconda/envs/jupyter_env/lib/python3.10/site-packages (from python-dateutil<3,>=2.7.0->graphene<4->mlflow) (1.17.0)\nRequirement already satisfied: charset_normalizer<4,>=2 in /anaconda/envs/jupyter_env/lib/python3.10/site-packages (from requests<3,>=2.17.3->mlflow-skinny==3.8.1->mlflow) (3.4.2)\nRequirement already satisfied: idna<4,>=2.5 in /anaconda/envs/jupyter_env/lib/python3.10/site-packages (from requests<3,>=2.17.3->mlflow-skinny==3.8.1->mlflow) (3.10)\nRequirement already satisfied: certifi>=2017.4.17 in /anaconda/envs/jupyter_env/lib/python3.10/site-packages (from requests<3,>=2.17.3->mlflow-skinny==3.8.1->mlflow) (2025.7.9)\nRequirement already satisfied: pyasn1>=0.1.3 in /anaconda/envs/jupyter_env/lib/python3.10/site-packages (from rsa<5,>=3.1.4->google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==3.8.1->mlflow) (0.6.1)\nRequirement already satisfied: joblib>=1.2.0 in /anaconda/envs/jupyter_env/lib/python3.10/site-packages (from scikit-learn<2->mlflow) (1.5.3)\nRequirement already satisfied: threadpoolctl>=3.1.0 in /anaconda/envs/jupyter_env/lib/python3.10/site-packages (from scikit-learn<2->mlflow) (3.6.0)\nRequirement already satisfied: greenlet>=1 in /anaconda/envs/jupyter_env/lib/python3.10/site-packages (from sqlalchemy<3,>=1.4.0->mlflow) (3.3.0)\nRequirement already satisfied: anyio<5,>=3.6.2 in /anaconda/envs/jupyter_env/lib/python3.10/site-packages (from starlette<0.51.0,>=0.40.0->fastapi<1->mlflow-skinny==3.8.1->mlflow) (3.7.1)\nRequirement already satisfied: sniffio>=1.1 in /anaconda/envs/jupyter_env/lib/python3.10/site-packages (from anyio<5,>=3.6.2->starlette<0.51.0,>=0.40.0->fastapi<1->mlflow-skinny==3.8.1->mlflow) (1.3.1)\nRequirement already satisfied: exceptiongroup in /anaconda/envs/jupyter_env/lib/python3.10/site-packages (from anyio<5,>=3.6.2->starlette<0.51.0,>=0.40.0->fastapi<1->mlflow-skinny==3.8.1->mlflow) (1.3.0)\nRequirement already satisfied: h11>=0.8 in /anaconda/envs/jupyter_env/lib/python3.10/site-packages (from uvicorn<1->mlflow-skinny==3.8.1->mlflow) (0.16.0)\nRequirement already satisfied: pycparser in /anaconda/envs/jupyter_env/lib/python3.10/site-packages (from cffi>=1.14->cryptography<47,>=43.0.0->mlflow) (2.22)\nNote: you may need to restart the kernel to use updated packages.\n"
        }
      ],
      "execution_count": 1,
      "metadata": {
        "trusted": true,
        "gather": {
          "logged": 1767550146920
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install xgboost lightgbm\n",
        "%pip install imbalanced-learn\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Collecting xgboost\n  Downloading xgboost-3.1.2-py3-none-manylinux_2_28_x86_64.whl.metadata (2.1 kB)\nCollecting lightgbm\n  Downloading lightgbm-4.6.0-py3-none-manylinux_2_28_x86_64.whl.metadata (17 kB)\nRequirement already satisfied: numpy in /anaconda/envs/jupyter_env/lib/python3.10/site-packages (from xgboost) (1.25.2)\nCollecting nvidia-nccl-cu12 (from xgboost)\n  Downloading nvidia_nccl_cu12-2.28.9-py3-none-manylinux_2_18_x86_64.whl.metadata (2.0 kB)\nRequirement already satisfied: scipy in /anaconda/envs/jupyter_env/lib/python3.10/site-packages (from xgboost) (1.15.3)\nDownloading xgboost-3.1.2-py3-none-manylinux_2_28_x86_64.whl (115.9 MB)\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m115.9/115.9 MB\u001b[0m \u001b[31m70.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading lightgbm-4.6.0-py3-none-manylinux_2_28_x86_64.whl (3.6 MB)\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m97.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading nvidia_nccl_cu12-2.28.9-py3-none-manylinux_2_18_x86_64.whl (296.8 MB)\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m296.8/296.8 MB\u001b[0m \u001b[31m69.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: nvidia-nccl-cu12, xgboost, lightgbm\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m3/3\u001b[0m [lightgbm]2/3\u001b[0m [lightgbm]\n\u001b[1A\u001b[2KSuccessfully installed lightgbm-4.6.0 nvidia-nccl-cu12-2.28.9 xgboost-3.1.2\nNote: you may need to restart the kernel to use updated packages.\nRequirement already satisfied: imbalanced-learn in /anaconda/envs/jupyter_env/lib/python3.10/site-packages (0.14.1)\nRequirement already satisfied: numpy<3,>=1.25.2 in /anaconda/envs/jupyter_env/lib/python3.10/site-packages (from imbalanced-learn) (1.25.2)\nRequirement already satisfied: scipy<2,>=1.11.4 in /anaconda/envs/jupyter_env/lib/python3.10/site-packages (from imbalanced-learn) (1.15.3)\nRequirement already satisfied: scikit-learn<2,>=1.4.2 in /anaconda/envs/jupyter_env/lib/python3.10/site-packages (from imbalanced-learn) (1.7.2)\nRequirement already satisfied: sklearn-compat<0.2,>=0.1.5 in /anaconda/envs/jupyter_env/lib/python3.10/site-packages (from imbalanced-learn) (0.1.5)\nRequirement already satisfied: joblib<2,>=1.2.0 in /anaconda/envs/jupyter_env/lib/python3.10/site-packages (from imbalanced-learn) (1.5.3)\nRequirement already satisfied: threadpoolctl<4,>=2.0.0 in /anaconda/envs/jupyter_env/lib/python3.10/site-packages (from imbalanced-learn) (3.6.0)\nNote: you may need to restart the kernel to use updated packages.\n"
        }
      ],
      "execution_count": 2,
      "metadata": {
        "gather": {
          "logged": 1767356898975
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install seaborn\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Requirement already satisfied: seaborn in /anaconda/envs/jupyter_env/lib/python3.10/site-packages (0.13.2)\nRequirement already satisfied: numpy!=1.24.0,>=1.20 in /anaconda/envs/jupyter_env/lib/python3.10/site-packages (from seaborn) (1.25.2)\nRequirement already satisfied: pandas>=1.2 in /anaconda/envs/jupyter_env/lib/python3.10/site-packages (from seaborn) (2.3.1)\nRequirement already satisfied: matplotlib!=3.6.1,>=3.4 in /anaconda/envs/jupyter_env/lib/python3.10/site-packages (from seaborn) (3.10.8)\nRequirement already satisfied: contourpy>=1.0.1 in /anaconda/envs/jupyter_env/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.3.2)\nRequirement already satisfied: cycler>=0.10 in /anaconda/envs/jupyter_env/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /anaconda/envs/jupyter_env/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (4.61.1)\nRequirement already satisfied: kiwisolver>=1.3.1 in /anaconda/envs/jupyter_env/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.4.9)\nRequirement already satisfied: packaging>=20.0 in /anaconda/envs/jupyter_env/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (25.0)\nRequirement already satisfied: pillow>=8 in /anaconda/envs/jupyter_env/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (12.1.0)\nRequirement already satisfied: pyparsing>=3 in /anaconda/envs/jupyter_env/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (3.3.1)\nRequirement already satisfied: python-dateutil>=2.7 in /anaconda/envs/jupyter_env/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /anaconda/envs/jupyter_env/lib/python3.10/site-packages (from pandas>=1.2->seaborn) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /anaconda/envs/jupyter_env/lib/python3.10/site-packages (from pandas>=1.2->seaborn) (2025.2)\nRequirement already satisfied: six>=1.5 in /anaconda/envs/jupyter_env/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib!=3.6.1,>=3.4->seaborn) (1.17.0)\nNote: you may need to restart the kernel to use updated packages.\n"
        }
      ],
      "execution_count": 3,
      "metadata": {
        "gather": {
          "logged": 1767550164189
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Data manipulation\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datetime import datetime\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Visualization\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "sns.set_style('whitegrid')\n",
        "\n",
        "# Data preprocessing\n",
        "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
        "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
        "from sklearn.utils import resample\n",
        "\n",
        "# Balancing techniques\n",
        "from imblearn.over_sampling import SMOTE, ADASYN, BorderlineSMOTE\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "from imblearn.combine import SMOTETomek, SMOTEENN\n",
        "\n",
        "# Machine Learning models\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from lightgbm import LGBMClassifier\n",
        "\n",
        "# Model evaluation\n",
        "from sklearn.metrics import (\n",
        "    classification_report, confusion_matrix, \n",
        "    accuracy_score, precision_score, recall_score, \n",
        "    f1_score, roc_auc_score, roc_curve,\n",
        "    precision_recall_curve, average_precision_score\n",
        ")\n",
        "\n",
        "# Model persistence\n",
        "import joblib\n",
        "import pickle\n",
        "\n",
        "# For experiment tracking (if using MLflow)\n",
        "try:\n",
        "    import mlflow\n",
        "    import mlflow.sklearn\n",
        "    MLFLOW_AVAILABLE = True\n",
        "    print(\"‚úÖ MLflow is available for experiment tracking\")\n",
        "except ImportError:\n",
        "    MLFLOW_AVAILABLE = False\n",
        "    print(\"‚ö†Ô∏è MLflow not available. Install with: pip install mlflow\")\n",
        "\n",
        "print(\"\\n‚úÖ All libraries imported successfully!\")\n",
        "print(f\"Pandas version: {pd.__version__}\")\n",
        "print(f\"NumPy version: {np.__version__}\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "‚úÖ MLflow is available for experiment tracking\n\n‚úÖ All libraries imported successfully!\nPandas version: 2.3.1\nNumPy version: 1.25.2\n"
        }
      ],
      "execution_count": 4,
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-12-16T13:16:03.334377Z",
          "iopub.execute_input": "2025-12-16T13:16:03.334769Z",
          "iopub.status.idle": "2025-12-16T13:16:09.760633Z",
          "shell.execute_reply.started": "2025-12-16T13:16:03.334741Z",
          "shell.execute_reply": "2025-12-16T13:16:09.759570Z"
        },
        "gather": {
          "logged": 1767550166441
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üìä 2. Load and Explore Dataset"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "pip install azureml-dataset-runtime --upgrade"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Collecting azureml-dataset-runtime\n  Downloading azureml_dataset_runtime-1.61.0-py3-none-any.whl.metadata (1.4 kB)\nCollecting azureml-dataprep<5.5.0a,>=5.1.0a (from azureml-dataset-runtime)\n  Downloading azureml_dataprep-5.4.2-py3-none-any.whl.metadata (2.3 kB)\nRequirement already satisfied: pyarrow<21.0.0,>=0.17.0 in /anaconda/envs/jupyter_env/lib/python3.10/site-packages (from azureml-dataset-runtime) (20.0.0)\nRequirement already satisfied: numpy!=1.19.3,<2.3.3 in /anaconda/envs/jupyter_env/lib/python3.10/site-packages (from azureml-dataset-runtime) (1.25.2)\nCollecting azureml-dataprep-native<43.0.0,>=42.0.0 (from azureml-dataprep<5.5.0a,>=5.1.0a->azureml-dataset-runtime)\n  Downloading azureml_dataprep_native-42.1.0-cp310-cp310-manylinux1_x86_64.whl.metadata (1.5 kB)\nCollecting azureml-dataprep-rslex~=2.25.1 (from azureml-dataprep<5.5.0a,>=5.1.0a->azureml-dataset-runtime)\n  Downloading azureml_dataprep_rslex-2.25.2-cp310-cp310-manylinux1_x86_64.whl.metadata (1.8 kB)\nCollecting cloudpickle<3.0.0,>=1.1.0 (from azureml-dataprep<5.5.0a,>=5.1.0a->azureml-dataset-runtime)\n  Downloading cloudpickle-2.2.1-py3-none-any.whl.metadata (6.9 kB)\nCollecting azure-identity<=1.17.0,>=1.16.0 (from azureml-dataprep<5.5.0a,>=5.1.0a->azureml-dataset-runtime)\n  Downloading azure_identity-1.17.0-py3-none-any.whl.metadata (79 kB)\nRequirement already satisfied: jsonschema in /anaconda/envs/jupyter_env/lib/python3.10/site-packages (from azureml-dataprep<5.5.0a,>=5.1.0a->azureml-dataset-runtime) (4.24.0)\nRequirement already satisfied: pyyaml<7.0.0,>=5.1.0 in /anaconda/envs/jupyter_env/lib/python3.10/site-packages (from azureml-dataprep<5.5.0a,>=5.1.0a->azureml-dataset-runtime) (6.0.2)\nCollecting pip>=25.3 (from azureml-dataprep<5.5.0a,>=5.1.0a->azureml-dataset-runtime)\n  Downloading pip-25.3-py3-none-any.whl.metadata (4.7 kB)\nRequirement already satisfied: azure-core>=1.23.0 in /anaconda/envs/jupyter_env/lib/python3.10/site-packages (from azure-identity<=1.17.0,>=1.16.0->azureml-dataprep<5.5.0a,>=5.1.0a->azureml-dataset-runtime) (1.35.0)\nRequirement already satisfied: cryptography>=2.5 in /anaconda/envs/jupyter_env/lib/python3.10/site-packages (from azure-identity<=1.17.0,>=1.16.0->azureml-dataprep<5.5.0a,>=5.1.0a->azureml-dataset-runtime) (45.0.5)\nRequirement already satisfied: msal>=1.24.0 in /anaconda/envs/jupyter_env/lib/python3.10/site-packages (from azure-identity<=1.17.0,>=1.16.0->azureml-dataprep<5.5.0a,>=5.1.0a->azureml-dataset-runtime) (1.32.3)\nRequirement already satisfied: msal-extensions>=0.3.0 in /anaconda/envs/jupyter_env/lib/python3.10/site-packages (from azure-identity<=1.17.0,>=1.16.0->azureml-dataprep<5.5.0a,>=5.1.0a->azureml-dataset-runtime) (1.3.1)\nRequirement already satisfied: typing-extensions>=4.0.0 in /anaconda/envs/jupyter_env/lib/python3.10/site-packages (from azure-identity<=1.17.0,>=1.16.0->azureml-dataprep<5.5.0a,>=5.1.0a->azureml-dataset-runtime) (4.14.1)\nRequirement already satisfied: requests>=2.21.0 in /anaconda/envs/jupyter_env/lib/python3.10/site-packages (from azure-core>=1.23.0->azure-identity<=1.17.0,>=1.16.0->azureml-dataprep<5.5.0a,>=5.1.0a->azureml-dataset-runtime) (2.32.4)\nRequirement already satisfied: six>=1.11.0 in /anaconda/envs/jupyter_env/lib/python3.10/site-packages (from azure-core>=1.23.0->azure-identity<=1.17.0,>=1.16.0->azureml-dataprep<5.5.0a,>=5.1.0a->azureml-dataset-runtime) (1.17.0)\nRequirement already satisfied: cffi>=1.14 in /anaconda/envs/jupyter_env/lib/python3.10/site-packages (from cryptography>=2.5->azure-identity<=1.17.0,>=1.16.0->azureml-dataprep<5.5.0a,>=5.1.0a->azureml-dataset-runtime) (1.15.0)\nRequirement already satisfied: pycparser in /anaconda/envs/jupyter_env/lib/python3.10/site-packages (from cffi>=1.14->cryptography>=2.5->azure-identity<=1.17.0,>=1.16.0->azureml-dataprep<5.5.0a,>=5.1.0a->azureml-dataset-runtime) (2.22)\nRequirement already satisfied: PyJWT<3,>=1.0.0 in /anaconda/envs/jupyter_env/lib/python3.10/site-packages (from PyJWT[crypto]<3,>=1.0.0->msal>=1.24.0->azure-identity<=1.17.0,>=1.16.0->azureml-dataprep<5.5.0a,>=5.1.0a->azureml-dataset-runtime) (2.10.1)\nRequirement already satisfied: charset_normalizer<4,>=2 in /anaconda/envs/jupyter_env/lib/python3.10/site-packages (from requests>=2.21.0->azure-core>=1.23.0->azure-identity<=1.17.0,>=1.16.0->azureml-dataprep<5.5.0a,>=5.1.0a->azureml-dataset-runtime) (3.4.2)\nRequirement already satisfied: idna<4,>=2.5 in /anaconda/envs/jupyter_env/lib/python3.10/site-packages (from requests>=2.21.0->azure-core>=1.23.0->azure-identity<=1.17.0,>=1.16.0->azureml-dataprep<5.5.0a,>=5.1.0a->azureml-dataset-runtime) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /anaconda/envs/jupyter_env/lib/python3.10/site-packages (from requests>=2.21.0->azure-core>=1.23.0->azure-identity<=1.17.0,>=1.16.0->azureml-dataprep<5.5.0a,>=5.1.0a->azureml-dataset-runtime) (2.5.0)\nRequirement already satisfied: certifi>=2017.4.17 in /anaconda/envs/jupyter_env/lib/python3.10/site-packages (from requests>=2.21.0->azure-core>=1.23.0->azure-identity<=1.17.0,>=1.16.0->azureml-dataprep<5.5.0a,>=5.1.0a->azureml-dataset-runtime) (2025.7.9)\nRequirement already satisfied: attrs>=22.2.0 in /anaconda/envs/jupyter_env/lib/python3.10/site-packages (from jsonschema->azureml-dataprep<5.5.0a,>=5.1.0a->azureml-dataset-runtime) (25.3.0)\nRequirement already satisfied: jsonschema-specifications>=2023.03.6 in /anaconda/envs/jupyter_env/lib/python3.10/site-packages (from jsonschema->azureml-dataprep<5.5.0a,>=5.1.0a->azureml-dataset-runtime) (2025.4.1)\nRequirement already satisfied: referencing>=0.28.4 in /anaconda/envs/jupyter_env/lib/python3.10/site-packages (from jsonschema->azureml-dataprep<5.5.0a,>=5.1.0a->azureml-dataset-runtime) (0.36.2)\nRequirement already satisfied: rpds-py>=0.7.1 in /anaconda/envs/jupyter_env/lib/python3.10/site-packages (from jsonschema->azureml-dataprep<5.5.0a,>=5.1.0a->azureml-dataset-runtime) (0.22.3)\nDownloading azureml_dataset_runtime-1.61.0-py3-none-any.whl (2.3 kB)\nDownloading azureml_dataprep-5.4.2-py3-none-any.whl (253 kB)\nDownloading azure_identity-1.17.0-py3-none-any.whl (173 kB)\nDownloading azureml_dataprep_native-42.1.0-cp310-cp310-manylinux1_x86_64.whl (187 kB)\nDownloading azureml_dataprep_rslex-2.25.2-cp310-cp310-manylinux1_x86_64.whl (26.1 MB)\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m26.1/26.1 MB\u001b[0m \u001b[31m58.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading cloudpickle-2.2.1-py3-none-any.whl (25 kB)\nDownloading pip-25.3-py3-none-any.whl (1.8 MB)\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m97.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: azureml-dataprep-rslex, pip, cloudpickle, azureml-dataprep-native, azure-identity, azureml-dataprep, azureml-dataset-runtime\n\u001b[2K  Attempting uninstall: pip‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m0/7\u001b[0m [azureml-dataprep-rslex]\n\u001b[2K    Found existing installation: pip 25.1‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m0/7\u001b[0m [azureml-dataprep-rslex]\n\u001b[2K    Uninstalling pip-25.1:m\u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1/7\u001b[0m [pip]\n\u001b[2K      Successfully uninstalled pip-25.1‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1/7\u001b[0m [pip]\n\u001b[2K  Attempting uninstall: cloudpickle‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1/7\u001b[0m [pip]\n\u001b[2K    Found existing installation: cloudpickle 3.1.1‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1/7\u001b[0m [pip]\n\u001b[2K    Uninstalling cloudpickle-3.1.1:‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1/7\u001b[0m [pip]\n\u001b[2K      Successfully uninstalled cloudpickle-3.1.1‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1/7\u001b[0m [pip]\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m7/7\u001b[0m [azureml-dataset-runtime]l-dataprep]\n\u001b[1A\u001b[2KSuccessfully installed azure-identity-1.17.0 azureml-dataprep-5.4.2 azureml-dataprep-native-42.1.0 azureml-dataprep-rslex-2.25.2 azureml-dataset-runtime-1.61.0 cloudpickle-2.2.1 pip-25.3\nNote: you may need to restart the kernel to use updated packages.\n"
        }
      ],
      "execution_count": 5,
      "metadata": {
        "gather": {
          "logged": 1767550172034
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Configuration\n",
        "from datetime import datetime\n",
        "import pandas as pd\n",
        "from azureml.core import Workspace, Dataset,Datastore\n",
        "\n",
        "DATA_URL = 'https://storagecreditfraud.blob.core.windows.net/creditfraudprojec/fraud_dataset.csv'  \n",
        "TARGET_COLUMN = 'isFraud'\n",
        "\n",
        "print(\"üìÇ Loading dataset...\")\n",
        "print(f\"File: {DATA_URL}\")\n",
        "print(f\"Loading started at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
        "\n",
        "# Connect to Azure ML Workspace\n",
        "# ws = Workspace.from_config()\n",
        "# datastore = ws.get_default_datastore()\n",
        "\n",
        "# Load dataset FROM BLOB STORAGE\n",
        "# dataset = Dataset.Tabular.from_delimited_files(\n",
        "#     path=[(datastore, DATA_URL)]\n",
        "# )\n",
        "\n",
        "# df = dataset.to_pandas_dataframe()\n",
        "ws = Workspace.from_config()\n",
        "\n",
        "Datastore.register_azure_blob_container(\n",
        "    workspace=ws,\n",
        "    datastore_name=\"fraud_blob\",\n",
        "    container_name=\"creditfraudprojec\",\n",
        "    account_name=\"storagecreditfraud\",\n",
        "    sas_token=\"sp=r&st=2026-01-02T18:08:06Z&se=2026-01-03T02:23:06Z&sv=2024-11-04&sr=c&sig=QxVn9dAN%2BbrtLNFcDq4iCXwergxtKVw19QMxSsji%2FLM%3D\"\n",
        ")\n",
        "datastore = ws.datastores[\"fraud_blob\"]\n",
        "\n",
        "dataset = Dataset.Tabular.from_delimited_files(\n",
        "    path=[(datastore, \"fraud_dataset.csv\")]\n",
        ")\n",
        "\n",
        "df = dataset.to_pandas_dataframe()\n",
        "\n",
        "\n",
        "print(f\"‚úÖ Dataset loaded successfully!\")\n",
        "print(f\"Loading completed at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
        "\n",
        "# Display basic information\n",
        "print(\"=\"*60)\n",
        "print(\"DATASET OVERVIEW\")\n",
        "print(\"=\"*60)\n",
        "print(f\"Total rows: {df.shape[0]:,}\")\n",
        "print(f\"Total columns: {df.shape[1]:,}\")\n",
        "print(f\"Memory usage: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
        "print(\"\\nFirst few rows:\")\n",
        "display(df.head())\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "üìÇ Loading dataset...\nFile: https://storagecreditfraud.blob.core.windows.net/creditfraudprojec/fraud_dataset.csv\nLoading started at: 2026-01-04 18:09:31\n\n"
        },
        {
          "output_type": "error",
          "ename": "ContextualVersionConflict",
          "evalue": "(pip 25.1 (/anaconda/envs/jupyter_env/lib/python3.10/site-packages), Requirement.parse('pip>=25.3'), {'azureml-dataprep'})",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mContextualVersionConflict\u001b[0m                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[6], line 34\u001b[0m\n\u001b[1;32m     25\u001b[0m Datastore\u001b[38;5;241m.\u001b[39mregister_azure_blob_container(\n\u001b[1;32m     26\u001b[0m     workspace\u001b[38;5;241m=\u001b[39mws,\n\u001b[1;32m     27\u001b[0m     datastore_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfraud_blob\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     30\u001b[0m     sas_token\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msp=r&st=2026-01-02T18:08:06Z&se=2026-01-03T02:23:06Z&sv=2024-11-04&sr=c&sig=QxVn9dAN\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m2BbrtLNFcDq4iCXwergxtKVw19QMxSsji\u001b[39m\u001b[38;5;132;01m%2F\u001b[39;00m\u001b[38;5;124mLM\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m3D\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     31\u001b[0m )\n\u001b[1;32m     32\u001b[0m datastore \u001b[38;5;241m=\u001b[39m ws\u001b[38;5;241m.\u001b[39mdatastores[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfraud_blob\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m---> 34\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[43mDataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTabular\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_delimited_files\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdatastore\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfraud_dataset.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     38\u001b[0m df \u001b[38;5;241m=\u001b[39m dataset\u001b[38;5;241m.\u001b[39mto_pandas_dataframe()\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m‚úÖ Dataset loaded successfully!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "File \u001b[0;32m/anaconda/envs/jupyter_env/lib/python3.10/site-packages/azureml/data/_loggerfactory.py:140\u001b[0m, in \u001b[0;36mtrack.<locals>.monitor.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _LoggerFactory\u001b[38;5;241m.\u001b[39mtrack_activity(logger, func\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, activity_type, custom_dimensions) \u001b[38;5;28;01mas\u001b[39;00m al:\n\u001b[1;32m    139\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 140\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    141\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    142\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(al, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mactivity_info\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(e, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124merror_code\u001b[39m\u001b[38;5;124m'\u001b[39m):\n",
            "File \u001b[0;32m/anaconda/envs/jupyter_env/lib/python3.10/site-packages/azureml/data/dataset_factory.py:340\u001b[0m, in \u001b[0;36mTabularDatasetFactory.from_delimited_files\u001b[0;34m(path, validate, include_path, infer_column_types, set_column_types, separator, header, partition_format, support_multi_line, empty_as_string, encoding)\u001b[0m\n\u001b[1;32m    338\u001b[0m path \u001b[38;5;241m=\u001b[39m _validate_and_normalize_path(path)\n\u001b[1;32m    339\u001b[0m _trace_dataset_creation(path)\n\u001b[0;32m--> 340\u001b[0m dataflow \u001b[38;5;241m=\u001b[39m \u001b[43mdataprep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mread_csv(path,\n\u001b[1;32m    341\u001b[0m                                verify_exists\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    342\u001b[0m                                include_path\u001b[38;5;241m=\u001b[39minclude_path,\n\u001b[1;32m    343\u001b[0m                                infer_column_types\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    344\u001b[0m                                separator\u001b[38;5;241m=\u001b[39mseparator,\n\u001b[1;32m    345\u001b[0m                                header\u001b[38;5;241m=\u001b[39mheader,\n\u001b[1;32m    346\u001b[0m                                quoting\u001b[38;5;241m=\u001b[39msupport_multi_line,\n\u001b[1;32m    347\u001b[0m                                empty_as_string\u001b[38;5;241m=\u001b[39mempty_as_string,\n\u001b[1;32m    348\u001b[0m                                encoding\u001b[38;5;241m=\u001b[39mencoding)\n\u001b[1;32m    350\u001b[0m dataflow \u001b[38;5;241m=\u001b[39m _transform_and_validate(\n\u001b[1;32m    351\u001b[0m     dataflow\u001b[38;5;241m=\u001b[39mdataflow,\n\u001b[1;32m    352\u001b[0m     partition_format\u001b[38;5;241m=\u001b[39mpartition_format,\n\u001b[1;32m    353\u001b[0m     validate\u001b[38;5;241m=\u001b[39mvalidate,\n\u001b[1;32m    354\u001b[0m     infer_column_types\u001b[38;5;241m=\u001b[39minfer_column_types \u001b[38;5;129;01mor\u001b[39;00m _is_inference_required(set_column_types, validate))\n\u001b[1;32m    355\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m infer_column_types:\n",
            "File \u001b[0;32m/anaconda/envs/jupyter_env/lib/python3.10/site-packages/azureml/data/_dataprep_helper.py:38\u001b[0m, in \u001b[0;36mdataprep\u001b[0;34m()\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(get_dataprep_missing_message())\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mazureml\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdataprep\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m_dprep\u001b[39;00m\n\u001b[0;32m---> 38\u001b[0m \u001b[43mcheck_min_version\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _dprep\n",
            "File \u001b[0;32m/anaconda/envs/jupyter_env/lib/python3.10/site-packages/azureml/data/_dataprep_helper.py:20\u001b[0m, in \u001b[0;36mcheck_min_version\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpkg_resources\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m parse_version, get_distribution\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlogging\u001b[39;00m\n\u001b[0;32m---> 20\u001b[0m installed_version \u001b[38;5;241m=\u001b[39m \u001b[43mget_distribution\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mazureml-dataprep\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mversion\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m parse_version(installed_version) \u001b[38;5;241m<\u001b[39m parse_version(MIN_DATAPREP_VERSION):\n\u001b[1;32m     22\u001b[0m     logging\u001b[38;5;241m.\u001b[39mgetLogger()\u001b[38;5;241m.\u001b[39mwarning(\n\u001b[1;32m     23\u001b[0m         _dataprep_incompatible_version_error\u001b[38;5;241m.\u001b[39mformat(MIN_DATAPREP_VERSION, installed_version))\n",
            "File \u001b[0;32m/anaconda/envs/jupyter_env/lib/python3.10/site-packages/pkg_resources/__init__.py:527\u001b[0m, in \u001b[0;36mget_distribution\u001b[0;34m(dist)\u001b[0m\n\u001b[1;32m    525\u001b[0m     dist \u001b[38;5;241m=\u001b[39m Requirement\u001b[38;5;241m.\u001b[39mparse(dist)\n\u001b[1;32m    526\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(dist, Requirement):\n\u001b[0;32m--> 527\u001b[0m     dist \u001b[38;5;241m=\u001b[39m \u001b[43mget_provider\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdist\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    528\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(dist, Distribution):\n\u001b[1;32m    529\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected str, Requirement, or Distribution\u001b[39m\u001b[38;5;124m\"\u001b[39m, dist)\n",
            "File \u001b[0;32m/anaconda/envs/jupyter_env/lib/python3.10/site-packages/pkg_resources/__init__.py:414\u001b[0m, in \u001b[0;36mget_provider\u001b[0;34m(moduleOrReq)\u001b[0m\n\u001b[1;32m    412\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Return an IResourceProvider for the named module or requirement\"\"\"\u001b[39;00m\n\u001b[1;32m    413\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(moduleOrReq, Requirement):\n\u001b[0;32m--> 414\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m working_set\u001b[38;5;241m.\u001b[39mfind(moduleOrReq) \u001b[38;5;129;01mor\u001b[39;00m \u001b[43mrequire\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmoduleOrReq\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    415\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    416\u001b[0m     module \u001b[38;5;241m=\u001b[39m sys\u001b[38;5;241m.\u001b[39mmodules[moduleOrReq]\n",
            "File \u001b[0;32m/anaconda/envs/jupyter_env/lib/python3.10/site-packages/pkg_resources/__init__.py:1066\u001b[0m, in \u001b[0;36mWorkingSet.require\u001b[0;34m(self, *requirements)\u001b[0m\n\u001b[1;32m   1057\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mrequire\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39mrequirements: _NestedStr) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mlist\u001b[39m[Distribution]:\n\u001b[1;32m   1058\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Ensure that distributions matching `requirements` are activated\u001b[39;00m\n\u001b[1;32m   1059\u001b[0m \n\u001b[1;32m   1060\u001b[0m \u001b[38;5;124;03m    `requirements` must be a string or a (possibly-nested) sequence\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1064\u001b[0m \u001b[38;5;124;03m    included, even if they were already activated in this working set.\u001b[39;00m\n\u001b[1;32m   1065\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1066\u001b[0m     needed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresolve\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparse_requirements\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequirements\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1068\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m dist \u001b[38;5;129;01min\u001b[39;00m needed:\n\u001b[1;32m   1069\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madd(dist)\n",
            "File \u001b[0;32m/anaconda/envs/jupyter_env/lib/python3.10/site-packages/pkg_resources/__init__.py:893\u001b[0m, in \u001b[0;36mWorkingSet.resolve\u001b[0;34m(self, requirements, env, installer, replace_conflicting, extras)\u001b[0m\n\u001b[1;32m    890\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m req_extras\u001b[38;5;241m.\u001b[39mmarkers_pass(req, extras):\n\u001b[1;32m    891\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m--> 893\u001b[0m dist \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_resolve_dist\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    894\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreplace_conflicting\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minstaller\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequired_by\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mto_activate\u001b[49m\n\u001b[1;32m    895\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    897\u001b[0m \u001b[38;5;66;03m# push the new requirements onto the stack\u001b[39;00m\n\u001b[1;32m    898\u001b[0m new_requirements \u001b[38;5;241m=\u001b[39m dist\u001b[38;5;241m.\u001b[39mrequires(req\u001b[38;5;241m.\u001b[39mextras)[::\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n",
            "File \u001b[0;32m/anaconda/envs/jupyter_env/lib/python3.10/site-packages/pkg_resources/__init__.py:939\u001b[0m, in \u001b[0;36mWorkingSet._resolve_dist\u001b[0;34m(self, req, best, replace_conflicting, env, installer, required_by, to_activate)\u001b[0m\n\u001b[1;32m    936\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dist \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m req:\n\u001b[1;32m    937\u001b[0m     \u001b[38;5;66;03m# Oops, the \"best\" so far conflicts with a dependency\u001b[39;00m\n\u001b[1;32m    938\u001b[0m     dependent_req \u001b[38;5;241m=\u001b[39m required_by[req]\n\u001b[0;32m--> 939\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m VersionConflict(dist, req)\u001b[38;5;241m.\u001b[39mwith_context(dependent_req)\n\u001b[1;32m    940\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m dist\n",
            "\u001b[0;31mContextualVersionConflict\u001b[0m: (pip 25.1 (/anaconda/envs/jupyter_env/lib/python3.10/site-packages), Requirement.parse('pip>=25.3'), {'azureml-dataprep'})"
          ]
        }
      ],
      "execution_count": 6,
      "metadata": {
        "trusted": true,
        "gather": {
          "logged": 1767550175258
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Examine data types and missing values\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"DATA QUALITY CHECK\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "print(\"\\nüìã Column Information:\")\n",
        "df.info()\n",
        "\n",
        "print(\"\\n‚ùì Missing Values:\")\n",
        "missing = df.isnull().sum()\n",
        "missing_pct = 100 * missing / len(df)\n",
        "missing_df = pd.DataFrame({\n",
        "    'Missing Count': missing,\n",
        "    'Percentage': missing_pct\n",
        "}).sort_values('Missing Count', ascending=False)\n",
        "print(missing_df[missing_df['Missing Count'] > 0])\n",
        "\n",
        "print(\"\\nüìä Basic Statistics:\")\n",
        "display(df.describe())"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "trusted": true,
        "gather": {
          "logged": 1767550176201
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üéØ 3. Analyze Class Distribution (Before Truncation)"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "print(\"=\"*60)\n",
        "print(\"CLASS DISTRIBUTION ANALYSIS\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Count fraud vs non-fraud cases\n",
        "fraud_counts = df[TARGET_COLUMN].value_counts()\n",
        "fraud_pct = df[TARGET_COLUMN].value_counts(normalize=True) * 100\n",
        "\n",
        "print(\"\\nüìä Original Dataset Distribution:\")\n",
        "print(f\"\\nNon-Fraud (0): {fraud_counts[0]:,} ({fraud_pct[0]:.2f}%)\")\n",
        "print(f\"Fraud (1):     {fraud_counts[1]:,} ({fraud_pct[1]:.2f}%)\")\n",
        "print(f\"\\nImbalance Ratio: 1:{fraud_counts[0]/fraud_counts[1]:.1f}\")\n",
        "print(f\"Total samples: {len(df):,}\")\n",
        "\n",
        "# Visualization\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# Bar plot\n",
        "fraud_counts.plot(kind='bar', ax=axes[0], color=['#2ecc71', '#e74c3c'])\n",
        "axes[0].set_title('Class Distribution (Count)', fontsize=14, fontweight='bold')\n",
        "axes[0].set_xlabel('Class (0=Non-Fraud, 1=Fraud)')\n",
        "axes[0].set_ylabel('Count')\n",
        "axes[0].set_xticklabels(['Non-Fraud', 'Fraud'], rotation=0)\n",
        "for i, v in enumerate(fraud_counts):\n",
        "    axes[0].text(i, v + max(fraud_counts)*0.02, f'{v:,}', ha='center', fontweight='bold')\n",
        "\n",
        "# Pie chart\n",
        "axes[1].pie(fraud_counts, labels=['Non-Fraud', 'Fraud'], autopct='%1.2f%%',\n",
        "            colors=['#2ecc71', '#e74c3c'], startangle=90, textprops={'fontweight': 'bold'})\n",
        "axes[1].set_title('Class Distribution (Percentage)', fontsize=14, fontweight='bold')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n‚ö†Ô∏è This dataset is highly imbalanced - balancing is necessary!\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "trusted": true,
        "gather": {
          "logged": 1767550176229
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ‚úÇÔ∏è 4. Strategic Dataset Truncation\n",
        "\n",
        "**Strategy:**\n",
        "- Keep ALL fraud cases (minority class)\n",
        "- Sample 500,000 non-fraud cases (majority class)\n",
        "- This reduces computational load while preserving all fraud patterns"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"=\"*60)\n",
        "print(\"DATASET TRUNCATION\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Separate fraud and non-fraud cases\n",
        "fraud_cases = df[df[TARGET_COLUMN] == 1]\n",
        "non_fraud_cases = df[df[TARGET_COLUMN] == 0]\n",
        "\n",
        "print(f\"\\nüìä Original Split:\")\n",
        "print(f\"Fraud cases:     {len(fraud_cases):,}\")\n",
        "print(f\"Non-fraud cases: {len(non_fraud_cases):,}\")\n",
        "\n",
        "# Keep all frauds, sample non-frauds\n",
        "NON_FRAUD_SAMPLE_SIZE = 500_000\n",
        "\n",
        "print(f\"\\n‚úÇÔ∏è Truncation Strategy:\")\n",
        "print(f\"‚úì Keep ALL {len(fraud_cases):,} fraud cases\")\n",
        "print(f\"‚úì Sample {NON_FRAUD_SAMPLE_SIZE:,} non-fraud cases\")\n",
        "\n",
        "# Random sampling of non-fraud cases\n",
        "non_fraud_sampled = non_fraud_cases.sample(\n",
        "    n=min(NON_FRAUD_SAMPLE_SIZE, len(non_fraud_cases)),\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Combine truncated datasets\n",
        "df_truncated = pd.concat([fraud_cases, non_fraud_sampled], axis=0)\n",
        "df_truncated = df_truncated.sample(frac=1, random_state=42).reset_index(drop=True)  # Shuffle\n",
        "\n",
        "print(f\"\\n‚úÖ Truncation Complete!\")\n",
        "print(f\"\\nüìä Truncated Dataset:\")\n",
        "print(f\"Total rows: {len(df_truncated):,}\")\n",
        "print(f\"Fraud:      {len(df_truncated[df_truncated[TARGET_COLUMN] == 1]):,}\")\n",
        "print(f\"Non-fraud:  {len(df_truncated[df_truncated[TARGET_COLUMN] == 0]):,}\")\n",
        "\n",
        "truncated_ratio = len(df_truncated[df_truncated[TARGET_COLUMN] == 0]) / len(df_truncated[df_truncated[TARGET_COLUMN] == 1])\n",
        "print(f\"\\nNew Imbalance Ratio: 1:{truncated_ratio:.1f}\")\n",
        "print(f\"Size reduction: {100 * (1 - len(df_truncated)/len(df)):.1f}%\")\n",
        "\n",
        "# Memory cleanup\n",
        "del df, fraud_cases, non_fraud_cases, non_fraud_sampled\n",
        "import gc\n",
        "gc.collect()\n",
        "\n",
        "print(\"\\nüßπ Memory cleaned up\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "trusted": true,
        "gather": {
          "logged": 1767550176258
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üîß 5. Feature Engineering & Preprocessing"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "!{sys.executable} -m pip install category_encoders"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1767550176279
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Assuming df_truncated is already loaded\n",
        "TARGET_COLUMN = 'isFraud'  # Replace with your actual target column name\n",
        "\n",
        "# 1Ô∏è‚É£ Identify categorical columns\n",
        "categorical_cols = df_truncated.select_dtypes(include=['object', 'category']).columns\n",
        "numeric_cols = df_truncated.select_dtypes(include=[np.number]).columns\n",
        "\n",
        "# 2Ô∏è‚É£ Handle missing values\n",
        "print(\"=\"*60)\n",
        "print(\"FEATURE ENGINEERING - Missing Values\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "for col in numeric_cols:\n",
        "    if df_truncated[col].isnull().sum() > 0:\n",
        "        df_truncated[col].fillna(df_truncated[col].median(), inplace=True)\n",
        "        print(f\"Filled missing values in {col} with median\")\n",
        "\n",
        "for col in categorical_cols:\n",
        "    if df_truncated[col].isnull().sum() > 0:\n",
        "        df_truncated[col].fillna(df_truncated[col].mode()[0], inplace=True)\n",
        "        print(f\"Filled missing values in {col} with mode\")\n",
        "\n",
        "# 3Ô∏è‚É£ Encode categorical columns using Target Encoding\n",
        "!pip install -q category_encoders\n",
        "import category_encoders as ce\n",
        "\n",
        "if len(categorical_cols) > 0:\n",
        "    print(f\"\\nüî§ Encoding {len(categorical_cols)} categorical columns using Target Encoding...\")\n",
        "    encoder = ce.TargetEncoder(cols=categorical_cols)\n",
        "    df_truncated[categorical_cols] = encoder.fit_transform(df_truncated[categorical_cols], df_truncated[TARGET_COLUMN])\n",
        "    print(\"‚úì Encoding complete\")\n",
        "else:\n",
        "    print(\"No categorical columns to encode\")\n",
        "\n",
        "# 4Ô∏è‚É£ Optional: Scale numeric columns\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "scaler = StandardScaler()\n",
        "df_truncated[numeric_cols] = scaler.fit_transform(df_truncated[numeric_cols])\n",
        "df_processed = df_truncated.copy()\n",
        "\n",
        "print(\"\\n‚úì Feature scaling complete\")\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "trusted": true,
        "gather": {
          "logged": 1767550176295
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üéØ 6. Data Balancing with SMOTE\n",
        "\n",
        "**Multiple balancing strategies available:**\n",
        "- **SMOTE**: Synthetic Minority Over-sampling Technique\n",
        "- **ADASYN**: Adaptive Synthetic Sampling\n",
        "- **BorderlineSMOTE**: Focus on borderline cases\n",
        "- **SMOTETomek**: SMOTE + Tomek Links cleaning\n",
        "\n",
        "We'll use **SMOTE** with customizable ratio (40:60, 45:55, or 50:50)"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "import imblearn\n",
        "from imblearn.over_sampling import SMOTE\n",
        "print(imblearn.__file__)\n",
        "print(SMOTE)\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1767550176321
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from imblearn.over_sampling import SMOTE\n",
        "print(\"=\"*60)\n",
        "print(\"DATA BALANCING WITH SMOTE\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Separate features and target\n",
        "X = df_processed.drop(columns=[TARGET_COLUMN])\n",
        "y = df_processed[TARGET_COLUMN]\n",
        "\n",
        "print(f\"\\nüìä Before Balancing:\")\n",
        "print(f\"Features shape: {X.shape}\")\n",
        "print(f\"Target distribution:\")\n",
        "print(y.value_counts())\n",
        "print(f\"Fraud ratio: {100 * y.mean():.2f}%\")\n",
        "\n",
        "# Choose your desired balance ratio\n",
        "# Options: 0.4 (40%), 0.45 (45%), 0.5 (50%)\n",
        "DESIRED_FRAUD_RATIO = 0.40  # ‚ö†Ô∏è ADJUST THIS (0.40 = 40/60 split)\n",
        "\n",
        "# Calculate sampling strategy\n",
        "# sampling_strategy: ratio of minority class to majority class after resampling\n",
        "# For 40% fraud: we want fraud/(non-fraud) = 0.4/0.6 = 0.667\n",
        "sampling_strategy = DESIRED_FRAUD_RATIO / (1 - DESIRED_FRAUD_RATIO)\n",
        "\n",
        "print(f\"\\nüéØ Target Balance:\")\n",
        "print(f\"Desired fraud ratio: {DESIRED_FRAUD_RATIO*100:.0f}%\")\n",
        "print(f\"Sampling strategy: {sampling_strategy:.3f}\")\n",
        "\n",
        "# Apply SMOTE\n",
        "print(f\"\\n‚öôÔ∏è Applying SMOTE...\")\n",
        "smote = SMOTE(\n",
        "    sampling_strategy=sampling_strategy,\n",
        "    random_state=42,\n",
        "    k_neighbors=5)\n",
        "\n",
        "X_balanced, y_balanced = smote.fit_resample(X, y)\n",
        "\n",
        "print(f\"\\n‚úÖ SMOTE Complete!\")\n",
        "print(f\"\\nüìä After Balancing:\")\n",
        "print(f\"Features shape: {X_balanced.shape}\")\n",
        "print(f\"Target distribution:\")\n",
        "print(pd.Series(y_balanced).value_counts())\n",
        "fraud_pct_balanced = 100 * pd.Series(y_balanced).mean()\n",
        "print(f\"\\nFraud ratio: {fraud_pct_balanced:.2f}%\")\n",
        "print(f\"Total samples: {len(X_balanced):,}\")\n",
        "print(f\"Synthetic samples created: {len(X_balanced) - len(X):,}\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "trusted": true,
        "gather": {
          "logged": 1767550176345
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize the balancing effect\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# Before SMOTE\n",
        "y.value_counts().plot(kind='bar', ax=axes[0], color=['#2ecc71', '#e74c3c'])\n",
        "axes[0].set_title('Before SMOTE', fontsize=14, fontweight='bold')\n",
        "axes[0].set_xlabel('Class')\n",
        "axes[0].set_ylabel('Count')\n",
        "axes[0].set_xticklabels(['Non-Fraud', 'Fraud'], rotation=0)\n",
        "for i, v in enumerate(y.value_counts()):\n",
        "    axes[0].text(i, v + max(y.value_counts())*0.02, f'{v:,}', ha='center', fontweight='bold')\n",
        "\n",
        "# After SMOTE\n",
        "pd.Series(y_balanced).value_counts().plot(kind='bar', ax=axes[1], color=['#2ecc71', '#e74c3c'])\n",
        "axes[1].set_title('After SMOTE', fontsize=14, fontweight='bold')\n",
        "axes[1].set_xlabel('Class')\n",
        "axes[1].set_ylabel('Count')\n",
        "axes[1].set_xticklabels(['Non-Fraud', 'Fraud'], rotation=0)\n",
        "for i, v in enumerate(pd.Series(y_balanced).value_counts()):\n",
        "    axes[1].text(i, v + max(pd.Series(y_balanced).value_counts())*0.02, f'{v:,}', ha='center', fontweight='bold')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nüìà Dataset is now balanced and ready for training!\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "trusted": true,
        "gather": {
          "logged": 1767550176389
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üîÄ 7. Train-Test Split & Scaling"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import RobustScaler\n",
        "\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"TRAIN-TEST SPLIT & FEATURE SCALING\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Split the balanced dataset\n",
        "TEST_SIZE = 0.2  # 80% train, 20% test\n",
        "RANDOM_STATE = 42\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X_balanced, y_balanced,\n",
        "    test_size=TEST_SIZE,\n",
        "    random_state=RANDOM_STATE,\n",
        "    stratify=y_balanced  # Maintain class distribution\n",
        ")\n",
        "\n",
        "print(f\"\\n‚úÖ Data split complete:\")\n",
        "print(f\"\\nTraining set:   {X_train.shape[0]:,} samples ({100*(1-TEST_SIZE):.0f}%)\")\n",
        "print(f\"Test set:       {X_test.shape[0]:,} samples ({100*TEST_SIZE:.0f}%)\")\n",
        "print(f\"Number of features: {X_train.shape[1]}\")\n",
        "\n",
        "print(f\"\\nüìä Class distribution:\")\n",
        "print(f\"Train - Fraud: {pd.Series(y_train).mean()*100:.2f}%\")\n",
        "print(f\"Test  - Fraud: {pd.Series(y_test).mean()*100:.2f}%\")\n",
        "\n",
        "# Feature Scaling (Important for many algorithms)\n",
        "print(f\"\\n‚öôÔ∏è Applying feature scaling...\")\n",
        "\n",
        "# RobustScaler is better for data with outliers (common in fraud detection)\n",
        "scaler = RobustScaler()\n",
        "# Alternative: StandardScaler() for normal distribution\n",
        "\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "print(\"‚úÖ Scaling complete using RobustScaler\")\n",
        "print(\"\\nüì¶ Data is now ready for model training!\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "trusted": true,
        "gather": {
          "logged": 1767550176422
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ü§ñ 8. Model Training & Evaluation\n",
        "\n",
        "We'll train multiple models and compare their performance:\n",
        "1. Logistic Regression (baseline)\n",
        "2. Random Forest\n",
        "3. Gradient Boosting\n",
        "4. XGBoost\n",
        "5. LightGBM"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# PARTIE 1: CONFIGURATION AZURE ML + MLFLOW\n",
        "# =============================================================================\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\" \"*20 + \"üöÄ AZURE ML - MODEL TRAINING PIPELINE\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Imports essentiels\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datetime import datetime\n",
        "import json\n",
        "import os\n",
        "\n",
        "# Azure ML\n",
        "from azureml.core import Workspace, Experiment, Run, Dataset, Model\n",
        "from azureml.core.compute import ComputeTarget\n",
        "import mlflow\n",
        "import mlflow.sklearn\n",
        "from mlflow.tracking import MlflowClient\n",
        "\n",
        "# Machine Learning\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from xgboost import XGBClassifier\n",
        "from lightgbm import LGBMClassifier\n",
        "\n",
        "# M√©triques\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, precision_score, recall_score, f1_score,\n",
        "    roc_auc_score, confusion_matrix, classification_report,\n",
        "    roc_curve, precision_recall_curve\n",
        ")\n",
        "\n",
        "# Visualisation\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "sns.set_style('whitegrid')\n",
        "\n",
        "print(\"\\n‚úÖ Toutes les biblioth√®ques import√©es avec succ√®s!\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1767550176450
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install azureml-mlflow"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1767550176472
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# CONNEXION AU WORKSPACE AZURE ML\n",
        "# =============================================================================\n",
        "import mlflow\n",
        "import mlflow.sklearn\n",
        "\n",
        "from azureml.core import Workspace, Experiment, Run\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"üì° CONNEXION √Ä AZURE ML WORKSPACE\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "try:\n",
        "    # M√©thode 1: Depuis le contexte du run (si dans Azure ML Compute)\n",
        "    run_context = Run.get_context()\n",
        "    \n",
        "    if hasattr(run_context, 'experiment'):\n",
        "        ws = run_context.experiment.workspace\n",
        "        print(\"‚úÖ Connect√© depuis Azure ML Compute Instance\")\n",
        "    else:\n",
        "        # M√©thode 2: Depuis config.json local\n",
        "        ws = Workspace.from_config()\n",
        "        print(\"‚úÖ Connect√© depuis configuration locale\")\n",
        "    \n",
        "    print(f\"\\nüìã Workspace Details:\")\n",
        "    print(f\"   Nom: {ws.name}\")\n",
        "    print(f\"   R√©gion: {ws.location}\")\n",
        "    print(f\"   Resource Group: {ws.resource_group}\")\n",
        "    print(f\"   Subscription ID: {ws.subscription_id[:8]}...\")\n",
        "    \n",
        "    # Configuration MLflow pour Azure ML\n",
        "    mlflow_uri = ws.get_mlflow_tracking_uri()\n",
        "    mlflow.set_tracking_uri(mlflow_uri)\n",
        "    print(f\"\\nüîó MLflow Tracking URI configur√©: {mlflow_uri}\")\n",
        "    \n",
        "    AZURE_ML_CONNECTED = True\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"‚ö†Ô∏è Erreur de connexion: {str(e)}\")\n",
        "    print(\"üí° Utilisation du mode local pour d√©monstration\")\n",
        "    AZURE_ML_CONNECTED = False\n",
        "    mlflow.set_tracking_uri(\"./mlruns\")\n",
        "\n",
        "# Cr√©er/R√©cup√©rer l'exp√©rience\n",
        "EXPERIMENT_NAME = \"fraud-detection-training\"\n",
        "experiment = Experiment(workspace=ws, name=EXPERIMENT_NAME) if AZURE_ML_CONNECTED else None\n",
        "\n",
        "if AZURE_ML_CONNECTED:\n",
        "    print(f\"\\nüß™ Exp√©rience: {EXPERIMENT_NAME}\")\n",
        "    mlflow.set_experiment(EXPERIMENT_NAME)\n",
        "else:\n",
        "    mlflow.set_experiment(\"fraud-detection-local\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1767550176491
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# PARTIE 2: CHARGEMENT DES DONN√âES (depuis Azure Blob Storage)\n",
        "# =============================================================================\n",
        "import pandas as pd\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"üìÇ CHARGEMENT DES DONN√âES DEPUIS AZURE BLOB STORAGE\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Supposons que X_train_scaled, X_test_scaled, y_train, y_test sont d√©j√† pr√©par√©s\n",
        "# Si ce n'est pas le cas, d√©commentez le code ci-dessous:\n",
        "\n",
        "\"\"\"\n",
        "# Charger depuis Azure Blob Storage\n",
        "datastore = ws.get_default_datastore()\n",
        "dataset = Dataset.Tabular.from_delimited_files(\n",
        "    path=[(datastore, 'fraud_dataset_processed.csv')]\n",
        ")\n",
        "df = dataset.to_pandas_dataframe()\n",
        "\n",
        "# S√©parer features et target\n",
        "X = df.drop(columns=['isFraud'])\n",
        "y = df['isFraud']\n",
        "\n",
        "# Split train/test\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "# Scaling\n",
        "from sklearn.preprocessing import RobustScaler\n",
        "scaler = RobustScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\"\"\"\n",
        "\n",
        "print(f\"‚úÖ Donn√©es charg√©es:\")\n",
        "print(f\"   Training samples: {len(X_train_scaled):,}\")\n",
        "print(f\"   Test samples: {len(X_test_scaled):,}\")\n",
        "print(f\"   Features: {X_train_scaled.shape[1]}\")\n",
        "print(f\"   Fraud ratio (train): {pd.Series(y_train).mean()*100:.2f}%\")\n",
        "\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1767550176519
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# PARTIE 3: FONCTION D'√âVALUATION COMPL√àTE\n",
        "# =============================================================================\n",
        "\n",
        "def evaluate_model_comprehensive(model, X_test, y_test, model_name=\"Model\"):\n",
        "    \"\"\"\n",
        "    √âvaluation compl√®te du mod√®le avec visualisations professionnelles\n",
        "    \n",
        "    Returns:\n",
        "        dict: M√©triques de performance\n",
        "        tuple: Figures matplotlib pour logging\n",
        "    \"\"\"\n",
        "    \n",
        "    # Pr√©dictions\n",
        "    y_pred = model.predict(X_test)\n",
        "    y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
        "    \n",
        "    # Calcul des m√©triques\n",
        "    metrics = {\n",
        "        'accuracy': accuracy_score(y_test, y_pred),\n",
        "        'precision': precision_score(y_test, y_pred, zero_division=0),\n",
        "        'recall': recall_score(y_test, y_pred, zero_division=0),\n",
        "        'f1_score': f1_score(y_test, y_pred, zero_division=0),\n",
        "        'roc_auc': roc_auc_score(y_test, y_pred_proba)\n",
        "    }\n",
        "    \n",
        "    # Confusion Matrix\n",
        "    cm = confusion_matrix(y_test, y_pred)\n",
        "    \n",
        "    # M√©triques suppl√©mentaires pour fraude\n",
        "    tn, fp, fn, tp = cm.ravel()\n",
        "    metrics['true_negatives'] = int(tn)\n",
        "    metrics['false_positives'] = int(fp)\n",
        "    metrics['false_negatives'] = int(fn)\n",
        "    metrics['true_positives'] = int(tp)\n",
        "    metrics['fraud_detection_rate'] = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
        "    metrics['false_alarm_rate'] = fp / (fp + tn) if (fp + tn) > 0 else 0\n",
        "    \n",
        "    # Affichage des r√©sultats\n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(f\"üìä {model_name} - R√âSULTATS D'√âVALUATION\")\n",
        "    print(f\"{'='*70}\")\n",
        "    print(f\"Accuracy:  {metrics['accuracy']:.4f} ({metrics['accuracy']*100:.2f}%)\")\n",
        "    print(f\"Precision: {metrics['precision']:.4f} (Qualit√© des alertes fraude)\")\n",
        "    print(f\"Recall:    {metrics['recall']:.4f} (D√©tection des fraudes)\")\n",
        "    print(f\"F1-Score:  {metrics['f1_score']:.4f} (Balance Precision/Recall)\")\n",
        "    print(f\"ROC-AUC:   {metrics['roc_auc']:.4f} (Capacit√© de discrimination)\")\n",
        "    \n",
        "    print(f\"\\nüéØ M√©triques m√©tier sp√©cifiques:\")\n",
        "    print(f\"Taux de d√©tection fraude: {metrics['fraud_detection_rate']*100:.2f}%\")\n",
        "    print(f\"Taux de fausses alertes:  {metrics['false_alarm_rate']*100:.2f}%\")\n",
        "    \n",
        "    print(f\"\\nüî¢ Confusion Matrix:\")\n",
        "    print(f\"   TN: {tn:,}  |  FP: {fp:,}\")\n",
        "    print(f\"   FN: {fn:,}  |  TP: {tp:,}\")\n",
        "    \n",
        "    # Visualisations\n",
        "    fig = plt.figure(figsize=(18, 5))\n",
        "    \n",
        "    # 1. Confusion Matrix\n",
        "    ax1 = plt.subplot(1, 3, 1)\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
        "                xticklabels=['Non-Fraude', 'Fraude'],\n",
        "                yticklabels=['Non-Fraude', 'Fraude'],\n",
        "                ax=ax1, cbar_kws={'label': 'Nombre de cas'})\n",
        "    ax1.set_title(f'{model_name}\\nMatrice de Confusion', fontweight='bold', fontsize=12)\n",
        "    ax1.set_ylabel('Vraie Classe')\n",
        "    ax1.set_xlabel('Classe Pr√©dite')\n",
        "    \n",
        "    # 2. ROC Curve\n",
        "    ax2 = plt.subplot(1, 3, 2)\n",
        "    fpr, tpr, _ = roc_curve(y_test, y_pred_proba)\n",
        "    ax2.plot(fpr, tpr, linewidth=2.5, \n",
        "             label=f'ROC (AUC = {metrics[\"roc_auc\"]:.3f})', color='#e74c3c')\n",
        "    ax2.plot([0, 1], [0, 1], 'k--', linewidth=1.5, label='Hasard')\n",
        "    ax2.set_xlabel('Taux de Faux Positifs (FPR)')\n",
        "    ax2.set_ylabel('Taux de Vrais Positifs (TPR)')\n",
        "    ax2.set_title(f'{model_name}\\nCourbe ROC', fontweight='bold', fontsize=12)\n",
        "    ax2.legend(loc='lower right')\n",
        "    ax2.grid(alpha=0.3)\n",
        "    \n",
        "    # 3. Precision-Recall Curve\n",
        "    ax3 = plt.subplot(1, 3, 3)\n",
        "    precision_vals, recall_vals, _ = precision_recall_curve(y_test, y_pred_proba)\n",
        "    ax3.plot(recall_vals, precision_vals, linewidth=2.5, color='#2ecc71')\n",
        "    ax3.set_xlabel('Recall (Taux de d√©tection)')\n",
        "    ax3.set_ylabel('Precision (Qualit√© des alertes)')\n",
        "    ax3.set_title(f'{model_name}\\nCourbe Precision-Recall', fontweight='bold', fontsize=12)\n",
        "    ax3.grid(alpha=0.3)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show() \n",
        "    return metrics, fig\n",
        "\n",
        "print(\"\\n‚úÖ Fonction d'√©valuation configur√©e\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1767550176552
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# PARTIE 4: CONFIGURATION DES MOD√àLES (6. MODEL TRAINING)\n",
        "# =============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"ü§ñ CONFIGURATION DES MOD√àLES DE MACHINE LEARNING\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Configuration des mod√®les avec hyperparam√®tres optimis√©s pour d√©tection de fraude\n",
        "models_config = {\n",
        "    'Logistic_Regression': {\n",
        "        'model': LogisticRegression(\n",
        "            max_iter=1000,\n",
        "            random_state=42,\n",
        "            n_jobs=-1,\n",
        "            class_weight='balanced',  # Important pour classes d√©s√©quilibr√©es\n",
        "            solver='saga',\n",
        "            penalty='l2'\n",
        "        ),\n",
        "        'description': 'Mod√®le lin√©aire baseline - Rapide et interpr√©table',\n",
        "        'color': '#3498db'\n",
        "    },\n",
        "    \n",
        "    'Random_Forest': {\n",
        "        'model': RandomForestClassifier(\n",
        "            n_estimators=150,\n",
        "            max_depth=20,\n",
        "            min_samples_split=5,\n",
        "            min_samples_leaf=2,\n",
        "            max_features='sqrt',\n",
        "            random_state=42,\n",
        "            n_jobs=-1,\n",
        "            class_weight='balanced',\n",
        "            verbose=0\n",
        "        ),\n",
        "        'description': 'Ensemble d\\'arbres - Robuste et performant',\n",
        "        'color': '#2ecc71'\n",
        "    },\n",
        "    \n",
        "    'XGBoost': {\n",
        "        'model': XGBClassifier(\n",
        "            n_estimators=150,\n",
        "            max_depth=10,\n",
        "            learning_rate=0.1,\n",
        "            subsample=0.8,\n",
        "            colsample_bytree=0.8,\n",
        "            gamma=0.1,\n",
        "            scale_pos_weight=1,  # Ajuster selon le d√©s√©quilibre\n",
        "            random_state=42,\n",
        "            n_jobs=-1,\n",
        "            eval_metric='logloss',\n",
        "            verbosity=0\n",
        "        ),\n",
        "        'description': 'Gradient Boosting optimis√© - Tr√®s haute performance',\n",
        "        'color': '#e74c3c'\n",
        "    },\n",
        "    \n",
        "    'LightGBM': {\n",
        "        'model': LGBMClassifier(\n",
        "            n_estimators=150,\n",
        "            max_depth=10,\n",
        "            learning_rate=0.1,\n",
        "            subsample=0.8,\n",
        "            colsample_bytree=0.8,\n",
        "            num_leaves=31,\n",
        "            min_child_samples=20,\n",
        "            random_state=42,\n",
        "            n_jobs=-1,\n",
        "            verbose=-1,\n",
        "            is_unbalance=True  # Pour classes d√©s√©quilibr√©es\n",
        "        ),\n",
        "        'description': 'Gradient Boosting l√©ger - Rapide et efficace',\n",
        "        'color': '#f39c12'\n",
        "    }\n",
        "}\n",
        "\n",
        "print(f\"\\nüìã {len(models_config)} mod√®les configur√©s:\")\n",
        "for name, config in models_config.items():\n",
        "    print(f\"   ‚úì {name}: {config['description']}\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1767550176597
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"üöÄ ENTRA√éNEMENT DES MOD√àLES (VERSION CORRIG√âE)\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Dictionnaire pour stocker les r√©sultats\n",
        "all_results = {}\n",
        "\n",
        "# Boucle d'entra√Ænement\n",
        "for model_name, config in models_config.items():\n",
        "    \n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(f\"üîµ Entra√Ænement: {model_name}\")\n",
        "    print(f\"   {config['description']}\")\n",
        "    print(f\"{'='*80}\")\n",
        "    \n",
        "    # D√©marrer un run MLflow\n",
        "    with mlflow.start_run(run_name=model_name) as run:\n",
        "        \n",
        "        # Obtenir le mod√®le\n",
        "        model = config['model']\n",
        "        \n",
        "        # ‚è±Ô∏è Entra√Ænement\n",
        "        print(\"‚è≥ Entra√Ænement en cours...\")\n",
        "        start_time = datetime.now()\n",
        "        \n",
        "        model.fit(X_train_scaled, y_train)\n",
        "        \n",
        "        training_time = (datetime.now() - start_time).total_seconds()\n",
        "        print(f\"‚úÖ Entra√Ænement termin√© en {training_time:.2f} secondes\")\n",
        "        \n",
        "        # üìä √âvaluation\n",
        "        print(\"\\nüìä √âvaluation du mod√®le...\")\n",
        "        metrics, viz_figure = evaluate_model_comprehensive(\n",
        "            model, X_test_scaled, y_test, model_name\n",
        "        )\n",
        "        plt.show()\n",
        "        # Ajouter le temps d'entra√Ænement\n",
        "        metrics['training_time_seconds'] = training_time\n",
        "        \n",
        "        # üìù Logging vers MLflow\n",
        "        print(\"\\nüìù Logging vers MLflow...\")\n",
        "        \n",
        "        # 1. Param√®tres\n",
        "        model_params = {\n",
        "            'model_type': model_name,\n",
        "            'algorithm': model.__class__.__name__,\n",
        "            'train_samples': len(X_train_scaled),\n",
        "            'test_samples': len(X_test_scaled),\n",
        "            'n_features': X_train_scaled.shape[1],\n",
        "            'training_time': training_time\n",
        "        }\n",
        "        \n",
        "        # Ajouter les hyperparam√®tres (filtrer les non-JSON serializable)\n",
        "        if hasattr(model, 'get_params'):\n",
        "            hyperparams = model.get_params()\n",
        "            for key, value in hyperparams.items():\n",
        "                if isinstance(value, (int, float, str, bool, type(None))):\n",
        "                    model_params[f'hp_{key}'] = value\n",
        "        \n",
        "        mlflow.log_params(model_params)\n",
        "        \n",
        "        # 2. M√©triques (filtrer pour ne garder que les nombres)\n",
        "        numeric_metrics = {\n",
        "            k: float(v) for k, v in metrics.items() \n",
        "            if isinstance(v, (int, float, np.integer, np.floating))\n",
        "        }\n",
        "        mlflow.log_metrics(numeric_metrics)\n",
        "        \n",
        "        # 3. Visualisations\n",
        "        mlflow.log_figure(viz_figure, f\"{model_name}_evaluation.png\")\n",
        "        plt.close(viz_figure)\n",
        "        \n",
        "        # 4. Classification Report\n",
        "        report = classification_report(\n",
        "            y_test, model.predict(X_test_scaled),\n",
        "            target_names=['Non-Fraude', 'Fraude']\n",
        "        )\n",
        "        with open(f\"{model_name}_report.txt\", \"w\") as f:\n",
        "            f.write(report)\n",
        "        mlflow.log_artifact(f\"{model_name}_report.txt\")\n",
        "        os.remove(f\"{model_name}_report.txt\")\n",
        "        \n",
        "        # 5. Sauvegarder le mod√®le (SANS registered_model_name pour √©viter l'erreur)\n",
        "        try:\n",
        "            mlflow.sklearn.log_model(\n",
        "                model, \n",
        "                \"model\"\n",
        "                # PAS de registered_model_name ici\n",
        "            )\n",
        "            print(\"‚úÖ Mod√®le sauvegard√© dans MLflow\")\n",
        "        except Exception as e:\n",
        "            print(f\"‚ö†Ô∏è Avertissement MLflow: {str(e)}\")\n",
        "            print(\"   Le mod√®le sera enregistr√© manuellement\")\n",
        "        \n",
        "        # 6. Tags pour organisation\n",
        "        mlflow.set_tags({\n",
        "            'project': 'CDDA-Fraud-Detection',\n",
        "            'framework': 'scikit-learn',\n",
        "            'use_case': 'fraud_detection',\n",
        "            'data_balancing': 'SMOTE',\n",
        "            'deployment_ready': 'true'\n",
        "        })\n",
        "        \n",
        "        print(f\"‚úÖ Run ID: {run.info.run_id}\")\n",
        "        \n",
        "        # Stocker les r√©sultats\n",
        "        all_results[model_name] = {\n",
        "            'model': model,\n",
        "            'metrics': metrics,\n",
        "            'run_id': run.info.run_id,\n",
        "            'color': config['color']\n",
        "        }\n",
        "    \n",
        "    print(f\"\\n{'='*80}\")\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1767550176625
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# ENREGISTREMENT MANUEL DU MEILLEUR MOD√àLE (apr√®s comparaison)\n",
        "# =============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"üíæ ENREGISTREMENT MANUEL DES MOD√àLES DANS AZURE ML\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Fonction pour enregistrer un mod√®le manuellement\n",
        "def register_model_manually(model, model_name, metrics, run_id):\n",
        "    \"\"\"\n",
        "    Enregistre un mod√®le dans Azure ML Model Registry de mani√®re manuelle\n",
        "    \"\"\"\n",
        "    try:\n",
        "        from azureml.core import Model\n",
        "        \n",
        "        # Cr√©er un dossier temporaire\n",
        "        temp_dir = f'temp_model_{model_name}'\n",
        "        os.makedirs(temp_dir, exist_ok=True)\n",
        "        \n",
        "        # Sauvegarder le mod√®le localement\n",
        "        model_path = os.path.join(temp_dir, 'model.pkl')\n",
        "        joblib.dump(model, model_path)\n",
        "        \n",
        "        # Enregistrer dans Azure ML\n",
        "        registered_model = Model.register(\n",
        "            workspace=ws,\n",
        "            model_path=temp_dir,\n",
        "            model_name=f'fraud_detection_{model_name.lower()}',\n",
        "            description=f'{model_name} - F1:{metrics[\"f1_score\"]:.4f} ROC-AUC:{metrics[\"roc_auc\"]:.4f}',\n",
        "            tags={\n",
        "                'model_type': model_name,\n",
        "                'f1_score': f'{metrics[\"f1_score\"]:.4f}',\n",
        "                'roc_auc': f'{metrics[\"roc_auc\"]:.4f}',\n",
        "                'accuracy': f'{metrics[\"accuracy\"]:.4f}',\n",
        "                'mlflow_run_id': run_id,\n",
        "                'deployment_ready': 'true'\n",
        "            },\n",
        "            model_framework='ScikitLearn'\n",
        "        )\n",
        "        \n",
        "        # Nettoyer\n",
        "        import shutil\n",
        "        shutil.rmtree(temp_dir)\n",
        "        \n",
        "        print(f\"‚úÖ {model_name} enregistr√©:\")\n",
        "        print(f\"   Nom: {registered_model.name}\")\n",
        "        print(f\"   Version: {registered_model.version}\")\n",
        "        print(f\"   ID: {registered_model.id}\")\n",
        "        \n",
        "        return registered_model\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Erreur lors de l'enregistrement de {model_name}: {str(e)}\")\n",
        "        return None\n",
        "\n",
        "# Enregistrer tous les mod√®les\n",
        "if AZURE_ML_CONNECTED:\n",
        "    print(\"\\nüìù Enregistrement de tous les mod√®les dans Azure ML...\")\n",
        "    registered_models = {}\n",
        "    \n",
        "    for model_name, results in all_results.items():\n",
        "        registered_model = register_model_manually(\n",
        "            model=results['model'],\n",
        "            model_name=model_name,\n",
        "            metrics=results['metrics'],\n",
        "            run_id=results['run_id']\n",
        "        )\n",
        "        if registered_model:\n",
        "            registered_models[model_name] = registered_model\n",
        "    \n",
        "    print(f\"\\n‚úÖ {len(registered_models)} mod√®les enregistr√©s avec succ√®s!\")\n",
        "else:\n",
        "    print(\"\\n‚ö†Ô∏è Mode local - enregistrement Azure ML d√©sactiv√©\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1767550176669
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# SAUVEGARDER LE MEILLEUR MOD√àLE LOCALEMENT POUR L'ENREGISTREMENT\n",
        "# =============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"üèÜ IDENTIFICATION ET SAUVEGARDE DU MEILLEUR MOD√àLE\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# 1. Trouver le meilleur mod√®le bas√© sur F1-Score ou ROC-AUC\n",
        "best_model_name = None\n",
        "best_score = 0\n",
        "best_metric = 'f1_score'  # ou 'roc_auc' selon votre pr√©f√©rence\n",
        "\n",
        "print(f\"\\nüìä Comparaison des mod√®les (crit√®re: {best_metric}):\")\n",
        "for model_name, results in all_results.items():\n",
        "    score = results['metrics'][best_metric]\n",
        "    print(f\"   {model_name}: {score:.4f}\")\n",
        "    \n",
        "    if score > best_score:\n",
        "        best_score = score\n",
        "        best_model_name = model_name\n",
        "\n",
        "print(f\"\\nü•á Meilleur mod√®le: {best_model_name} ({best_metric}={best_score:.4f})\")\n",
        "\n",
        "# 2. R√©cup√©rer le meilleur mod√®le\n",
        "best_model = all_results[best_model_name]['model']\n",
        "best_metrics = all_results[best_model_name]['metrics']\n",
        "\n",
        "# 3. Cr√©er le dossier outputs\n",
        "os.makedirs('outputs', exist_ok=True)\n",
        "\n",
        "# 4. Sauvegarder le meilleur mod√®le\n",
        "print(\"\\nüíæ Sauvegarde des artifacts dans /outputs...\")\n",
        "\n",
        "# Sauvegarder le mod√®le\n",
        "with open('outputs/best_model.pkl', 'wb') as f:\n",
        "    joblib.dump(best_model, f)\n",
        "print(\"   ‚úÖ best_model.pkl\")\n",
        "\n",
        "# Sauvegarder le scaler (si vous l'avez utilis√©)\n",
        "if 'scaler' in globals():\n",
        "    with open('outputs/scaler.pkl', 'wb') as f:\n",
        "        joblib.dump(scaler, f)\n",
        "    print(\"   ‚úÖ scaler.pkl\")\n",
        "else:\n",
        "    # Cr√©er un scaler dummy si n√©cessaire\n",
        "    from sklearn.preprocessing import StandardScaler\n",
        "    dummy_scaler = StandardScaler()\n",
        "    with open('outputs/scaler.pkl', 'wb') as f:\n",
        "        joblib.dump(dummy_scaler, f)\n",
        "    print(\"   ‚ö†Ô∏è  scaler.pkl (dummy - remplacez par le vrai scaler)\")\n",
        "\n",
        "# Sauvegarder les m√©tadonn√©es\n",
        "metadata = {\n",
        "    'best_model_name': best_model_name,\n",
        "    'best_model_type': type(best_model).__name__,\n",
        "    'metrics': {k: float(v) for k, v in best_metrics.items()},\n",
        "    'training_date': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
        "    'all_models_comparison': {\n",
        "        name: {\n",
        "            'f1_score': float(res['metrics']['f1_score']),\n",
        "            'roc_auc': float(res['metrics']['roc_auc']),\n",
        "            'accuracy': float(res['metrics']['accuracy'])\n",
        "        }\n",
        "        for name, res in all_results.items()\n",
        "    },\n",
        "    'feature_names': X_test.columns.tolist() if hasattr(X_test, 'columns') else [],\n",
        "    'n_features': X_test.shape[1] if hasattr(X_test, 'shape') else 0\n",
        "}\n",
        "\n",
        "with open('outputs/metadata.json', 'w') as f:\n",
        "    json.dump(metadata, f, indent=2)\n",
        "print(\"   ‚úÖ metadata.json\")\n",
        "\n",
        "# 5. V√©rification\n",
        "print(\"\\nüîç V√©rification des fichiers:\")\n",
        "for filename in ['best_model.pkl', 'scaler.pkl', 'metadata.json']:\n",
        "    filepath = f'outputs/{filename}'\n",
        "    if os.path.exists(filepath):\n",
        "        size = os.path.getsize(filepath) / 1024  # En KB\n",
        "        print(f\"   ‚úÖ {filename} ({size:.2f} KB)\")\n",
        "    else:\n",
        "        print(f\"   ‚ùå {filename} MANQUANT!\")\n",
        "\n",
        "print(\"\\n‚úÖ Artifacts pr√™ts pour l'enregistrement Azure ML!\")\n",
        "print(\"   Vous pouvez maintenant ex√©cuter le code d'enregistrement.\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1767550176698
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# SAUVEGARDE LOCALE (Toujours fonctionnel)\n",
        "# =============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"üíæ SAUVEGARDE LOCALE DES MOD√àLES (Backup)\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Cr√©er le dossier outputs\n",
        "os.makedirs('outputs', exist_ok=True)\n",
        "\n",
        "# Sauvegarder tous les mod√®les localement\n",
        "for model_name, results in all_results.items():\n",
        "    model_path = f'outputs/{model_name.lower().replace(\" \", \"_\")}.pkl'\n",
        "    joblib.dump(results['model'], model_path)\n",
        "    print(f\"‚úÖ {model_name} ‚Üí {model_path}\")\n",
        "\n",
        "# Sauvegarder le scaler\n",
        "scaler_path = 'outputs/scaler.pkl'\n",
        "joblib.dump(scaler, scaler_path)\n",
        "print(f\"‚úÖ Scaler ‚Üí {scaler_path}\")\n",
        "\n",
        "# Sauvegarder les m√©tadonn√©es de tous les mod√®les\n",
        "all_metadata = {}\n",
        "for model_name, results in all_results.items():\n",
        "    all_metadata[model_name] = {\n",
        "        'metrics': {\n",
        "            k: float(v) if isinstance(v, (int, float, np.integer, np.floating)) else str(v)\n",
        "            for k, v in results['metrics'].items()\n",
        "        },\n",
        "        'run_id': results['run_id']\n",
        "    }\n",
        "\n",
        "with open('outputs/all_models_metadata.json', 'w') as f:\n",
        "    json.dump(all_metadata, f, indent=4)\n",
        "print(f\"‚úÖ M√©tadonn√©es ‚Üí outputs/all_models_metadata.json\")\n",
        "\n",
        "print(\"\\n‚úÖ Tous les mod√®les sauvegard√©s localement!\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1767550176739
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üìä 9. Deploiement"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# √âTAPE 1: PR√âPARER LES FICHIERS DE D√âPLOIEMENT\n",
        "# =============================================================================\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\" \"*20 + \"üöÄ D√âPLOIEMENT AZURE ML ENDPOINT\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "import os\n",
        "from azureml.core import Workspace, Model, Environment\n",
        "from azureml.core.model import InferenceConfig\n",
        "from azureml.core.webservice import AciWebservice, Webservice\n",
        "from azureml.exceptions import WebserviceException\n",
        "import json\n",
        "\n",
        "# Connexion au workspace\n",
        "try:\n",
        "    ws = Workspace.from_config()\n",
        "    print(f\"‚úÖ Connect√© au workspace: {ws.name}\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Erreur de connexion: {e}\")\n",
        "    print(\"üí° Assurez-vous que config.json existe\")\n",
        "    raise"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "trusted": true,
        "gather": {
          "logged": 1767550176764
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# √âTAPE 2: CR√âER LE SCRIPT DE SCORING\n",
        "# =============================================================================\n",
        "\n",
        "print(\"\\nüìù Cr√©ation du script de scoring (score.py)...\")\n",
        "\n",
        "score_script = \"\"\"\n",
        "import json\n",
        "import joblib\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from azureml.core.model import Model\n",
        "\n",
        "def init():\n",
        "    '''\n",
        "    Cette fonction est appel√©e une seule fois au d√©marrage du service.\n",
        "    Elle charge le mod√®le et le scaler en m√©moire.\n",
        "    '''\n",
        "    global model, scaler, feature_names\n",
        "    \n",
        "    try:\n",
        "        # Charger le mod√®le\n",
        "        model_path = Model.get_model_path('fraud-detection-model')\n",
        "        model = joblib.load(model_path)\n",
        "        print(f\"‚úÖ Mod√®le charg√©: {type(model).__name__}\")\n",
        "        \n",
        "        # Charger le scaler\n",
        "        scaler_path = Model.get_model_path('fraud-detection-scaler')\n",
        "        scaler = joblib.load(scaler_path)\n",
        "        print(f\"‚úÖ Scaler charg√©: {type(scaler).__name__}\")\n",
        "        \n",
        "        # Charger les noms de features (optionnel)\n",
        "        try:\n",
        "            metadata_path = Model.get_model_path('fraud-detection-metadata')\n",
        "            with open(metadata_path, 'r') as f:\n",
        "                metadata = json.load(f)\n",
        "                feature_names = metadata.get('feature_names', [])\n",
        "                print(f\"‚úÖ M√©tadonn√©es charg√©es ({len(feature_names)} features)\")\n",
        "        except:\n",
        "            feature_names = []\n",
        "            print(\"‚ö†Ô∏è M√©tadonn√©es non disponibles\")\n",
        "        \n",
        "        print(\"‚úÖ Initialisation r√©ussie!\")\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Erreur lors de l'initialisation: {str(e)}\")\n",
        "        raise\n",
        "\n",
        "def run(raw_data):\n",
        "    '''\n",
        "    Cette fonction est appel√©e pour chaque requ√™te HTTP.\n",
        "    Elle re√ßoit les donn√©es, fait la pr√©diction et retourne le r√©sultat.\n",
        "    \n",
        "    Format d'entr√©e attendu:\n",
        "    {\n",
        "        \"data\": [[feature1, feature2, ..., featureN]],\n",
        "        \"transaction_ids\": [\"TXN_001\", \"TXN_002\"]  # optionnel\n",
        "    }\n",
        "    \n",
        "    Format de sortie:\n",
        "    {\n",
        "        \"predictions\": [\n",
        "            {\n",
        "                \"transaction_id\": \"TXN_001\",\n",
        "                \"is_fraud\": true/false,\n",
        "                \"fraud_probability\": 0.85,\n",
        "                \"confidence\": 0.85,\n",
        "                \"risk_level\": \"HIGH/MEDIUM/LOW\"\n",
        "            }\n",
        "        ],\n",
        "        \"model_info\": {\n",
        "            \"model_name\": \"XGBoost\",\n",
        "            \"version\": \"1.0\"\n",
        "        },\n",
        "        \"status\": \"success\"\n",
        "    }\n",
        "    '''\n",
        "    try:\n",
        "        # Parser les donn√©es JSON\n",
        "        data = json.loads(raw_data)\n",
        "        \n",
        "        # Extraire les features\n",
        "        if 'data' not in data:\n",
        "            return json.dumps({\n",
        "                'error': 'Missing \"data\" field in request',\n",
        "                'status': 'error'\n",
        "            })\n",
        "        \n",
        "        input_data = np.array(data['data'])\n",
        "        transaction_ids = data.get('transaction_ids', \n",
        "                                   [f'TXN_{i:04d}' for i in range(len(input_data))])\n",
        "        \n",
        "        # Validation de la forme des donn√©es\n",
        "        if len(input_data.shape) == 1:\n",
        "            input_data = input_data.reshape(1, -1)\n",
        "        \n",
        "        # Preprocessing: Scaling\n",
        "        scaled_data = scaler.transform(input_data)\n",
        "        \n",
        "        # Pr√©diction\n",
        "        predictions = model.predict(scaled_data)\n",
        "        probabilities = model.predict_proba(scaled_data)\n",
        "        \n",
        "        # Formater les r√©sultats\n",
        "        results = []\n",
        "        for i, (pred, proba) in enumerate(zip(predictions, probabilities)):\n",
        "            fraud_prob = float(proba[1])\n",
        "            \n",
        "            # D√©terminer le niveau de risque\n",
        "            if fraud_prob >= 0.7:\n",
        "                risk_level = \"HIGH\"\n",
        "            elif fraud_prob >= 0.4:\n",
        "                risk_level = \"MEDIUM\"\n",
        "            else:\n",
        "                risk_level = \"LOW\"\n",
        "            \n",
        "            results.append({\n",
        "                'transaction_id': transaction_ids[i],\n",
        "                'is_fraud': bool(pred == 1),\n",
        "                'fraud_probability': round(fraud_prob, 4),\n",
        "                'legitimate_probability': round(float(proba[0]), 4),\n",
        "                'confidence': round(float(max(proba)), 4),\n",
        "                'risk_level': risk_level,\n",
        "                'recommendation': (\n",
        "                    'BLOCK - Fraude probable' if fraud_prob >= 0.7 else\n",
        "                    'REVIEW - Investigation recommand√©e' if fraud_prob >= 0.4 else\n",
        "                    'APPROVE - Transaction s√ªre'\n",
        "                )\n",
        "            })\n",
        "        \n",
        "        # R√©ponse compl√®te\n",
        "        response = {\n",
        "            'predictions': results,\n",
        "            'model_info': {\n",
        "                'model_name': type(model).__name__,\n",
        "                'version': '1.0',\n",
        "                'features_count': input_data.shape[1]\n",
        "            },\n",
        "            'metadata': {\n",
        "                'total_transactions': len(results),\n",
        "                'fraud_detected': sum(1 for r in results if r['is_fraud']),\n",
        "                'avg_fraud_probability': round(\n",
        "                    sum(r['fraud_probability'] for r in results) / len(results), 4\n",
        "                )\n",
        "            },\n",
        "            'status': 'success'\n",
        "        }\n",
        "        \n",
        "        return json.dumps(response)\n",
        "        \n",
        "    except Exception as e:\n",
        "        error_response = {\n",
        "            'error': str(e),\n",
        "            'error_type': type(e).__name__,\n",
        "            'status': 'error',\n",
        "            'message': 'Une erreur est survenue lors de la pr√©diction'\n",
        "        }\n",
        "        return json.dumps(error_response)\n",
        "\"\"\"\n",
        "\n",
        "# Sauvegarder le script\n",
        "with open('score.py', 'w', encoding='utf-8') as f:\n",
        "    f.write(score_script)\n",
        "\n",
        "print(\"‚úÖ score.py cr√©√© avec succ√®s\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1767550176787
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# √âTAPE 3: CR√âER LE FICHIER D'ENVIRONNEMENT\n",
        "# =============================================================================\n",
        "\n",
        "print(\"\\nüì¶ Cr√©ation de l'environnement (fraud_env.yml)...\")\n",
        "\n",
        "env_yml = \"\"\"name: fraud-detection-env\n",
        "channels:\n",
        "  - conda-forge\n",
        "  - defaults\n",
        "dependencies:\n",
        "  - python=3.8\n",
        "  - pip\n",
        "  - pip:\n",
        "    - azureml-defaults>=1.38.0\n",
        "    - inference-schema\n",
        "    - scikit-learn==1.0.2\n",
        "    - pandas==1.4.4\n",
        "    - numpy==1.22.4\n",
        "    - joblib==1.1.0\n",
        "    - xgboost==1.6.2\n",
        "    - lightgbm==3.3.5\n",
        "\"\"\"\n",
        "\n",
        "with open('fraud_env.yml', 'w') as f:\n",
        "    f.write(env_yml)\n",
        "\n",
        "print(\"‚úÖ fraud_env.yml cr√©√©\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1767550176833
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# √âTAPE 4: ENREGISTRER LES MOD√àLES DANS AZURE ML\n",
        "# =============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"üìù ENREGISTREMENT DES MOD√àLES DANS AZURE ML MODEL REGISTRY\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "def register_model_file(workspace, model_path, model_name, description, tags=None):\n",
        "    \"\"\"Enregistre un mod√®le dans Azure ML\"\"\"\n",
        "    try:\n",
        "        # V√©rifier si le fichier existe\n",
        "        if not os.path.exists(model_path):\n",
        "            print(f\"‚ùå Fichier introuvable: {model_path}\")\n",
        "            return None\n",
        "        \n",
        "        # Enregistrer\n",
        "        model = Model.register(\n",
        "            workspace=workspace,\n",
        "            model_path=model_path,\n",
        "            model_name=model_name,\n",
        "            description=description,\n",
        "            tags=tags or {},\n",
        "            model_framework='ScikitLearn'\n",
        "        )\n",
        "        \n",
        "        print(f\"‚úÖ {model_name}\")\n",
        "        print(f\"   Version: {model.version}\")\n",
        "        print(f\"   ID: {model.id}\")\n",
        "        return model\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Erreur lors de l'enregistrement de {model_name}: {str(e)}\")\n",
        "        return None\n",
        "\n",
        "# Enregistrer le meilleur mod√®le\n",
        "print(\"\\n1Ô∏è‚É£ Enregistrement du mod√®le principal...\")\n",
        "model_registered = register_model_file(\n",
        "    workspace=ws,\n",
        "    model_path='outputs/best_model.pkl',\n",
        "    model_name='fraud-detection-model',\n",
        "    description='Meilleur mod√®le de d√©tection de fraude (XGBoost/LightGBM)',\n",
        "    tags={\n",
        "        'type': 'fraud_detection',\n",
        "        'framework': 'scikit-learn',\n",
        "        'deployment': 'production'\n",
        "    }\n",
        ")\n",
        "\n",
        "# Enregistrer le scaler\n",
        "print(\"\\n2Ô∏è‚É£ Enregistrement du scaler...\")\n",
        "scaler_registered = register_model_file(\n",
        "    workspace=ws,\n",
        "    model_path='outputs/scaler.pkl',\n",
        "    model_name='fraud-detection-scaler',\n",
        "    description='RobustScaler pour preprocessing des features',\n",
        "    tags={'type': 'preprocessing'}\n",
        ")\n",
        "\n",
        "# Enregistrer les m√©tadonn√©es (optionnel)\n",
        "print(\"\\n3Ô∏è‚É£ Enregistrement des m√©tadonn√©es...\")\n",
        "if os.path.exists('outputs/metadata.json'):\n",
        "    metadata_registered = register_model_file(\n",
        "        workspace=ws,\n",
        "        model_path='outputs/metadata.json',\n",
        "        model_name='fraud-detection-metadata',\n",
        "        description='M√©tadonn√©es du mod√®le (features, m√©triques)',\n",
        "        tags={'type': 'metadata'}\n",
        "    )\n",
        "\n",
        "if not model_registered or not scaler_registered:\n",
        "    print(\"\\n‚ùå Enregistrement des mod√®les √©chou√©. V√©rifiez les fichiers dans /outputs\")\n",
        "    raise Exception(\"Mod√®les non enregistr√©s\")\n",
        "\n",
        "print(\"\\n‚úÖ Tous les artifacts enregistr√©s avec succ√®s!\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1767550176858
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# √âTAPE 5: CR√âER L'ENVIRONNEMENT AZURE ML\n",
        "# =============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"üåç CR√âATION DE L'ENVIRONNEMENT AZURE ML\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Cr√©er l'environnement depuis le fichier YAML\n",
        "env = Environment.from_conda_specification(\n",
        "    name='fraud-detection-env',\n",
        "    file_path='fraud_env.yml'\n",
        ")\n",
        "\n",
        "# Enregistrer l'environnement\n",
        "env.register(workspace=ws)\n",
        "print(f\"‚úÖ Environnement '{env.name}' cr√©√© et enregistr√©\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1767550176885
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# √âTAPE 6: CONFIGURATION DU D√âPLOIEMENT\n",
        "# =============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"‚öôÔ∏è CONFIGURATION DU D√âPLOIEMENT\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Configuration de l'inf√©rence\n",
        "inference_config = InferenceConfig(\n",
        "    entry_script='score.py',\n",
        "    environment=env\n",
        ")\n",
        "print(\"‚úÖ Configuration d'inf√©rence cr√©√©e\")\n",
        "\n",
        "# Configuration du service (Azure Container Instance)\n",
        "deployment_config = AciWebservice.deploy_configuration(\n",
        "    cpu_cores=2,\n",
        "    memory_gb=4,\n",
        "    auth_enabled=True,  # Activer l'authentification\n",
        "    enable_app_insights=True,  # Activer le monitoring\n",
        "    collect_model_data=True,  # Collecter les donn√©es pour monitoring\n",
        "    description='API REST de d√©tection de fraude - Projet CDDA',\n",
        "    tags={\n",
        "        'project': 'CDDA',\n",
        "        'use_case': 'fraud_detection',\n",
        "        'department': 'data_science'\n",
        "    }\n",
        ")\n",
        "print(\"‚úÖ Configuration de d√©ploiement cr√©√©e (ACI)\")\n",
        "print(\"   ‚Ä¢ CPU: 2 cores\")\n",
        "print(\"   ‚Ä¢ RAM: 4 GB\")\n",
        "print(\"   ‚Ä¢ Auth: Activ√©e\")\n",
        "print(\"   ‚Ä¢ Monitoring: Activ√©\")\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1767550176904
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# √âTAPE 7: D√âPLOYER LE SERVICE\n",
        "# =============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"üöÄ D√âPLOIEMENT DU SERVICE\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "service_name = 'fraud-detection-api'\n",
        "\n",
        "# V√©rifier si le service existe d√©j√†\n",
        "try:\n",
        "    existing_service = Webservice(workspace=ws, name=service_name)\n",
        "    print(f\"‚ö†Ô∏è Service '{service_name}' existe d√©j√†\")\n",
        "    \n",
        "    # Demander confirmation pour mise √† jour\n",
        "    update_service = True  # Mettre √† False si vous voulez garder l'ancien\n",
        "    \n",
        "    if update_service:\n",
        "        print(\"üîÑ Mise √† jour du service existant...\")\n",
        "        existing_service.delete()\n",
        "        print(\"‚úÖ Ancien service supprim√©\")\n",
        "    else:\n",
        "        print(\"‚ÑπÔ∏è Utilisation du service existant\")\n",
        "        service = existing_service\n",
        "        \n",
        "except WebserviceException:\n",
        "    print(f\"‚ÑπÔ∏è Aucun service existant nomm√© '{service_name}'\")\n",
        "\n",
        "# D√©ployer le nouveau service\n",
        "if 'service' not in locals():\n",
        "    print(f\"\\nüöÄ D√©ploiement en cours de '{service_name}'...\")\n",
        "    print(\"‚è≥ Cela peut prendre 5-10 minutes...\")\n",
        "    \n",
        "    service = Model.deploy(\n",
        "        workspace=ws,\n",
        "        name=service_name,\n",
        "        models=[model_registered, scaler_registered],\n",
        "        inference_config=inference_config,\n",
        "        deployment_config=deployment_config,\n",
        "        overwrite=True\n",
        "    )\n",
        "    \n",
        "    # Attendre la fin du d√©ploiement\n",
        "    service.wait_for_deployment(show_output=True)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1767550176924
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# √âTAPE 8: V√âRIFIER LE D√âPLOIEMENT\n",
        "# =============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"‚úÖ D√âPLOIEMENT R√âUSSI!\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "print(f\"\\nüìä Informations du service:\")\n",
        "print(f\"   Nom: {service.name}\")\n",
        "print(f\"   √âtat: {service.state}\")\n",
        "print(f\"   Scoring URI: {service.scoring_uri}\")\n",
        "\n",
        "# R√©cup√©rer les cl√©s d'authentification\n",
        "try:\n",
        "    primary_key, secondary_key = service.get_keys()\n",
        "    print(f\"\\nüîë Cl√©s d'authentification:\")\n",
        "    print(f\"   Primary Key: {primary_key[:20]}...\")\n",
        "    print(f\"   Secondary Key: {secondary_key[:20]}...\")\n",
        "except:\n",
        "    print(\"\\n‚ö†Ô∏è Authentification par cl√© non disponible\")\n",
        "\n",
        "# Swagger URI (documentation auto-g√©n√©r√©e)\n",
        "if hasattr(service, 'swagger_uri') and service.swagger_uri:\n",
        "    print(f\"\\nüìñ Documentation Swagger: {service.swagger_uri}\")\n",
        "\n",
        "# Sauvegarder les informations de d√©ploiement\n",
        "deployment_info = {\n",
        "    'service_name': service.name,\n",
        "    'scoring_uri': service.scoring_uri,\n",
        "    'state': service.state,\n",
        "    'primary_key': primary_key if 'primary_key' in locals() else None,\n",
        "    'deployment_date': str(pd.Timestamp.now())\n",
        "}\n",
        "\n",
        "with open('outputs/deployment_info.json', 'w') as f:\n",
        "    json.dump(deployment_info, f, indent=4)\n",
        "\n",
        "print(\"\\nüíæ Informations sauvegard√©es dans: outputs/deployment_info.json\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1767550176947
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# √âTAPE 9: TESTER L'ENDPOINT\n",
        "# =============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"üß™ TEST DE L'ENDPOINT\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "import requests\n",
        "import pandas as pd\n",
        "\n",
        "# Pr√©parer des donn√©es de test\n",
        "print(\"\\nüìä Pr√©paration des donn√©es de test...\")\n",
        "\n",
        "# Charger quelques transactions de test\n",
        "# (Remplacez par vos vraies donn√©es)\n",
        "test_data = {\n",
        "    'data': [\n",
        "        # Transaction 1: Caract√©ristiques normales\n",
        "        [100.0, 5000.0, 3000.0, 0.5, 0.2],\n",
        "        # Transaction 2: Caract√©ristiques suspectes\n",
        "        [50000.0, 100.0, 200000.0, 0.9, 0.8],\n",
        "        # Transaction 3: Borderline\n",
        "        [1000.0, 2000.0, 2500.0, 0.45, 0.35]\n",
        "    ],\n",
        "    'transaction_ids': ['TXN_001', 'TXN_002', 'TXN_003']\n",
        "}\n",
        "\n",
        "print(f\"‚úÖ {len(test_data['data'])} transactions de test pr√©par√©es\")\n",
        "\n",
        "# Faire la requ√™te\n",
        "print(\"\\nüåê Envoi de la requ√™te √† l'endpoint...\")\n",
        "\n",
        "headers = {\n",
        "    'Content-Type': 'application/json',\n",
        "    'Authorization': f'Bearer {primary_key}' if 'primary_key' in locals() else ''\n",
        "}\n",
        "\n",
        "try:\n",
        "    response = requests.post(\n",
        "        service.scoring_uri,\n",
        "        data=json.dumps(test_data),\n",
        "        headers=headers,\n",
        "        timeout=30\n",
        "    )\n",
        "    \n",
        "    if response.status_code == 200:\n",
        "        results = response.json()\n",
        "        \n",
        "        print(\"\\n‚úÖ PR√âDICTIONS RE√áUES:\")\n",
        "        print(\"=\"*70)\n",
        "        \n",
        "        for pred in results['predictions']:\n",
        "            status = \"üö® FRAUDE\" if pred['is_fraud'] else \"‚úÖ L√âGITIME\"\n",
        "            risk = pred['risk_level']\n",
        "            prob = pred['fraud_probability'] * 100\n",
        "            \n",
        "            print(f\"\\n{pred['transaction_id']}: {status}\")\n",
        "            print(f\"   Probabilit√© fraude: {prob:.1f}%\")\n",
        "            print(f\"   Niveau de risque: {risk}\")\n",
        "            print(f\"   Recommandation: {pred['recommendation']}\")\n",
        "        \n",
        "        print(\"\\n\" + \"=\"*70)\n",
        "        print(f\"üìä Statistiques globales:\")\n",
        "        print(f\"   Total: {results['metadata']['total_transactions']}\")\n",
        "        print(f\"   Fraudes d√©tect√©es: {results['metadata']['fraud_detected']}\")\n",
        "        print(f\"   Prob. moyenne: {results['metadata']['avg_fraud_probability']*100:.1f}%\")\n",
        "        \n",
        "    else:\n",
        "        print(f\"‚ùå Erreur HTTP {response.status_code}\")\n",
        "        print(f\"R√©ponse: {response.text}\")\n",
        "        \n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Erreur lors du test: {str(e)}\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1767550176970
        }
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1765924758360
        }
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.18",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [],
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": false
    },
    "microsoft": {
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      },
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      }
    },
    "kernel_info": {
      "name": "python3"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}