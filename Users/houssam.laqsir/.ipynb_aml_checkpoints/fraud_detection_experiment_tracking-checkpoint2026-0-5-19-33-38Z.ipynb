{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# D√©tection de Fraude - Experiment Tracking avec Plusieurs Mod√®les\n",
    "## Azure ML + MLflow - Comparaison de Versions\n",
    "\n",
    "**Objectif**: Entra√Æner et comparer plusieurs versions de mod√®les de d√©tection de fraude\n",
    "\n",
    "**Mod√®les test√©s**:\n",
    "1. Random Forest (baseline)\n",
    "2. Random Forest (optimis√©)\n",
    "3. Gradient Boosting\n",
    "4. XGBoost\n",
    "5. LightGBM\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Configuration et Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installation des packages\n",
    "!pip install azureml-sdk pandas numpy scikit-learn imbalanced-learn matplotlib seaborn mlflow joblib xgboost lightgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Azure ML\n",
    "from azureml.core import Workspace, Experiment, Run\n",
    "from azureml.core.model import Model\n",
    "\n",
    "# Machine Learning\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.metrics import (\n",
    "    classification_report, confusion_matrix, \n",
    "    roc_auc_score, roc_curve, precision_recall_curve,\n",
    "    f1_score, precision_score, recall_score, accuracy_score\n",
    ")\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "# Algorithmes avanc√©s\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "\n",
    "# MLflow\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "import mlflow.xgboost\n",
    "import mlflow.lightgbm\n",
    "\n",
    "import joblib\n",
    "\n",
    "print(\"‚úÖ Biblioth√®ques import√©es avec succ√®s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Connexion Azure ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connexion au workspace\n",
    "try:\n",
    "    ws = Workspace.from_config()\n",
    "    print(f\"‚úÖ Connect√© au workspace: {ws.name}\")\n",
    "except:\n",
    "    ws = Workspace(\n",
    "        subscription_id='<VOTRE_SUBSCRIPTION_ID>',\n",
    "        resource_group='<VOTRE_RESOURCE_GROUP>',\n",
    "        workspace_name='<VOTRE_WORKSPACE_NAME>'\n",
    "    )\n",
    "    print(f\"‚úÖ Connect√© au workspace: {ws.name}\")\n",
    "\n",
    "# Cr√©er l'exp√©rience principale\n",
    "experiment_name = 'fraud-detection-model-comparison'\n",
    "experiment = Experiment(workspace=ws, name=experiment_name)\n",
    "print(f\"‚úÖ Exp√©rience cr√©√©e: {experiment_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Chargement et Pr√©paration des Donn√©es"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Charger les donn√©es\n",
    "df = pd.read_csv('fraud_transactions.csv')\n",
    "\n",
    "print(f\"‚úÖ Donn√©es charg√©es: {df.shape[0]:,} lignes, {df.shape[1]} colonnes\")\n",
    "print(f\"\\nüìä Distribution des classes:\")\n",
    "print(df['isFraud'].value_counts())\n",
    "print(f\"\\nTaux de fraude: {df['isFraud'].mean()*100:.4f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Engineering\n",
    "print(\"üîß Feature Engineering en cours...\")\n",
    "\n",
    "# Encoder le type de transaction\n",
    "df = pd.get_dummies(df, columns=['type'], prefix='type', drop_first=False)\n",
    "\n",
    "# Features d√©riv√©es\n",
    "df['balanceChange_orig'] = df['oldbalanceOrg'] - df['newbalanceOrig']\n",
    "df['balanceChange_dest'] = df['newbalanceDest'] - df['oldbalanceDest']\n",
    "df['amountToBalanceRatio_orig'] = df['amount'] / (df['oldbalanceOrg'] + 1)\n",
    "df['isOriginEmpty'] = (df['newbalanceOrig'] == 0).astype(int)\n",
    "df['isDestEmpty'] = (df['oldbalanceDest'] == 0).astype(int)\n",
    "df['errorBalanceOrig'] = df['balanceChange_orig'] - df['amount']\n",
    "df['errorBalanceDest'] = df['balanceChange_dest'] - df['amount']\n",
    "\n",
    "# Supprimer colonnes non n√©cessaires\n",
    "df = df.drop(columns=['nameOrig', 'nameDest', 'isFlaggedFraud'], errors='ignore')\n",
    "\n",
    "print(f\"‚úÖ Feature Engineering compl√©t√© - {df.shape[1]} features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# S√©paration X/y\n",
    "X = df.drop('isFraud', axis=1)\n",
    "y = df['isFraud']\n",
    "feature_names = X.columns.tolist()\n",
    "\n",
    "print(f\"Features (X): {X.shape}\")\n",
    "print(f\"Target (y): {y.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split Train/Test\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"Train set: {X_train.shape[0]:,} transactions\")\n",
    "print(f\"Test set: {X_test.shape[0]:,} transactions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalisation\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(\"‚úÖ Donn√©es normalis√©es\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Balancing\n",
    "print(\"‚öñÔ∏è R√©√©quilibrage des donn√©es...\")\n",
    "\n",
    "over = SMOTE(sampling_strategy=0.5, random_state=42)\n",
    "under = RandomUnderSampler(sampling_strategy=0.8, random_state=42)\n",
    "\n",
    "X_train_balanced, y_train_balanced = over.fit_resample(X_train_scaled, y_train)\n",
    "X_train_balanced, y_train_balanced = under.fit_resample(X_train_balanced, y_train_balanced)\n",
    "\n",
    "print(f\"‚úÖ Donn√©es r√©√©quilibr√©es: {len(y_train_balanced):,} transactions\")\n",
    "print(f\"Distribution: {pd.Series(y_train_balanced).value_counts().to_dict()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Fonction Utilitaire pour l'Experiment Tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate_model(model, model_name, X_train, y_train, X_test, y_test, run):\n",
    "    \"\"\"\n",
    "    Entra√Æne un mod√®le et log toutes les m√©triques dans Azure ML\n",
    "    \n",
    "    Args:\n",
    "        model: Le mod√®le √† entra√Æner\n",
    "        model_name: Nom du mod√®le pour le tracking\n",
    "        X_train, y_train: Donn√©es d'entra√Ænement\n",
    "        X_test, y_test: Donn√©es de test\n",
    "        run: Azure ML Run object\n",
    "    \n",
    "    Returns:\n",
    "        dict: M√©triques de performance\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"üöÄ Entra√Ænement: {model_name}\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    # Entra√Ænement\n",
    "    start_time = datetime.now()\n",
    "    model.fit(X_train, y_train)\n",
    "    training_time = (datetime.now() - start_time).total_seconds()\n",
    "    \n",
    "    # Pr√©dictions\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    # Calcul des m√©triques\n",
    "    metrics = {\n",
    "        'model_name': model_name,\n",
    "        'accuracy': accuracy_score(y_test, y_pred),\n",
    "        'precision': precision_score(y_test, y_pred),\n",
    "        'recall': recall_score(y_test, y_pred),\n",
    "        'f1_score': f1_score(y_test, y_pred),\n",
    "        'roc_auc': roc_auc_score(y_test, y_pred_proba),\n",
    "        'training_time_seconds': training_time\n",
    "    }\n",
    "    \n",
    "    # Log des m√©triques dans Azure ML\n",
    "    run.log('model_name', model_name)\n",
    "    run.log('accuracy', metrics['accuracy'])\n",
    "    run.log('precision', metrics['precision'])\n",
    "    run.log('recall', metrics['recall'])\n",
    "    run.log('f1_score', metrics['f1_score'])\n",
    "    run.log('roc_auc', metrics['roc_auc'])\n",
    "    run.log('training_time_seconds', training_time)\n",
    "    \n",
    "    # Affichage des r√©sultats\n",
    "    print(f\"\\nüìä R√©sultats:\")\n",
    "    print(f\"  Accuracy:  {metrics['accuracy']:.4f}\")\n",
    "    print(f\"  Precision: {metrics['precision']:.4f}\")\n",
    "    print(f\"  Recall:    {metrics['recall']:.4f}\")\n",
    "    print(f\"  F1-Score:  {metrics['f1_score']:.4f}\")\n",
    "    print(f\"  ROC-AUC:   {metrics['roc_auc']:.4f}\")\n",
    "    print(f\"  Temps:     {training_time:.2f}s\")\n",
    "    \n",
    "    # Matrice de confusion\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                xticklabels=['L√©gal', 'Fraude'],\n",
    "                yticklabels=['L√©gal', 'Fraude'])\n",
    "    plt.title(f'Matrice de Confusion - {model_name}', fontsize=14, fontweight='bold')\n",
    "    plt.ylabel('Vraie Classe')\n",
    "    plt.xlabel('Classe Pr√©dite')\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Sauvegarder et logger l'image\n",
    "    img_path = f'confusion_matrix_{model_name.replace(\" \", \"_\")}.png'\n",
    "    plt.savefig(img_path, dpi=300, bbox_inches='tight')\n",
    "    run.log_image(f'confusion_matrix_{model_name}', plot=plt)\n",
    "    plt.close()\n",
    "    \n",
    "    return metrics, model, y_pred_proba\n",
    "\n",
    "print(\"‚úÖ Fonction d'√©valuation cr√©√©e\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Experiment Tracking - Entra√Ænement de Plusieurs Mod√®les\n",
    "\n",
    "Nous allons entra√Æner et comparer 5 versions diff√©rentes de mod√®les"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# D√©marrer le run parent\n",
    "parent_run = experiment.start_logging()\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üß™ D√âBUT DE L'EXPERIMENT TRACKING\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Exp√©rience: {experiment.name}\")\n",
    "print(f\"Run ID: {parent_run.id}\")\n",
    "\n",
    "# Dictionnaire pour stocker tous les r√©sultats\n",
    "all_results = []\n",
    "all_models = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mod√®le 1: Random Forest (Baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Child run 1\n",
    "child_run_1 = parent_run.child_run(name=\"RF_Baseline\")\n",
    "\n",
    "model_1 = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    "    class_weight='balanced'\n",
    ")\n",
    "\n",
    "# Log des hyperparam√®tres\n",
    "child_run_1.log('n_estimators', 50)\n",
    "child_run_1.log('max_depth', 10)\n",
    "child_run_1.log('algorithm', 'RandomForest')\n",
    "\n",
    "metrics_1, trained_model_1, proba_1 = train_and_evaluate_model(\n",
    "    model_1, 'Random Forest (Baseline)', \n",
    "    X_train_balanced, y_train_balanced, \n",
    "    X_test_scaled, y_test,\n",
    "    child_run_1\n",
    ")\n",
    "\n",
    "all_results.append(metrics_1)\n",
    "all_models['RF_Baseline'] = trained_model_1\n",
    "child_run_1.complete()\n",
    "print(\"‚úÖ Mod√®le 1 compl√©t√©\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mod√®le 2: Random Forest (Optimis√©)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Child run 2\n",
    "child_run_2 = parent_run.child_run(name=\"RF_Optimized\")\n",
    "\n",
    "model_2 = RandomForestClassifier(\n",
    "    n_estimators=100,\n",
    "    max_depth=20,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    "    class_weight='balanced'\n",
    ")\n",
    "\n",
    "child_run_2.log('n_estimators', 100)\n",
    "child_run_2.log('max_depth', 20)\n",
    "child_run_2.log('min_samples_split', 10)\n",
    "child_run_2.log('min_samples_leaf', 5)\n",
    "child_run_2.log('algorithm', 'RandomForest')\n",
    "\n",
    "metrics_2, trained_model_2, proba_2 = train_and_evaluate_model(\n",
    "    model_2, 'Random Forest (Optimis√©)', \n",
    "    X_train_balanced, y_train_balanced, \n",
    "    X_test_scaled, y_test,\n",
    "    child_run_2\n",
    ")\n",
    "\n",
    "all_results.append(metrics_2)\n",
    "all_models['RF_Optimized'] = trained_model_2\n",
    "child_run_2.complete()\n",
    "print(\"‚úÖ Mod√®le 2 compl√©t√©\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mod√®le 3: Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Child run 3\n",
    "child_run_3 = parent_run.child_run(name=\"GradientBoosting\")\n",
    "\n",
    "model_3 = GradientBoostingClassifier(\n",
    "    n_estimators=100,\n",
    "    learning_rate=0.1,\n",
    "    max_depth=5,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "child_run_3.log('n_estimators', 100)\n",
    "child_run_3.log('learning_rate', 0.1)\n",
    "child_run_3.log('max_depth', 5)\n",
    "child_run_3.log('algorithm', 'GradientBoosting')\n",
    "\n",
    "metrics_3, trained_model_3, proba_3 = train_and_evaluate_model(\n",
    "    model_3, 'Gradient Boosting', \n",
    "    X_train_balanced, y_train_balanced, \n",
    "    X_test_scaled, y_test,\n",
    "    child_run_3\n",
    ")\n",
    "\n",
    "all_results.append(metrics_3)\n",
    "all_models['GradientBoosting'] = trained_model_3\n",
    "child_run_3.complete()\n",
    "print(\"‚úÖ Mod√®le 3 compl√©t√©\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mod√®le 4: XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Child run 4\n",
    "child_run_4 = parent_run.child_run(name=\"XGBoost\")\n",
    "\n",
    "# Calculer le scale_pos_weight pour g√©rer le d√©s√©quilibre\n",
    "scale_pos_weight = (y_train_balanced == 0).sum() / (y_train_balanced == 1).sum()\n",
    "\n",
    "model_4 = xgb.XGBClassifier(\n",
    "    n_estimators=100,\n",
    "    max_depth=6,\n",
    "    learning_rate=0.1,\n",
    "    scale_pos_weight=scale_pos_weight,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "child_run_4.log('n_estimators', 100)\n",
    "child_run_4.log('max_depth', 6)\n",
    "child_run_4.log('learning_rate', 0.1)\n",
    "child_run_4.log('scale_pos_weight', scale_pos_weight)\n",
    "child_run_4.log('algorithm', 'XGBoost')\n",
    "\n",
    "metrics_4, trained_model_4, proba_4 = train_and_evaluate_model(\n",
    "    model_4, 'XGBoost', \n",
    "    X_train_balanced, y_train_balanced, \n",
    "    X_test_scaled, y_test,\n",
    "    child_run_4\n",
    ")\n",
    "\n",
    "all_results.append(metrics_4)\n",
    "all_models['XGBoost'] = trained_model_4\n",
    "child_run_4.complete()\n",
    "print(\"‚úÖ Mod√®le 4 compl√©t√©\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mod√®le 5: LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Child run 5\n",
    "child_run_5 = parent_run.child_run(name=\"LightGBM\")\n",
    "\n",
    "model_5 = lgb.LGBMClassifier(\n",
    "    n_estimators=100,\n",
    "    max_depth=6,\n",
    "    learning_rate=0.1,\n",
    "    num_leaves=31,\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    "    class_weight='balanced'\n",
    ")\n",
    "\n",
    "child_run_5.log('n_estimators', 100)\n",
    "child_run_5.log('max_depth', 6)\n",
    "child_run_5.log('learning_rate', 0.1)\n",
    "child_run_5.log('num_leaves', 31)\n",
    "child_run_5.log('algorithm', 'LightGBM')\n",
    "\n",
    "metrics_5, trained_model_5, proba_5 = train_and_evaluate_model(\n",
    "    model_5, 'LightGBM', \n",
    "    X_train_balanced, y_train_balanced, \n",
    "    X_test_scaled, y_test,\n",
    "    child_run_5\n",
    ")\n",
    "\n",
    "all_results.append(metrics_5)\n",
    "all_models['LightGBM'] = trained_model_5\n",
    "child_run_5.complete()\n",
    "print(\"‚úÖ Mod√®le 5 compl√©t√©\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Comparaison des R√©sultats de Tous les Mod√®les"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cr√©er un DataFrame de comparaison\n",
    "comparison_df = pd.DataFrame(all_results)\n",
    "comparison_df = comparison_df.sort_values('f1_score', ascending=False)\n",
    "\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"üìä TABLEAU COMPARATIF DES MOD√àLES\")\n",
    "print(\"=\"*100)\n",
    "print(comparison_df.to_string(index=False))\n",
    "print(\"=\"*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sauvegarder le tableau de comparaison\n",
    "comparison_df.to_csv('model_comparison.csv', index=False)\n",
    "parent_run.upload_file('model_comparison.csv', 'model_comparison.csv')\n",
    "print(\"‚úÖ Tableau de comparaison sauvegard√©\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisation comparative - Graphique en barres\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "fig.suptitle('Comparaison des Performances des Mod√®les', fontsize=16, fontweight='bold')\n",
    "\n",
    "metrics_to_plot = ['accuracy', 'precision', 'recall', 'f1_score', 'roc_auc', 'training_time_seconds']\n",
    "metric_names = ['Accuracy', 'Precision', 'Recall', 'F1-Score', 'ROC-AUC', 'Temps (s)']\n",
    "\n",
    "for idx, (metric, name) in enumerate(zip(metrics_to_plot, metric_names)):\n",
    "    row = idx // 3\n",
    "    col = idx % 3\n",
    "    ax = axes[row, col]\n",
    "    \n",
    "    # Extraire les valeurs\n",
    "    models = comparison_df['model_name'].values\n",
    "    values = comparison_df[metric].values\n",
    "    \n",
    "    # Cr√©er le graphique\n",
    "    colors = plt.cm.viridis(np.linspace(0.3, 0.9, len(models)))\n",
    "    bars = ax.bar(range(len(models)), values, color=colors, edgecolor='black', linewidth=1.5)\n",
    "    \n",
    "    # Ajouter les valeurs sur les barres\n",
    "    for i, (bar, val) in enumerate(zip(bars, values)):\n",
    "        height = bar.get_height()\n",
    "        ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                f'{val:.4f}' if metric != 'training_time_seconds' else f'{val:.2f}',\n",
    "                ha='center', va='bottom', fontsize=9, fontweight='bold')\n",
    "    \n",
    "    ax.set_xticks(range(len(models)))\n",
    "    ax.set_xticklabels(models, rotation=45, ha='right', fontsize=9)\n",
    "    ax.set_ylabel(name, fontsize=11, fontweight='bold')\n",
    "    ax.set_title(name, fontsize=12, fontweight='bold')\n",
    "    ax.grid(axis='y', alpha=0.3, linestyle='--')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('model_comparison_bars.png', dpi=300, bbox_inches='tight')\n",
    "parent_run.log_image('model_comparison_bars', plot=plt)\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úÖ Graphiques de comparaison cr√©√©s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Radar Chart pour visualisation multi-dimensionnelle\n",
    "from math import pi\n",
    "\n",
    "# Pr√©parer les donn√©es pour le radar chart\n",
    "categories = ['Accuracy', 'Precision', 'Recall', 'F1-Score', 'ROC-AUC']\n",
    "N = len(categories)\n",
    "\n",
    "# Cr√©er les angles pour chaque axe\n",
    "angles = [n / float(N) * 2 * pi for n in range(N)]\n",
    "angles += angles[:1]\n",
    "\n",
    "# Initialiser le plot\n",
    "fig, ax = plt.subplots(figsize=(10, 10), subplot_kw=dict(projection='polar'))\n",
    "\n",
    "# Tracer chaque mod√®le\n",
    "colors = ['#FF6B6B', '#4ECDC4', '#45B7D1', '#FFA07A', '#98D8C8']\n",
    "for idx, row in comparison_df.iterrows():\n",
    "    values = [\n",
    "        row['accuracy'],\n",
    "        row['precision'],\n",
    "        row['recall'],\n",
    "        row['f1_score'],\n",
    "        row['roc_auc']\n",
    "    ]\n",
    "    values += values[:1]\n",
    "    \n",
    "    ax.plot(angles, values, 'o-', linewidth=2, label=row['model_name'], color=colors[idx % len(colors)])\n",
    "    ax.fill(angles, values, alpha=0.15, color=colors[idx % len(colors)])\n",
    "\n",
    "# Configurer le graphique\n",
    "ax.set_xticks(angles[:-1])\n",
    "ax.set_xticklabels(categories, size=11)\n",
    "ax.set_ylim(0, 1)\n",
    "ax.set_title('Comparaison Multi-Dimensionnelle des Mod√®les', size=16, fontweight='bold', pad=20)\n",
    "ax.legend(loc='upper right', bbox_to_anchor=(1.3, 1.1), fontsize=10)\n",
    "ax.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('model_comparison_radar.png', dpi=300, bbox_inches='tight')\n",
    "parent_run.log_image('model_comparison_radar', plot=plt)\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úÖ Radar chart cr√©√©\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. S√©lection et Enregistrement du Meilleur Mod√®le"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identifier le meilleur mod√®le bas√© sur le F1-Score\n",
    "best_model_row = comparison_df.iloc[0]\n",
    "best_model_name = best_model_row['model_name']\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üèÜ MEILLEUR MOD√àLE S√âLECTIONN√â\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nMod√®le: {best_model_name}\")\n",
    "print(f\"\\nPerformances:\")\n",
    "print(f\"  - Accuracy:  {best_model_row['accuracy']:.4f}\")\n",
    "print(f\"  - Precision: {best_model_row['precision']:.4f}\")\n",
    "print(f\"  - Recall:    {best_model_row['recall']:.4f}\")\n",
    "print(f\"  - F1-Score:  {best_model_row['f1_score']:.4f}\")\n",
    "print(f\"  - ROC-AUC:   {best_model_row['roc_auc']:.4f}\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# R√©cup√©rer le meilleur mod√®le\n",
    "model_key = best_model_name.replace(' ', '_').replace('(', '').replace(')', '')\n",
    "if 'Baseline' in best_model_name:\n",
    "    best_model = all_models['RF_Baseline']\n",
    "elif 'Optimis√©' in best_model_name:\n",
    "    best_model = all_models['RF_Optimized']\n",
    "elif 'Gradient' in best_model_name:\n",
    "    best_model = all_models['GradientBoosting']\n",
    "elif 'XGBoost' in best_model_name:\n",
    "    best_model = all_models['XGBoost']\n",
    "elif 'LightGBM' in best_model_name:\n",
    "    best_model = all_models['LightGBM']\n",
    "\n",
    "# Sauvegarder le meilleur mod√®le\n",
    "best_model_filename = 'best_fraud_detection_model.pkl'\n",
    "joblib.dump(best_model, best_model_filename)\n",
    "joblib.dump(scaler, 'scaler.pkl')\n",
    "\n",
    "print(f\"‚úÖ Meilleur mod√®le sauvegard√©: {best_model_filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enregistrer dans Azure ML\n",
    "parent_run.upload_file(name='outputs/' + best_model_filename, path_or_stream=best_model_filename)\n",
    "parent_run.upload_file(name='outputs/scaler.pkl', path_or_stream='scaler.pkl')\n",
    "\n",
    "# Enregistrer comme mod√®le Azure ML\n",
    "registered_model = parent_run.register_model(\n",
    "    model_name='fraud-detection-best',\n",
    "    model_path='outputs/' + best_model_filename,\n",
    "    description=f'Best fraud detection model: {best_model_name}',\n",
    "    tags={\n",
    "        'algorithm': best_model_name,\n",
    "        'accuracy': f\"{best_model_row['accuracy']:.4f}\",\n",
    "        'f1_score': f\"{best_model_row['f1_score']:.4f}\",\n",
    "        'roc_auc': f\"{best_model_row['roc_auc']:.4f}\",\n",
    "        'experiment': experiment_name\n",
    "    }\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Mod√®le enregistr√© dans Azure ML: {registered_model.name}, Version: {registered_model.version}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enregistrer tous les mod√®les individuellement\n",
    "print(\"\\nüì¶ Enregistrement de tous les mod√®les...\")\n",
    "\n",
    "for idx, row in comparison_df.iterrows():\n",
    "    model_name = row['model_name']\n",
    "    model_key = model_name.replace(' ', '_').replace('(', '').replace(')', '')\n",
    "    \n",
    "    if 'Baseline' in model_name:\n",
    "        model = all_models['RF_Baseline']\n",
    "    elif 'Optimis√©' in model_name:\n",
    "        model = all_models['RF_Optimized']\n",
    "    elif 'Gradient' in model_name:\n",
    "        model = all_models['GradientBoosting']\n",
    "    elif 'XGBoost' in model_name:\n",
    "        model = all_models['XGBoost']\n",
    "    elif 'LightGBM' in model_name:\n",
    "        model = all_models['LightGBM']\n",
    "    \n",
    "    filename = f\"model_{model_key}.pkl\"\n",
    "    joblib.dump(model, filename)\n",
    "    print(f\"  ‚úì {model_name} sauvegard√©\")\n",
    "\n",
    "print(\"\\n‚úÖ Tous les mod√®les ont √©t√© enregistr√©s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Terminer le run parent\n",
    "parent_run.complete()\n",
    "print(\"\\n‚úÖ Exp√©rience Azure ML termin√©e\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Export pour Power BI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export du tableau de comparaison pour Power BI\n",
    "comparison_df.to_csv('model_comparison_powerbi.csv', index=False)\n",
    "\n",
    "# Cr√©er un r√©sum√© d√©taill√©\n",
    "summary_df = comparison_df.copy()\n",
    "summary_df['rank'] = range(1, len(summary_df) + 1)\n",
    "summary_df = summary_df[[\n",
    "    'rank', 'model_name', 'accuracy', 'precision', 'recall', \n",
    "    'f1_score', 'roc_auc', 'training_time_seconds'\n",
    "]]\n",
    "\n",
    "summary_df.to_csv('models_detailed_summary.csv', index=False)\n",
    "\n",
    "print(\"‚úÖ Fichiers export√©s pour Power BI:\")\n",
    "print(\"   - model_comparison_powerbi.csv\")\n",
    "print(\"   - models_detailed_summary.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. R√©capitulatif Final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\"\"\n",
    "‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó\n",
    "‚ïë              R√âCAPITULATIF DE L'EXPERIMENT TRACKING                ‚ïë\n",
    "‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù\n",
    "\n",
    "‚úÖ MOD√àLES ENTRA√éN√âS ET COMPAR√âS:\n",
    "   1. Random Forest (Baseline) - Configuration simple\n",
    "   2. Random Forest (Optimis√©) - Hyperparam√®tres tun√©s\n",
    "   3. Gradient Boosting - Approche boosting traditionnelle\n",
    "   4. XGBoost - Extreme Gradient Boosting\n",
    "   5. LightGBM - Light Gradient Boosting Machine\n",
    "\n",
    "üìä M√âTRIQUES TRACK√âES:\n",
    "   ‚Ä¢ Accuracy\n",
    "   ‚Ä¢ Precision\n",
    "   ‚Ä¢ Recall\n",
    "   ‚Ä¢ F1-Score\n",
    "   ‚Ä¢ ROC-AUC\n",
    "   ‚Ä¢ Temps d'entra√Ænement\n",
    "\n",
    "üîÑ EXPERIMENT TRACKING:\n",
    "   ‚Ä¢ Toutes les exp√©riences logg√©es dans Azure ML\n",
    "   ‚Ä¢ M√©triques comparables dans le portail Azure\n",
    "   ‚Ä¢ Visualisations automatiquement g√©n√©r√©es\n",
    "   ‚Ä¢ Mod√®les versionn√©s et sauvegard√©s\n",
    "\n",
    "üìà VISUALISATIONS CR√â√âES:\n",
    "   ‚Ä¢ Matrices de confusion pour chaque mod√®le\n",
    "   ‚Ä¢ Graphiques en barres comparatifs\n",
    "   ‚Ä¢ Radar chart multi-dimensionnel\n",
    "\n",
    "üíæ FICHIERS G√âN√âR√âS:\n",
    "   ‚Ä¢ model_comparison.csv - Tableau comparatif complet\n",
    "   ‚Ä¢ best_fraud_detection_model.pkl - Meilleur mod√®le\n",
    "   ‚Ä¢ model_*.pkl - Tous les mod√®les individuels\n",
    "   ‚Ä¢ scaler.pkl - Normalisation des donn√©es\n",
    "   ‚Ä¢ *_powerbi.csv - Fichiers pour Power BI\n",
    "\n",
    "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "\"\"\")\n",
    "\n",
    "print(f\"\\nüèÜ MEILLEUR MOD√àLE: {best_model_name}\")\n",
    "print(f\"   F1-Score: {best_model_row['f1_score']:.4f}\")\n",
    "print(f\"   ROC-AUC: {best_model_row['roc_auc']:.4f}\")\n",
    "print(f\"\\n‚è∞ Date de fin: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(\"\\nüéâ EXPERIMENT TRACKING COMPL√âT√â AVEC SUCC√àS! üéâ\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
